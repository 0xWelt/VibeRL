{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VibeRL Documentation","text":"<p>Welcome to VibeRL - A modern Reinforcement Learning framework built with type safety and modern Python practices.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/0xWelt/VibeRL.git\ncd VibeRL\n\n# Install using [uv](https://docs.astral.sh/uv/)\nuv pip install -e \".\"\n\n# Or install with development dependencies\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"#your-first-training","title":"Your First Training","text":"<pre><code># Train a REINFORCE agent\nviberl-train --alg reinforce --episodes 1000 --grid-size 10\n\n# Train a DQN agent\nviberl-train --alg dqn --episodes 2000 --grid-size 15 --memory-size 10000\n\n# Train a PPO agent\nviberl-train --alg ppo --episodes 1000 --grid-size 12 --ppo-epochs 4\n</code></pre>"},{"location":"#evaluate-and-demo","title":"Evaluate and Demo","text":"<pre><code># Evaluate a trained model\nviberl-eval --model-path experiments/reinforce_snake_20241231_120000/final_model.pth --episodes 10 --render\n\n# Run demo with random actions\nviberl-demo --episodes 5 --grid-size 15\n\n# Monitor training with TensorBoard\ntensorboard --logdir experiments/\n</code></pre>"},{"location":"#experiment-management","title":"Experiment Management","text":""},{"location":"#directory-structure","title":"Directory Structure","text":"<p>When training agents, VibeRL automatically creates organized experiment directories:</p> <pre><code>experiments/\n\u251c\u2500\u2500 reinforce_snake_20241231_120000/\n\u2502   \u251c\u2500\u2500 final_model.pth\n\u2502   \u251c\u2500\u2500 best_model.pth\n\u2502   \u251c\u2500\u2500 config.json\n\u2502   \u251c\u2500\u2500 metrics.json\n\u2502   \u251c\u2500\u2500 tensorboard/\n\u2502   \u2502   \u2514\u2500\u2500 events.out.tfevents.*\n\u2502   \u2514\u2500\u2500 logs/\n\u2502       \u2514\u2500\u2500 training.log\n\u251c\u2500\u2500 dqn_snake_20250101_143000/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ppo_snake_20250102_090000/\n    \u2514\u2500\u2500 ...\n</code></pre> <p>Each experiment directory contains: - Model files: <code>final_model.pth</code>, <code>best_model.pth</code> (highest reward) - Configuration: <code>config.json</code> with all training parameters - Metrics: <code>metrics.json</code> with training statistics - TensorBoard logs: Real-time training metrics - Training logs: Detailed log files</p>"},{"location":"#tensorboard-integration","title":"TensorBoard Integration","text":"<p>Monitor your training progress in real-time with TensorBoard:</p> <pre><code># Start TensorBoard for all experiments\ntensorboard --logdir experiments/\n\n# Start TensorBoard for specific algorithm\ntensorboard --logdir experiments/reinforce_*\n\n# Start TensorBoard for specific run\ntensorboard --logdir experiments/reinforce_snake_20241231_120000/tensorboard/\n</code></pre> <p>Access TensorBoard at <code>http://localhost:6006</code> to view: - Episode rewards and lengths - Loss curves (policy, value, total) - Learning rates and hyperparameters - Action distributions - Custom metrics per algorithm</p>"},{"location":"#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<p>Track experiments with Weights &amp; Biases for enhanced experiment management:</p> <pre><code># Enable wandb logging during training\nviberl-train --alg dqn --episodes 1000 --wandb --name my_experiment\n\n# All CLI arguments are automatically logged to wandb\nviberl-train --alg ppo --episodes 500 --lr 3e-4 --wandb --name ppo_tuning\n</code></pre> <p>Features include: - Automatic hyperparameter tracking - Real-time metric visualization - Experiment comparison - Artifact storage for models and logs - Collaborative experiment sharing</p>"},{"location":"#python-api","title":"Python API","text":""},{"location":"#basic-training","title":"Basic Training","text":"<pre><code>from viberl.agents.reinforce import REINFORCEAgent\nfrom viberl.envs import SnakeGameEnv\nfrom viberl.utils.training import train_agent\n\n# Create environment\nenv = SnakeGameEnv(grid_size=10)\n\n# Create agent\nagent = REINFORCEAgent(\n    state_size=100,  # 10x10 grid\n    action_size=4,   # 4 directions\n    learning_rate=0.001\n)\n\n# Train the agent\ntrain_agent(\n    agent=agent,\n    env=env,\n    episodes=1000,\n    save_path=\"models/reinforce_snake.pth\"\n)\n</code></pre>"},{"location":"#custom-training-loop","title":"Custom Training Loop","text":"<pre><code>import numpy as np\nfrom loguru import logger\nfrom viberl.typing import Trajectory, Transition\nfrom viberl.agents.dqn import DQNAgent\n\nenv = SnakeGameEnv(grid_size=10)\nagent = DQNAgent(state_size=100, action_size=4)\n\nfor episode in range(1000):\n    state, _ = env.reset()\n    transitions = []\n\n    while True:\n        action = agent.act(state, training=True)\n        next_state, reward, done, truncated, info = env.step(action.action)\n\n        transitions.append(Transition(\n            state=state, action=action, reward=reward,\n            next_state=next_state, done=done\n        ))\n\n        state = next_state\n        if done or truncated:\n            break\n\n    trajectory = Trajectory.from_transitions(transitions)\n    metrics = agent.learn(trajectory)\n\n    if episode % 100 == 0:\n        logger.info(f\"Episode {episode}, Reward: {trajectory.total_reward}\")\n</code></pre>"},{"location":"#experiment-management-api","title":"Experiment Management API","text":"<pre><code>from viberl.utils.experiment_manager import ExperimentManager\nfrom viberl.agents.ppo import PPOAgent\nfrom viberl.envs import SnakeGameEnv\n\n# Create experiment with automatic directory structure\nexperiment = ExperimentManager(\n    algorithm=\"ppo\",\n    grid_size=12,\n    learning_rate=0.001,\n    experiment_dir=\"experiments\"\n)\n\n# Access experiment paths\nlogger.info(f\"Model will be saved to: {experiment.model_path}\")\nlogger.info(f\"TensorBoard logs: {experiment.tensorboard_path}\")\nlogger.info(f\"Config file: {experiment.config_path}\")\n\n# Use with training\nenv = SnakeGameEnv(grid_size=12)\nagent = PPOAgent(\n    state_size=144,  # 12x12 grid\n    action_size=4,\n    learning_rate=0.001,\n    device=experiment.device\n)\n\n# Train with automatic logging\ntraining_metrics = experiment.train_agent(\n    agent=agent,\n    env=env,\n    episodes=1000,\n    log_frequency=100\n)\n\n# View TensorBoard (run in terminal)\n# tensorboard --logdir experiments/ppo_*\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>Modern Type System: Pydantic-based Action, Transition, Trajectory classes</li> <li>Three Algorithms: REINFORCE, DQN, PPO with unified interface</li> <li>Type Safety: Full type annotations throughout</li> <li>CLI Interface: Complete training, evaluation, and demo commands</li> <li>Experiment Management: Automatic directory structure with TensorBoard and Weights &amp; Biases logging</li> <li>50+ Tests: Comprehensive test suite</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>The framework follows a clean architecture:</p> <ul> <li><code>viberl/typing.py</code>: Modern type system</li> <li><code>viberl/agents/</code>: RL algorithms (REINFORCE, DQN, PPO)</li> <li><code>viberl/envs/</code>: Environments (SnakeGameEnv)</li> <li><code>viberl/networks/</code>: Neural network implementations</li> <li><code>viberl/utils/</code>: Training utilities, experiment management, and unified logging</li> <li><code>viberl/cli.py</code>: Command-line interface</li> </ul>"},{"location":"#algorithms","title":"Algorithms","text":""},{"location":"#reinforce","title":"REINFORCE","text":"<p>Policy gradient method using Monte Carlo returns.</p>"},{"location":"#dqn","title":"DQN","text":"<p>Deep Q-Network with experience replay and target networks.</p>"},{"location":"#ppo","title":"PPO","text":"<p>Proximal Policy Optimization with clipping and multiple epochs.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the VibeRL API documentation. This section provides file-level documentation matching the exact structure of the source code.</p>"},{"location":"api/#file-structure","title":"File Structure","text":"<p>Each <code>.py</code> file in the source code has a corresponding <code>.md</code> file in this documentation:</p> <pre><code>viberl/\n\u251c\u2500\u2500 agents/\n\u2502   \u251c\u2500\u2500 base.py \u2192 base.md\n\u2502   \u251c\u2500\u2500 reinforce.py \u2192 reinforce.md\n\u2502   \u251c\u2500\u2500 dqn.py \u2192 dqn.md\n\u2502   \u2514\u2500\u2500 ppo.py \u2192 ppo.md\n\u251c\u2500\u2500 networks/\n\u2502   \u251c\u2500\u2500 base_network.py \u2192 base_network.md\n\u2502   \u251c\u2500\u2500 policy_network.py \u2192 policy_network.md\n\u2502   \u2514\u2500\u2500 value_network.py \u2192 value_network.md\n\u251c\u2500\u2500 envs/\n\u2502   \u2514\u2500\u2500 grid_world/\n\u2502       \u2514\u2500\u2500 snake_env.py \u2192 grid_world/snake_env.md\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 common.py \u2192 common.md\n\u2502   \u251c\u2500\u2500 training.py \u2192 training.md\n\u2502   \u251c\u2500\u2500 mock_env.py \u2192 mock_env.md\n\u2502   \u2514\u2500\u2500 experiment_manager.py \u2192 experiment_manager.md\n\u251c\u2500\u2500 typing.py \u2192 typing.md\n\u2514\u2500\u2500 cli.py \u2192 cli.md\n</code></pre>"},{"location":"api/#navigation","title":"Navigation","text":"<p>Each page contains: 1. Module Overview - File-level documentation with hyperlinks 2. Classes &amp; Functions - Detailed documentation for all classes and functions in the file</p> <p>Click on any file in the navigation menu to explore its contents.</p>"},{"location":"api/cli/","title":"<code>viberl.cli</code>","text":"<p>Command-line interface for Tiny RL.</p> <p>Functions:</p> Name Description <code>train_main</code> <p>Main training CLI entry point.</p> <code>eval_main</code> <p>Main evaluation CLI entry point.</p> <code>demo_main</code> <p>Demo CLI entry point.</p>"},{"location":"api/cli/#viberl.cli.train_main","title":"train_main","text":"<pre><code>train_main()\n</code></pre> <p>Main training CLI entry point.</p> Source code in <code>viberl/cli.py</code> <pre><code>def train_main():\n    \"\"\"Main training CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(description='Train RL agents')\n    parser.add_argument('--env', choices=['snake'], default='snake', help='Environment to train on')\n    parser.add_argument(\n        '--alg',\n        choices=['reinforce', 'dqn', 'ppo'],\n        default='reinforce',\n        help='Reinforcement learning algorithm',\n    )\n    parser.add_argument('--episodes', type=int, default=1000, help='Number of training episodes')\n    parser.add_argument('--grid-size', type=int, default=15, help='Grid size for snake environment')\n    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')\n    parser.add_argument('--gamma', type=float, default=0.99, help='Discount factor')\n    parser.add_argument('--hidden-size', type=int, default=128, help='Hidden layer size')\n    parser.add_argument('--num-hidden-layers', type=int, default=2, help='Number of hidden layers')\n    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n    parser.add_argument('--name', type=str, help='Experiment name (auto-generated if not provided)')\n    parser.add_argument('--render-interval', type=int, help='Render every N episodes')\n    parser.add_argument('--save-interval', type=int, help='Save model every N episodes (optional)')\n    parser.add_argument('--device', choices=['cpu', 'cuda'], default='auto', help='Device to use')\n    parser.add_argument(\n        '--epsilon-start', type=float, default=1.0, help='Initial exploration rate (DQN)'\n    )\n    parser.add_argument(\n        '--epsilon-end', type=float, default=0.01, help='Final exploration rate (DQN)'\n    )\n    parser.add_argument(\n        '--epsilon-decay', type=float, default=0.995, help='Exploration decay rate (DQN)'\n    )\n    parser.add_argument('--memory-size', type=int, default=10000, help='Replay memory size (DQN)')\n    parser.add_argument('--batch-size', type=int, default=64, help='Batch size for training')\n    parser.add_argument(\n        '--target-update', type=int, default=10, help='Target network update frequency (DQN)'\n    )\n    parser.add_argument('--clip-epsilon', type=float, default=0.2, help='PPO clipping parameter')\n    parser.add_argument('--ppo-epochs', type=int, default=4, help='PPO epochs per update')\n    parser.add_argument('--gae-lambda', type=float, default=0.95, help='GAE lambda parameter (PPO)')\n    parser.add_argument(\n        '--value-loss-coef', type=float, default=0.5, help='Value loss coefficient (PPO)'\n    )\n    parser.add_argument(\n        '--entropy-coef', type=float, default=0.01, help='Entropy coefficient (PPO)'\n    )\n    parser.add_argument('--max-grad-norm', type=float, default=0.5, help='Max gradient norm (PPO)')\n    parser.add_argument('--max-steps', type=int, default=1000, help='Maximum steps per episode')\n    parser.add_argument('--eval-episodes', type=int, default=10, help='Evaluation episodes')\n    parser.add_argument(\n        '--eval-interval', type=int, default=100, help='Evaluation interval during training'\n    )\n    parser.add_argument(\n        '--log-interval', type=int, default=1000, help='Log summary interval during training'\n    )\n    parser.add_argument(\n        '--trajectory-batch',\n        type=int,\n        default=8,\n        help='Number of trajectories to collect per training iteration',\n    )\n    parser.add_argument('--no-eval', action='store_true', help='Skip evaluation after training')\n    parser.add_argument('--quiet', action='store_true', help='Suppress training progress output')\n    parser.add_argument('--wandb', action='store_true', help='Enable Weights &amp; Biases logging')\n\n    args = parser.parse_args()\n\n    # Set random seed\n    set_seed(args.seed)\n\n    # Get device\n    device = get_device() if args.device == 'auto' else torch.device(args.device)\n\n    logger.info(f'Using device: {device}')\n\n    # Create environment\n    if args.env == 'snake':\n        env = SnakeGameEnv(grid_size=args.grid_size)\n        state_size = args.grid_size * args.grid_size\n        action_size = 4\n    else:\n        raise ValueError(f'Unknown environment: {args.env}')\n\n    # Create agent\n    base_params = {\n        'state_size': state_size,\n        'action_size': action_size,\n        'learning_rate': args.lr,\n        'gamma': args.gamma,\n        'hidden_size': args.hidden_size,\n        'num_hidden_layers': args.num_hidden_layers,\n    }\n\n    if args.alg == 'reinforce':\n        agent = REINFORCEAgent(**base_params)\n    elif args.alg == 'dqn':\n        agent = DQNAgent(\n            **base_params,\n            epsilon_start=args.epsilon_start,\n            epsilon_end=args.epsilon_end,\n            epsilon_decay=args.epsilon_decay,\n            memory_size=args.memory_size,\n            batch_size=args.batch_size,\n            target_update=args.target_update,\n        )\n    elif args.alg == 'ppo':\n        agent = PPOAgent(\n            **base_params,\n            clip_epsilon=args.clip_epsilon,\n            ppo_epochs=args.ppo_epochs,\n            lam=args.gae_lambda,\n            value_loss_coef=args.value_loss_coef,\n            entropy_coef=args.entropy_coef,\n            max_grad_norm=args.max_grad_norm,\n            batch_size=args.batch_size,\n        )\n    else:\n        raise ValueError(f'Unknown algorithm: {args.alg}')\n\n    # Move agent to device\n    if hasattr(agent, 'policy_network'):\n        agent.policy_network.to(device)\n    if hasattr(agent, 'q_network'):\n        agent.q_network.to(device)\n        agent.target_network.to(device)\n    if hasattr(agent, 'value_network'):\n        agent.value_network.to(device)\n\n    logger.info(f'Training {args.alg} agent on {args.env} environment...')\n    logger.info(f'Episodes: {args.episodes}')\n    logger.info(f'Grid size: {args.grid_size}')\n    logger.info(f'Learning rate: {args.lr}')\n    logger.info(f'Gamma: {args.gamma}')\n\n    # Create experiment with automatic directory structure\n    experiment_name = args.name or f'{args.alg}_{args.env}'\n    exp_manager = create_experiment(experiment_name)\n    tb_logs_dir = str(exp_manager.get_tb_logs_path())\n\n    # Configure file logging and log command line arguments\n    exp_manager.configure_file_logging(log_level='INFO')\n    exp_manager.log_command_line_args(args)\n\n    if tb_logs_dir:\n        logger.info(f'TensorBoard logs: {tb_logs_dir}')\n\n    # Create trainer and train agent\n    trainer = Trainer(\n        env=env,\n        agent=agent,\n        max_steps=args.max_steps,\n        log_dir=tb_logs_dir,\n        device=device,\n        enable_wandb=args.wandb,\n        wandb_config=vars(args),\n        run_name=experiment_name,\n        batch_size=args.trajectory_batch,\n    )\n\n    trainer.train(\n        num_episodes=args.episodes,\n        render_interval=args.render_interval,\n        save_interval=args.save_interval,\n        save_path=str(exp_manager.get_models_path()),\n        eval_interval=args.eval_interval,\n        eval_episodes=args.eval_episodes,\n        log_interval=args.log_interval,\n        verbose=not args.quiet,\n    )\n\n    # Save final model\n    models_dir = exp_manager.get_models_path()\n    final_model_path = str(models_dir / 'final_model.pth')\n    agent.save(final_model_path)\n    logger.success(f'Final model saved to {final_model_path}')\n    logger.success(f'All experiment files saved in: {exp_manager.get_experiment_path()}')\n\n    env.close()\n</code></pre>"},{"location":"api/cli/#viberl.cli.eval_main","title":"eval_main","text":"<pre><code>eval_main()\n</code></pre> <p>Main evaluation CLI entry point.</p> Source code in <code>viberl/cli.py</code> <pre><code>def eval_main():\n    \"\"\"Main evaluation CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(description='Evaluate trained RL agents')\n    parser.add_argument(\n        '--env', choices=['snake'], default='snake', help='Environment to evaluate on'\n    )\n    parser.add_argument(\n        '--agent', choices=['reinforce'], default='reinforce', help='Agent algorithm'\n    )\n    parser.add_argument('--model-path', type=str, required=True, help='Path to trained model')\n    parser.add_argument('--episodes', type=int, default=10, help='Number of evaluation episodes')\n    parser.add_argument('--grid-size', type=int, default=15, help='Grid size for snake environment')\n    parser.add_argument('--render', action='store_true', help='Render the environment')\n    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n    parser.add_argument('--device', choices=['cpu', 'cuda'], default='auto', help='Device to use')\n\n    args = parser.parse_args()\n\n    # Set random seed\n    set_seed(args.seed)\n\n    # Get device\n    device = get_device() if args.device == 'auto' else torch.device(args.device)\n\n    logger.info(f'Using device: {device}')\n\n    # Create environment\n    if args.env == 'snake':\n        env = SnakeGameEnv(render_mode='human' if args.render else None, grid_size=args.grid_size)\n        state_size = args.grid_size * args.grid_size\n        action_size = 4\n    else:\n        raise ValueError(f'Unknown environment: {args.env}')\n\n    # Create agent\n    if args.agent == 'reinforce':\n        agent = REINFORCEAgent(state_size=state_size, action_size=action_size)\n    else:\n        raise ValueError(f'Unknown agent: {args.agent}')\n\n    # Move agent to device\n    agent.policy_network.to(device)\n\n    # Load trained model\n    try:\n        agent.load(args.model_path)\n        logger.success(f'Loaded model from {args.model_path}')\n    except OSError as e:\n        logger.error(f'Failed to load model: {e}')\n        return\n\n    # Evaluate agent\n    logger.info(f'Evaluating {args.agent} agent on {args.env} environment...')\n\n    scores = evaluate_agent(env=env, agent=agent, num_episodes=args.episodes, render=args.render)\n\n    logger.info('\\nEvaluation Results:')\n    logger.success(f'Average score: {np.mean(scores):.2f} \u00b1 {np.std(scores):.2f}')\n    logger.info(f'Min score: {np.min(scores):.2f}')\n    logger.info(f'Max score: {np.max(scores):.2f}')\n\n    env.close()\n</code></pre>"},{"location":"api/cli/#viberl.cli.demo_main","title":"demo_main","text":"<pre><code>demo_main()\n</code></pre> <p>Demo CLI entry point.</p> Source code in <code>viberl/cli.py</code> <pre><code>def demo_main():\n    \"\"\"Demo CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(description='Run RL framework demos')\n    parser.add_argument('--env', choices=['snake'], default='snake', help='Environment to demo')\n    parser.add_argument('--episodes', type=int, default=5, help='Number of demo episodes')\n    parser.add_argument('--grid-size', type=int, default=15, help='Grid size for snake environment')\n    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n\n    args = parser.parse_args()\n\n    # Set random seed\n    set_seed(args.seed)\n\n    # Create environment\n    if args.env == 'snake':\n        env = SnakeGameEnv(render_mode='human', grid_size=args.grid_size)\n    else:\n        raise ValueError(f'Unknown environment: {args.env}')\n\n    logger.info(f'Running {args.env} demo for {args.episodes} episodes...')\n\n    # Run demo with random actions\n    for episode in range(args.episodes):\n        state, info = env.reset()\n        total_reward = 0\n        steps = 0\n\n        logger.info(f'\\nEpisode {episode + 1}/{args.episodes}')\n\n        while True:\n            action = env.action_space.sample()  # Random action\n            state, reward, terminated, truncated, info = env.step(action)\n            total_reward += reward\n            steps += 1\n\n            if terminated or truncated:\n                logger.success(\n                    f'Episode finished! Score: {info.get(\"score\", 0)}, Steps: {steps}, Total reward: {total_reward}'\n                )\n                break\n\n    env.close()\n    logger.success('\\nDemo completed!')\n</code></pre>"},{"location":"api/typing/","title":"<code>viberl.typing</code>","text":"<p>Custom typing classes for reinforcement learning using Pydantic.</p> <p>Classes:</p> Name Description <code>Action</code> <p>An action taken by an agent, optionally with log probabilities.</p> <code>Transition</code> <p>A single transition in an episode.</p> <code>Trajectory</code> <p>A complete trajectory (episode) consisting of multiple transitions.</p>"},{"location":"api/typing/#viberl.typing.Action","title":"Action","text":"<pre><code>Action(**data: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>An action taken by an agent, optionally with log probabilities.</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <code>action</code> <code>int</code> <code>logprobs</code> <code>Tensor | None</code> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api/typing/#viberl.typing.Action.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/typing/#viberl.typing.Action.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: int\n</code></pre>"},{"location":"api/typing/#viberl.typing.Action.logprobs","title":"logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logprobs: Tensor | None = None\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition","title":"Transition","text":"<pre><code>Transition(**data: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>A single transition in an episode.</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <code>state</code> <code>ndarray</code> <code>action</code> <code>Action</code> <code>reward</code> <code>float</code> <code>next_state</code> <code>ndarray</code> <code>done</code> <code>bool</code> <code>info</code> <code>dict[str, Any]</code> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.state","title":"state  <code>instance-attribute</code>","text":"<pre><code>state: ndarray\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: Action\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.reward","title":"reward  <code>instance-attribute</code>","text":"<pre><code>reward: float\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.next_state","title":"next_state  <code>instance-attribute</code>","text":"<pre><code>next_state: ndarray\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.done","title":"done  <code>instance-attribute</code>","text":"<pre><code>done: bool\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.info","title":"info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>info: dict[str, Any] = {}\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory","title":"Trajectory","text":"<pre><code>Trajectory(**data: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>A complete trajectory (episode) consisting of multiple transitions.</p> <p>Methods:</p> Name Description <code>from_transitions</code> <p>Create a trajectory from a list of transitions.</p> <code>to_dict</code> <p>Convert trajectory to dictionary format for agent learning.</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <code>transitions</code> <code>list[Transition]</code> <code>total_reward</code> <code>float</code> <code>length</code> <code>int</code> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.transitions","title":"transitions  <code>instance-attribute</code>","text":"<pre><code>transitions: list[Transition]\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.total_reward","title":"total_reward  <code>instance-attribute</code>","text":"<pre><code>total_reward: float\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.length","title":"length  <code>instance-attribute</code>","text":"<pre><code>length: int\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.from_transitions","title":"from_transitions  <code>classmethod</code>","text":"<pre><code>from_transitions(transitions: list[Transition]) -&gt; Trajectory\n</code></pre> <p>Create a trajectory from a list of transitions.</p> Source code in <code>viberl/typing.py</code> <pre><code>@classmethod\ndef from_transitions(cls, transitions: list[Transition]) -&gt; Trajectory:\n    \"\"\"Create a trajectory from a list of transitions.\"\"\"\n    total_reward = sum(t.reward for t in transitions)\n    return cls(transitions=transitions, total_reward=total_reward, length=len(transitions))\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert trajectory to dictionary format for agent learning.</p> Source code in <code>viberl/typing.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert trajectory to dictionary format for agent learning.\"\"\"\n    return {\n        'states': [t.state for t in self.transitions],\n        'actions': [t.action.action for t in self.transitions],\n        'rewards': [t.reward for t in self.transitions],\n        'next_states': [t.next_state for t in self.transitions],\n        'dones': [t.done for t in self.transitions],\n        'logprobs': [\n            t.action.logprobs for t in self.transitions if t.action.logprobs is not None\n        ],\n        'infos': [t.info for t in self.transitions],\n    }\n</code></pre>"},{"location":"api/agents/base/","title":"<code>viberl.agents.base</code>","text":"<p>Base Agent class for all RL agents.</p> <p>Classes:</p> Name Description <code>Agent</code> <p>Abstract base class for all RL agents.</p>"},{"location":"api/agents/base/#viberl.agents.base.Agent","title":"Agent","text":"<pre><code>Agent(state_size: int, action_size: int)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all RL agents.</p> <p>Initialize the agent.</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <code>int</code> <p>Size of the state space</p> required <code>action_size</code> <code>int</code> <p>Size of the action space</p> required <p>Methods:</p> Name Description <code>act</code> <p>Select an action given the current state.</p> <code>learn</code> <p>Perform one learning step.</p> <code>save</code> <p>Save agent state to file.</p> <code>load</code> <p>Load agent state from file.</p> <p>Attributes:</p> Name Type Description <code>state_size</code> <code>action_size</code> Source code in <code>viberl/agents/base.py</code> <pre><code>def __init__(self, state_size: int, action_size: int) -&gt; None:\n    \"\"\"Initialize the agent.\n\n    Args:\n        state_size: Size of the state space\n        action_size: Size of the action space\n    \"\"\"\n    self.state_size = state_size\n    self.action_size = action_size\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.state_size","title":"state_size  <code>instance-attribute</code>","text":"<pre><code>state_size = state_size\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.action_size","title":"action_size  <code>instance-attribute</code>","text":"<pre><code>action_size = action_size\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.act","title":"act  <code>abstractmethod</code>","text":"<pre><code>act(state: ndarray, training: bool = True) -&gt; Action\n</code></pre> <p>Select an action given the current state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Current state observation</p> required <code>training</code> <code>bool</code> <p>Whether in training mode (affects exploration)</p> <code>True</code> <p>Returns:</p> Type Description <code>Action</code> <p>Action object containing the selected action and optional metadata</p> Source code in <code>viberl/agents/base.py</code> <pre><code>@abstractmethod\ndef act(self, state: np.ndarray, training: bool = True) -&gt; Action:\n    \"\"\"Select an action given the current state.\n\n    Args:\n        state: Current state observation\n        training: Whether in training mode (affects exploration)\n\n    Returns:\n        Action object containing the selected action and optional metadata\n    \"\"\"\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.learn","title":"learn  <code>abstractmethod</code>","text":"<pre><code>learn(trajectories: list[Trajectory]) -&gt; dict[str, float]\n</code></pre> <p>Perform one learning step.</p> <p>Parameters:</p> Name Type Description Default <code>trajectories</code> <code>list[Trajectory]</code> <p>List of trajectories to learn from</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary of step metrics (e.g., loss values)</p> Source code in <code>viberl/agents/base.py</code> <pre><code>@abstractmethod\ndef learn(self, trajectories: list[Trajectory]) -&gt; dict[str, float]:\n    \"\"\"Perform one learning step.\n\n    Args:\n        trajectories: List of trajectories to learn from\n\n    Returns:\n        Dictionary of step metrics (e.g., loss values)\n    \"\"\"\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(filepath: str) -&gt; None\n</code></pre> <p>Save agent state to file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the model</p> required Source code in <code>viberl/agents/base.py</code> <pre><code>@abstractmethod\ndef save(self, filepath: str) -&gt; None:\n    \"\"\"Save agent state to file.\n\n    Args:\n        filepath: Path to save the model\n    \"\"\"\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(filepath: str) -&gt; None\n</code></pre> <p>Load agent state from file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to load the model from</p> required Source code in <code>viberl/agents/base.py</code> <pre><code>@abstractmethod\ndef load(self, filepath: str) -&gt; None:\n    \"\"\"Load agent state from file.\n\n    Args:\n        filepath: Path to load the model from\n    \"\"\"\n</code></pre>"},{"location":"api/agents/dqn/","title":"<code>viberl.agents.dqn</code>","text":"<p>DQN: Deep Q-Network combining Q-learning with deep neural networks for human-level control.</p> <p>Algorithm Overview:</p> <p>DQN combines traditional Q-learning with deep neural networks to learn optimal action-value functions in environments with high-dimensional state spaces. It addresses key challenges of applying deep learning to reinforcement learning through experience replay and target networks.</p> <p>Key Concepts:</p> <ul> <li>Deep Q-Learning: Uses neural networks to approximate Q-values \\(Q(s,a;\\theta)\\)</li> <li>Experience Replay: Stores and samples experiences to break correlation between samples</li> <li>Target Network: Separate frozen network provides stable target Q-values</li> <li>Epsilon-Greedy: Balances exploration and exploitation during training</li> <li>Temporal Difference: Uses TD error for Q-value updates</li> </ul> <p>Mathematical Foundation:</p> <p>Optimization Objective:</p> \\[L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim D}\\left[\\left(r + \\gamma \\max_{a'} Q_{\\text{target}}(s',a') - Q_\\theta(s,a)\\right)^2\\right]\\] <p>Bellman Optimality Equation:</p> \\[Q^*(s,a) = \\mathbb{E}\\left[r + \\gamma \\max_{a'} Q^*(s',a')\\right]\\] <p>Reference: Mnih, V., Kavukcuoglu, K., Silver, D., et al. Human-level control through deep reinforcement learning. Nature 518, 529-533 (2015). PDF</p> <p>Classes:</p> Name Description <code>DQNAgent</code> <p>DQN agent implementation with deep Q-learning and experience replay.</p>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent","title":"DQNAgent","text":"<pre><code>DQNAgent(\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 0.001,\n    gamma: float = 0.99,\n    epsilon_start: float = 1.0,\n    epsilon_end: float = 0.01,\n    epsilon_decay: float = 0.995,\n    memory_size: int = 10000,\n    batch_size: int = 64,\n    target_update: int = 10,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n)\n</code></pre> <p>               Bases: <code>Agent</code></p> <p>DQN agent implementation with deep Q-learning and experience replay.</p> <p>This agent implements the Deep Q-Network algorithm using neural networks to approximate Q-values, with experience replay and target networks for stability.</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <code>int</code> <p>Dimension of the state space. Must be positive.</p> required <code>action_size</code> <code>int</code> <p>Number of possible actions. Must be positive.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate for the Adam optimizer. Must be positive.</p> <code>0.001</code> <code>gamma</code> <code>float</code> <p>Discount factor for future rewards. Should be in (0, 1].</p> <code>0.99</code> <code>epsilon_start</code> <code>float</code> <p>Initial exploration rate. Should be in [0, 1].</p> <code>1.0</code> <code>epsilon_end</code> <code>float</code> <p>Final exploration rate. Should be in [0, 1].</p> <code>0.01</code> <code>epsilon_decay</code> <code>float</code> <p>Decay rate for exploration. Should be in (0, 1].</p> <code>0.995</code> <code>memory_size</code> <code>int</code> <p>Size of the experience replay buffer. Must be positive.</p> <code>10000</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Must be positive.</p> <code>64</code> <code>target_update</code> <code>int</code> <p>Frequency of target network updates. Must be positive.</p> <code>10</code> <code>hidden_size</code> <code>int</code> <p>Number of neurons in each hidden layer. Must be positive.</p> <code>128</code> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers in the Q-network. Must be non-negative.</p> <code>2</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any parameter is invalid.</p> <p>Methods:</p> Name Description <code>act</code> <p>Select action using epsilon-greedy policy.</p> <code>learn</code> <p>Update Q-network using Q-learning with experience replay.</p> <code>save</code> <p>Save the agent's neural network parameters to a file.</p> <code>load</code> <p>Load the agent's neural network parameters from a file.</p> <p>Attributes:</p> Name Type Description <code>learning_rate</code> <code>gamma</code> <code>epsilon_start</code> <code>epsilon_end</code> <code>epsilon_decay</code> <code>memory_size</code> <code>batch_size</code> <code>target_update</code> <code>epsilon</code> <code>q_network</code> <code>target_network</code> <code>optimizer</code> <code>memory</code> Source code in <code>viberl/agents/dqn.py</code> <pre><code>def __init__(\n    self,\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.99,\n    epsilon_start: float = 1.0,\n    epsilon_end: float = 0.01,\n    epsilon_decay: float = 0.995,\n    memory_size: int = 10000,\n    batch_size: int = 64,\n    target_update: int = 10,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n):\n    super().__init__(state_size, action_size)\n    self.learning_rate = learning_rate\n    self.gamma = gamma\n    self.epsilon_start = epsilon_start\n    self.epsilon_end = epsilon_end\n    self.epsilon_decay = epsilon_decay\n    self.memory_size = memory_size\n    self.batch_size = batch_size\n    self.target_update = target_update\n    self.epsilon = epsilon_start\n\n    # Neural networks\n    self.q_network = QNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n    self.target_network = QNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n    self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n\n    # Copy weights to target network\n    self._update_target_network()\n\n    # Experience replay buffer\n    self.memory = deque(maxlen=memory_size)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.learning_rate","title":"learning_rate  <code>instance-attribute</code>","text":"<pre><code>learning_rate = learning_rate\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.epsilon_start","title":"epsilon_start  <code>instance-attribute</code>","text":"<pre><code>epsilon_start = epsilon_start\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.epsilon_end","title":"epsilon_end  <code>instance-attribute</code>","text":"<pre><code>epsilon_end = epsilon_end\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.epsilon_decay","title":"epsilon_decay  <code>instance-attribute</code>","text":"<pre><code>epsilon_decay = epsilon_decay\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.memory_size","title":"memory_size  <code>instance-attribute</code>","text":"<pre><code>memory_size = memory_size\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.target_update","title":"target_update  <code>instance-attribute</code>","text":"<pre><code>target_update = target_update\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.epsilon","title":"epsilon  <code>instance-attribute</code>","text":"<pre><code>epsilon = epsilon_start\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.q_network","title":"q_network  <code>instance-attribute</code>","text":"<pre><code>q_network = QNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.target_network","title":"target_network  <code>instance-attribute</code>","text":"<pre><code>target_network = QNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = Adam(parameters(), lr=learning_rate)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.memory","title":"memory  <code>instance-attribute</code>","text":"<pre><code>memory = deque(maxlen=memory_size)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.act","title":"act","text":"<pre><code>act(state: ndarray, training: bool = True) -&gt; Action\n</code></pre> <p>Select action using epsilon-greedy policy.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Current state observation.</p> required <code>training</code> <code>bool</code> <p>Whether in training mode (affects exploration).</p> <code>True</code> <p>Returns:</p> Type Description <code>Action</code> <p>Action containing the selected action.</p> Source code in <code>viberl/agents/dqn.py</code> <pre><code>def act(self, state: np.ndarray, training: bool = True) -&gt; Action:\n    \"\"\"Select action using epsilon-greedy policy.\n\n    Args:\n        state: Current state observation.\n        training: Whether in training mode (affects exploration).\n\n    Returns:\n        Action containing the selected action.\n    \"\"\"\n    if training and random.random() &lt; self.epsilon:\n        action = random.randint(0, self.action_size - 1)\n    else:\n        with torch.no_grad():\n            q_values = self.q_network.get_q_values(state)\n            action = q_values.argmax().item()\n\n    return Action(action=action)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.learn","title":"learn","text":"<pre><code>learn(trajectories: list[Trajectory]) -&gt; dict[str, float]\n</code></pre> <p>Update Q-network using Q-learning with experience replay.</p> <p>Parameters:</p> Name Type Description Default <code>trajectories</code> <code>list[Trajectory]</code> <p>List of trajectories to learn from</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary containing loss, epsilon, and memory size.</p> Source code in <code>viberl/agents/dqn.py</code> <pre><code>def learn(self, trajectories: list[Trajectory]) -&gt; dict[str, float]:\n    \"\"\"Update Q-network using Q-learning with experience replay.\n\n    Args:\n        trajectories: List of trajectories to learn from\n\n    Returns:\n        Dictionary containing loss, epsilon, and memory size.\n    \"\"\"\n    if not trajectories:\n        return {}\n\n    # Store all transitions from all trajectories in memory\n    transitions_added = 0\n    for trajectory in trajectories:\n        for transition in trajectory.transitions:\n            self.memory.append(transition)\n            transitions_added += 1\n\n    if len(self.memory) &lt; self.batch_size:\n        return {\n            'dqn/memory_size': len(self.memory),\n            'dqn/transitions_added': transitions_added,\n            'dqn/batch_size': len(trajectories),\n        }\n\n    # Sample batch from memory\n    batch = random.sample(self.memory, self.batch_size)\n\n    # Extract batch data\n    states = torch.FloatTensor([t.state for t in batch])\n    actions = torch.LongTensor([t.action.action for t in batch])\n    rewards = torch.FloatTensor([t.reward for t in batch])\n    next_states = torch.FloatTensor([t.next_state for t in batch])\n    dones = torch.BoolTensor([t.done for t in batch])\n\n    # Current Q values\n    current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n\n    # Next Q values from target network\n    with torch.no_grad():\n        next_q_values = self.target_network(next_states).max(1)[0]\n        target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n\n    # Compute loss\n    loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n    # Optimize\n    self.optimizer.zero_grad()\n    loss.backward()\n    self.optimizer.step()\n\n    # Update target network\n    self._update_target_network()\n\n    # Decay epsilon\n    self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n\n    return {\n        'dqn/loss': loss.item(),\n        'dqn/epsilon': self.epsilon,\n        'dqn/memory_size': len(self.memory),\n        'dqn/transitions_added': transitions_added,\n        'dqn/batch_size': len(trajectories),\n    }\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.save","title":"save","text":"<pre><code>save(filepath: str) -&gt; None\n</code></pre> <p>Save the agent's neural network parameters to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where to save the model</p> required Source code in <code>viberl/agents/dqn.py</code> <pre><code>def save(self, filepath: str) -&gt; None:\n    \"\"\"Save the agent's neural network parameters to a file.\n\n    Args:\n        filepath: Path where to save the model\n    \"\"\"\n    torch.save(\n        {\n            'q_network': self.q_network.state_dict(),\n            'target_network': self.target_network.state_dict(),\n        },\n        filepath,\n    )\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.load","title":"load","text":"<pre><code>load(filepath: str) -&gt; None\n</code></pre> <p>Load the agent's neural network parameters from a file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path from which to load the model</p> required Source code in <code>viberl/agents/dqn.py</code> <pre><code>def load(self, filepath: str) -&gt; None:\n    \"\"\"Load the agent's neural network parameters from a file.\n\n    Args:\n        filepath: Path from which to load the model\n    \"\"\"\n    checkpoint = torch.load(filepath, map_location='cpu')\n    self.q_network.load_state_dict(checkpoint['q_network'])\n    self.target_network.load_state_dict(checkpoint['target_network'])\n</code></pre>"},{"location":"api/agents/ppo/","title":"<code>viberl.agents.ppo</code>","text":"<p>PPO: Proximal Policy Optimization for stable policy gradient updates.</p> <p>Algorithm Overview:</p> <p>PPO is a policy gradient method that prevents large policy updates through a clipped surrogate objective, making training more stable and reliable while maintaining sample efficiency.</p> <p>Key Concepts:</p> <ul> <li>Clipped Surrogate Objective: Prevents destructive policy updates</li> <li>Generalized Advantage Estimation (GAE): Computes stable advantage estimates</li> <li>Multiple PPO Epochs: Reuses collected data efficiently</li> <li>Policy Network: \\(\\pi_\\theta(a|s)\\) for action selection</li> <li>Value Network: \\(V_\\phi(s)\\) for baseline estimation</li> </ul> <p>Mathematical Foundation:</p> <p>Optimization Objective:</p> \\[L^{CLIP}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]\\] <p>Advantage Function:</p> \\[A_t = \\delta_t + (\\gamma\\lambda) \\delta_{t+1} + (\\gamma\\lambda)^2 \\delta_{t+2} + \\dots\\] <p>Reference: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. Proximal Policy Optimization Algorithms. arXiv preprint arXiv:1707.06347 (2017). PDF</p> <p>Classes:</p> Name Description <code>PPOAgent</code> <p>PPO agent implementation with clipped surrogate objective and GAE.</p>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent","title":"PPOAgent","text":"<pre><code>PPOAgent(\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 0.0003,\n    gamma: float = 0.99,\n    lam: float = 0.95,\n    clip_epsilon: float = 0.2,\n    value_loss_coef: float = 0.5,\n    entropy_coef: float = 0.01,\n    max_grad_norm: float = 0.5,\n    ppo_epochs: int = 4,\n    batch_size: int = 64,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n    device: str = 'auto',\n)\n</code></pre> <p>               Bases: <code>Agent</code></p> <p>PPO agent implementation with clipped surrogate objective and GAE.</p> <p>This agent implements Proximal Policy Optimization using a clipped surrogate objective to prevent large policy updates, along with Generalized Advantage Estimation for stable advantage computation.</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <code>int</code> <p>Dimension of the state space. Must be positive.</p> required <code>action_size</code> <code>int</code> <p>Number of possible actions. Must be positive.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate for the Adam optimizer. Must be positive.</p> <code>0.0003</code> <code>gamma</code> <code>float</code> <p>Discount factor for future rewards. Should be in (0, 1].</p> <code>0.99</code> <code>lam</code> <code>float</code> <p>GAE lambda parameter for advantage computation. Should be in [0, 1].</p> <code>0.95</code> <code>clip_epsilon</code> <code>float</code> <p>PPO clipping parameter. Should be positive.</p> <code>0.2</code> <code>value_loss_coef</code> <code>float</code> <p>Coefficient for value loss. Should be positive.</p> <code>0.5</code> <code>entropy_coef</code> <code>float</code> <p>Coefficient for entropy bonus. Should be positive.</p> <code>0.01</code> <code>max_grad_norm</code> <code>float</code> <p>Maximum gradient norm for clipping. Should be positive.</p> <code>0.5</code> <code>ppo_epochs</code> <code>int</code> <p>Number of PPO epochs per update. Must be positive.</p> <code>4</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Must be positive.</p> <code>64</code> <code>hidden_size</code> <code>int</code> <p>Number of neurons in each hidden layer. Must be positive.</p> <code>128</code> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers. Must be non-negative.</p> <code>2</code> <code>device</code> <code>str</code> <p>Device for computation ('auto', 'cpu', or 'cuda').</p> <code>'auto'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any parameter is invalid.</p> <p>Methods:</p> Name Description <code>act</code> <p>Select action using policy \\(\\pi(a|s;\\theta)\\).</p> <code>learn</code> <p>Update policy and value networks using PPO clipped objective.</p> <code>save</code> <p>Save the agent's neural network parameters to a file.</p> <code>load</code> <p>Load the agent's neural network parameters from a file.</p> <p>Attributes:</p> Name Type Description <code>gamma</code> <code>lam</code> <code>clip_epsilon</code> <code>value_loss_coef</code> <code>entropy_coef</code> <code>max_grad_norm</code> <code>ppo_epochs</code> <code>batch_size</code> <code>device</code> <code>policy_network</code> <code>value_network</code> <code>optimizer</code> Source code in <code>viberl/agents/ppo.py</code> <pre><code>def __init__(\n    self,\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 3e-4,\n    gamma: float = 0.99,\n    lam: float = 0.95,\n    clip_epsilon: float = 0.2,\n    value_loss_coef: float = 0.5,\n    entropy_coef: float = 0.01,\n    max_grad_norm: float = 0.5,\n    ppo_epochs: int = 4,\n    batch_size: int = 64,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n    device: str = 'auto',\n):\n    super().__init__(state_size, action_size)\n    self.gamma = gamma\n    self.lam = lam\n    self.clip_epsilon = clip_epsilon\n    self.value_loss_coef = value_loss_coef\n    self.entropy_coef = entropy_coef\n    self.max_grad_norm = max_grad_norm\n    self.ppo_epochs = ppo_epochs\n    self.batch_size = batch_size\n\n    # Set device\n    if device == 'auto':\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    else:\n        self.device = torch.device(device)\n\n    # Initialize networks\n    self.policy_network = PolicyNetwork(\n        state_size=state_size,\n        action_size=action_size,\n        hidden_size=hidden_size,\n        num_hidden_layers=num_hidden_layers,\n    ).to(self.device)\n\n    self.value_network = VNetwork(\n        state_size=state_size,\n        hidden_size=hidden_size,\n        num_hidden_layers=num_hidden_layers,\n    ).to(self.device)\n\n    # Initialize optimizer\n    self.optimizer = torch.optim.Adam(\n        list(self.policy_network.parameters()) + list(self.value_network.parameters()),\n        lr=learning_rate,\n    )\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.lam","title":"lam  <code>instance-attribute</code>","text":"<pre><code>lam = lam\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.clip_epsilon","title":"clip_epsilon  <code>instance-attribute</code>","text":"<pre><code>clip_epsilon = clip_epsilon\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.value_loss_coef","title":"value_loss_coef  <code>instance-attribute</code>","text":"<pre><code>value_loss_coef = value_loss_coef\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.entropy_coef","title":"entropy_coef  <code>instance-attribute</code>","text":"<pre><code>entropy_coef = entropy_coef\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.max_grad_norm","title":"max_grad_norm  <code>instance-attribute</code>","text":"<pre><code>max_grad_norm = max_grad_norm\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.ppo_epochs","title":"ppo_epochs  <code>instance-attribute</code>","text":"<pre><code>ppo_epochs = ppo_epochs\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device('cuda' if is_available() else 'cpu')\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.policy_network","title":"policy_network  <code>instance-attribute</code>","text":"<pre><code>policy_network = to(device)\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.value_network","title":"value_network  <code>instance-attribute</code>","text":"<pre><code>value_network = to(device)\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = Adam(list(parameters()) + list(parameters()), lr=learning_rate)\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.act","title":"act","text":"<pre><code>act(state: ndarray, training: bool = True) -&gt; Action\n</code></pre> <p>Select action using policy \\(\\pi(a|s;\\theta)\\).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Current state observation.</p> required <code>training</code> <code>bool</code> <p>Whether in training mode (affects exploration).</p> <code>True</code> <p>Returns:</p> Type Description <code>Action</code> <p>Action containing the selected action.</p> Source code in <code>viberl/agents/ppo.py</code> <pre><code>def act(self, state: np.ndarray, training: bool = True) -&gt; Action:\n    r\"\"\"Select action using policy $\\pi(a|s;\\theta)$.\n\n    Args:\n        state: Current state observation.\n        training: Whether in training mode (affects exploration).\n\n    Returns:\n        Action containing the selected action.\n    \"\"\"\n    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n    with torch.no_grad():\n        action_probs = self.policy_network(state_tensor)\n        dist = Categorical(action_probs)\n\n        if training:\n            # Training mode: sample from policy distribution\n            action = dist.sample()\n            log_prob = dist.log_prob(action)\n            return Action(action=action.item(), logprobs=log_prob)\n        else:\n            # Evaluation mode: select most likely action (greedy)\n            action = action_probs.argmax().item()\n            return Action(action=action)\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.learn","title":"learn","text":"<pre><code>learn(trajectories: list[Trajectory]) -&gt; dict[str, float]\n</code></pre> <p>Update policy and value networks using PPO clipped objective.</p> <p>Parameters:</p> Name Type Description Default <code>trajectories</code> <code>list[Trajectory]</code> <p>List of trajectories to learn from</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary containing policy loss, value loss, and total loss.</p> Source code in <code>viberl/agents/ppo.py</code> <pre><code>def learn(self, trajectories: list[Trajectory]) -&gt; dict[str, float]:\n    \"\"\"Update policy and value networks using PPO clipped objective.\n\n    Args:\n        trajectories: List of trajectories to learn from\n\n    Returns:\n        Dictionary containing policy loss, value loss, and total loss.\n    \"\"\"\n    if not trajectories:\n        return {}\n\n    # Collect all data from all trajectories\n    all_states = []\n    all_actions = []\n    all_rewards = []\n    all_log_probs = []\n    all_dones = []\n    all_values = []\n\n    for trajectory in trajectories:\n        if not trajectory.transitions:\n            continue\n\n        # Extract data from trajectory\n        states = [t.state for t in trajectory.transitions]\n        actions = [t.action.action for t in trajectory.transitions]\n        rewards = [t.reward for t in trajectory.transitions]\n        log_probs = [\n            t.action.logprobs.item() if t.action.logprobs is not None else 0.0\n            for t in trajectory.transitions\n        ]\n        dones = [t.done for t in trajectory.transitions]\n\n        # Compute values for each state\n        values = []\n        for state in states:\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            with torch.no_grad():\n                value = self.value_network(state_tensor).squeeze(-1).item()\n                values.append(value)\n\n        all_states.extend(states)\n        all_actions.extend(actions)\n        all_rewards.extend(rewards)\n        all_log_probs.extend(log_probs)\n        all_dones.extend(dones)\n        all_values.extend(values)\n\n    if not all_states:\n        return {}\n\n    # Convert to tensors\n    states_tensor = torch.FloatTensor(np.array(all_states)).to(self.device)\n    actions_tensor = torch.LongTensor(all_actions).to(self.device)\n    old_log_probs_tensor = torch.FloatTensor(all_log_probs).to(self.device)\n\n    # Compute advantages and returns\n    advantages, returns = self._compute_gae(all_rewards, all_values, all_dones)\n    advantages_tensor = torch.FloatTensor(advantages).to(self.device)\n    returns_tensor = torch.FloatTensor(returns).to(self.device)\n\n    # Normalize advantages (handle small sample sizes)\n    if len(advantages_tensor) &gt; 1:\n        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (\n            advantages_tensor.std() + 1e-8\n        )\n    else:\n        advantages_tensor = advantages_tensor - advantages_tensor.mean()\n\n    # Create dataset\n    dataset_size = len(all_states)\n    indices = np.arange(dataset_size)\n\n    metrics = {\n        'ppo/policy_loss': 0.0,\n        'ppo/value_loss': 0.0,\n        'ppo/entropy_loss': 0.0,\n        'ppo/total_loss': 0.0,\n        'ppo/batch_size': len(trajectories),\n    }\n\n    # PPO epochs\n    for _epoch in range(self.ppo_epochs):\n        np.random.shuffle(indices)\n\n        for start in range(0, dataset_size, self.batch_size):\n            end = start + self.batch_size\n            batch_indices = indices[start:end]\n\n            batch_states = states_tensor[batch_indices]\n            batch_actions = actions_tensor[batch_indices]\n            batch_old_log_probs = old_log_probs_tensor[batch_indices]\n            batch_advantages = advantages_tensor[batch_indices]\n            batch_returns = returns_tensor[batch_indices]\n\n            # Forward pass\n            action_probs = self.policy_network(batch_states)\n            # Ensure action_probs are valid probabilities\n            action_probs = torch.clamp(action_probs, 1e-8, 1 - 1e-8)\n            action_probs = action_probs / action_probs.sum(dim=1, keepdim=True)\n\n            values = self.value_network(batch_states).squeeze(-1)\n\n            dist = Categorical(action_probs)\n            new_log_probs = dist.log_prob(batch_actions)\n            entropy = dist.entropy()\n\n            # Compute ratio for PPO\n            ratio = torch.exp(new_log_probs - batch_old_log_probs)\n\n            # Clipped surrogate objective\n            surr1 = ratio * batch_advantages\n            surr2 = (\n                torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n                * batch_advantages\n            )\n            policy_loss = -torch.min(surr1, surr2).mean()\n\n            # Value loss\n            value_loss = nn.MSELoss()(values.squeeze(), batch_returns.squeeze())\n\n            # Entropy loss\n            entropy_loss = -entropy.mean()\n\n            # Total loss\n            total_loss = (\n                policy_loss\n                + self.value_loss_coef * value_loss\n                + self.entropy_coef * entropy_loss\n            )\n\n            # Update networks\n            self.optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(\n                list(self.policy_network.parameters()) + list(self.value_network.parameters()),\n                self.max_grad_norm,\n            )\n            self.optimizer.step()\n\n            # Accumulate metrics\n            metrics['ppo/policy_loss'] += policy_loss.item()\n            metrics['ppo/value_loss'] += value_loss.item()\n            metrics['ppo/entropy_loss'] += entropy_loss.item()\n            metrics['ppo/total_loss'] += total_loss.item()\n\n    # Average metrics over all batches and epochs\n    num_batches = (dataset_size + self.batch_size - 1) // self.batch_size\n    for key in ['ppo/policy_loss', 'ppo/value_loss', 'ppo/entropy_loss', 'ppo/total_loss']:\n        metrics[key] /= num_batches * self.ppo_epochs\n\n    return metrics\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.save","title":"save","text":"<pre><code>save(filepath: str) -&gt; None\n</code></pre> <p>Save the agent's neural network parameters to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where to save the model</p> required Source code in <code>viberl/agents/ppo.py</code> <pre><code>def save(self, filepath: str) -&gt; None:\n    \"\"\"Save the agent's neural network parameters to a file.\n\n    Args:\n        filepath: Path where to save the model\n    \"\"\"\n    torch.save(\n        {\n            'policy_network': self.policy_network.state_dict(),\n            'value_network': self.value_network.state_dict(),\n        },\n        filepath,\n    )\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.load","title":"load","text":"<pre><code>load(filepath: str) -&gt; None\n</code></pre> <p>Load the agent's neural network parameters from a file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path from which to load the model</p> required Source code in <code>viberl/agents/ppo.py</code> <pre><code>def load(self, filepath: str) -&gt; None:\n    \"\"\"Load the agent's neural network parameters from a file.\n\n    Args:\n        filepath: Path from which to load the model\n    \"\"\"\n    checkpoint = torch.load(filepath, map_location='cpu')\n    self.policy_network.load_state_dict(checkpoint['policy_network'])\n    self.value_network.load_state_dict(checkpoint['value_network'])\n</code></pre>"},{"location":"api/agents/reinforce/","title":"<code>viberl.agents.reinforce</code>","text":"<p>REINFORCE: Monte-Carlo policy gradient method for reinforcement learning.</p> <p>Algorithm Overview:</p> <p>REINFORCE is a policy gradient method that uses complete episode returns to update the policy parameters. It directly optimizes the policy without requiring a value function, making it conceptually simple but potentially high-variance.</p> <p>Key Concepts:</p> <ul> <li>Policy Gradient: Directly optimizes policy parameters \\(\\theta\\) to maximize expected return</li> <li>Likelihood Ratio Trick: Uses \\(\\nabla_\\theta \\log \\pi_\\theta(a|s)\\) for gradient computation</li> <li>Monte-Carlo Returns: Uses complete episode returns \\(G_t\\) for unbiased estimates</li> <li>High Variance: Large variance in gradient estimates but unbiased</li> <li>Episode-based Learning: Requires complete episodes before parameter updates</li> </ul> <p>Mathematical Foundation:</p> <p>Optimization Objective:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}\\left[\\sum_{t=0}^{T} \\log \\pi_{\\theta}(a_t|s_t) G_t\\right]\\] <p>Return Calculation:</p> \\[G_t = \\sum_{k=0}^{T-t} \\gamma^k r_{t+k}\\] <p>Reference: Williams, R.J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 229-256 (1992). PDF</p> <p>Classes:</p> Name Description <code>REINFORCEAgent</code> <p>REINFORCE agent implementation with policy gradient optimization.</p>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent","title":"REINFORCEAgent","text":"<pre><code>REINFORCEAgent(\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 0.001,\n    gamma: float = 0.99,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n)\n</code></pre> <p>               Bases: <code>Agent</code></p> <p>REINFORCE agent implementation with policy gradient optimization.</p> <p>This agent implements the REINFORCE algorithm using a policy network to directly optimize the policy parameters via Monte-Carlo gradient estimates.</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <code>int</code> <p>Dimension of the state space. Must be positive.</p> required <code>action_size</code> <code>int</code> <p>Number of possible actions. Must be positive.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate for the Adam optimizer. Must be positive.</p> <code>0.001</code> <code>gamma</code> <code>float</code> <p>Discount factor for future rewards. Should be in (0, 1].</p> <code>0.99</code> <code>hidden_size</code> <code>int</code> <p>Number of neurons in each hidden layer. Must be positive.</p> <code>128</code> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers. Must be non-negative.</p> <code>2</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any parameter is invalid (e.g., negative dimensions).</p> <p>Initialize the REINFORCE agent.</p> <p>Methods:</p> Name Description <code>act</code> <p>Select action using the current policy.</p> <code>learn</code> <p>Update policy parameters using the REINFORCE gradient algorithm.</p> <code>save</code> <p>Save the agent's policy network parameters to a file.</p> <code>load</code> <p>Load the agent's policy network parameters from a file.</p> <p>Attributes:</p> Name Type Description <code>gamma</code> <code>policy_network</code> <code>optimizer</code> Source code in <code>viberl/agents/reinforce.py</code> <pre><code>def __init__(\n    self,\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.99,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n):\n    \"\"\"Initialize the REINFORCE agent.\"\"\"\n    super().__init__(state_size, action_size)\n\n    if state_size &lt;= 0:\n        raise ValueError(f'state_size must be positive, got {state_size}')\n    if action_size &lt;= 0:\n        raise ValueError(f'action_size must be positive, got {action_size}')\n    if learning_rate &lt;= 0:\n        raise ValueError(f'learning_rate must be positive, got {learning_rate}')\n    if not 0 &lt; gamma &lt;= 1:\n        raise ValueError(f'gamma must be in (0, 1], got {gamma}')\n    if hidden_size &lt;= 0:\n        raise ValueError(f'hidden_size must be positive, got {hidden_size}')\n    if num_hidden_layers &lt; 0:\n        raise ValueError(f'num_hidden_layers must be non-negative, got {num_hidden_layers}')\n\n    self.gamma = gamma\n    self.policy_network = PolicyNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n    self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.policy_network","title":"policy_network  <code>instance-attribute</code>","text":"<pre><code>policy_network = PolicyNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = Adam(parameters(), lr=learning_rate)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.act","title":"act","text":"<pre><code>act(state: ndarray, training: bool = True) -&gt; Action\n</code></pre> <p>Select action using the current policy.</p> <p>Uses the policy network to compute action probabilities and selects an action based on the current mode (training vs evaluation).</p> <p>In training mode, samples from the policy distribution to ensure exploration. In evaluation mode, selects the most probable action (greedy selection).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Current state observation as a numpy array. Should have shape (state_size,) or be convertible to a tensor of that shape.</p> required <code>training</code> <code>bool</code> <p>Whether in training mode. If True, samples from the policy distribution for exploration. If False, selects the action with highest probability (greedy selection).</p> <code>True</code> <p>Returns:</p> Type Description <code>Action</code> <p>Action containing the selected action index as an integer.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the state has incorrect shape or type.</p> <code>RuntimeError</code> <p>If there's an error during action computation.</p> Source code in <code>viberl/agents/reinforce.py</code> <pre><code>def act(self, state: np.ndarray, training: bool = True) -&gt; Action:\n    \"\"\"Select action using the current policy.\n\n    Uses the policy network to compute action probabilities and selects\n    an action based on the current mode (training vs evaluation).\n\n    In training mode, samples from the policy distribution to ensure\n    exploration. In evaluation mode, selects the most probable action\n    (greedy selection).\n\n    Args:\n        state: Current state observation as a numpy array. Should have shape\n            (state_size,) or be convertible to a tensor of that shape.\n        training: Whether in training mode. If True, samples from the policy\n            distribution for exploration. If False, selects the action with\n            highest probability (greedy selection).\n\n    Returns:\n        Action containing the selected action index as an integer.\n\n    Raises:\n        ValueError: If the state has incorrect shape or type.\n        RuntimeError: If there's an error during action computation.\n    \"\"\"\n    if training:\n        # Training mode: sample from policy distribution\n        action = self.policy_network.act(state)\n    else:\n        # Evaluation mode: select most likely action (greedy)\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        with torch.no_grad():\n            action_probs = self.policy_network(state_tensor)\n            action = action_probs.argmax().item()\n\n    return Action(action=action)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.learn","title":"learn","text":"<pre><code>learn(trajectories: list[Trajectory]) -&gt; dict[str, float]\n</code></pre> <p>Update policy parameters using the REINFORCE gradient algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>trajectories</code> <code>list[Trajectory]</code> <p>List of trajectories to learn from</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary containing training metrics:</p> <code>dict[str, float]</code> <ul> <li>'reinforce/policy_loss': Policy loss value (float)</li> </ul> <code>dict[str, float]</code> <ul> <li>'reinforce/return_mean': Mean of normalized returns (float)</li> </ul> <code>dict[str, float]</code> <ul> <li>'reinforce/batch_size': Number of trajectories in batch (int)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no trajectories are provided or contains invalid data.</p> <code>RuntimeError</code> <p>If there's an error during gradient computation.</p> Source code in <code>viberl/agents/reinforce.py</code> <pre><code>def learn(self, trajectories: list[Trajectory]) -&gt; dict[str, float]:\n    \"\"\"Update policy parameters using the REINFORCE gradient algorithm.\n\n    Args:\n        trajectories: List of trajectories to learn from\n\n    Returns:\n        Dictionary containing training metrics:\n        - 'reinforce/policy_loss': Policy loss value (float)\n        - 'reinforce/return_mean': Mean of normalized returns (float)\n        - 'reinforce/batch_size': Number of trajectories in batch (int)\n\n    Raises:\n        ValueError: If no trajectories are provided or contains invalid data.\n        RuntimeError: If there's an error during gradient computation.\n    \"\"\"\n    if not trajectories:\n        return {}\n\n    # Collect all data from all trajectories\n    all_states = []\n    all_actions = []\n    all_returns = []\n\n    for trajectory in trajectories:\n        if not trajectory.transitions:\n            continue\n\n        # Extract data from trajectory\n        states = [t.state for t in trajectory.transitions]\n        actions = [t.action.action for t in trajectory.transitions]\n        rewards = [t.reward for t in trajectory.transitions]\n\n        # Compute returns for this trajectory\n        returns = self._compute_returns(rewards)\n\n        # Normalize returns for this trajectory\n        returns = torch.FloatTensor(returns)\n        if returns.std() &gt; 0:\n            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n\n        all_states.extend(states)\n        all_actions.extend(actions)\n        all_returns.extend(returns.tolist())\n\n    if not all_states:\n        return {}\n\n    # Convert to tensors\n    states_tensor = torch.FloatTensor(np.array(all_states))\n    actions_tensor = torch.LongTensor(all_actions)\n    returns_tensor = torch.FloatTensor(all_returns)\n\n    # Get action probabilities\n    action_probs = self.policy_network(states_tensor)\n\n    # Compute loss across all trajectories\n    m = Categorical(action_probs)\n    log_probs = m.log_prob(actions_tensor)\n    loss = -torch.mean(log_probs * returns_tensor)\n\n    # Update policy\n    self.optimizer.zero_grad()\n    loss.backward()\n    self.optimizer.step()\n\n    return {\n        'reinforce/policy_loss': loss.item(),\n        'reinforce/return_mean': returns_tensor.mean().item(),\n        'reinforce/batch_size': len(trajectories),\n    }\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.save","title":"save","text":"<pre><code>save(filepath: str) -&gt; None\n</code></pre> <p>Save the agent's policy network parameters to a file.</p> <p>Saves the complete state of the policy network, including all parameters and buffers. The saved file can be loaded later to restore the exact same policy.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where to save the model. Should include the .pth extension. The directory will be created if it doesn't exist.</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If there's an error writing to the file.</p> <code>ValueError</code> <p>If filepath is empty or invalid.</p> Example <p>agent.save('models/reinforce_policy.pth')</p> Source code in <code>viberl/agents/reinforce.py</code> <pre><code>def save(self, filepath: str) -&gt; None:\n    \"\"\"Save the agent's policy network parameters to a file.\n\n    Saves the complete state of the policy network, including all parameters\n    and buffers. The saved file can be loaded later to restore the exact\n    same policy.\n\n    Args:\n        filepath: Path where to save the model. Should include the .pth extension.\n            The directory will be created if it doesn't exist.\n\n    Raises:\n        IOError: If there's an error writing to the file.\n        ValueError: If filepath is empty or invalid.\n\n    Example:\n        &gt;&gt;&gt; agent.save('models/reinforce_policy.pth')\n        &gt;&gt;&gt; # Later: agent.load('models/reinforce_policy.pth')\n    \"\"\"\n    if not filepath:\n        raise ValueError('filepath cannot be empty')\n\n    # Ensure directory exists\n    import os\n\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    torch.save(self.policy_network.state_dict(), filepath)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.save--later-agentloadmodelsreinforce_policypth","title":"Later: agent.load('models/reinforce_policy.pth')","text":""},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.load","title":"load","text":"<pre><code>load(filepath: str) -&gt; None\n</code></pre> <p>Load the agent's policy network parameters from a file.</p> <p>Loads previously saved policy network parameters from disk. Can be used to restore an agent to a previously saved state.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path from which to load the model. Should point to a .pth file created by the save() method.</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If the file doesn't exist or can't be read.</p> <code>ValueError</code> <p>If filepath is empty or the file contains invalid data.</p> <code>RuntimeError</code> <p>If there's a mismatch between saved and current network architecture.</p> Example <p>agent.load('models/reinforce_policy.pth')</p> Source code in <code>viberl/agents/reinforce.py</code> <pre><code>def load(self, filepath: str) -&gt; None:\n    \"\"\"Load the agent's policy network parameters from a file.\n\n    Loads previously saved policy network parameters from disk. Can be used\n    to restore an agent to a previously saved state.\n\n    Args:\n        filepath: Path from which to load the model. Should point to a .pth file\n            created by the save() method.\n\n    Raises:\n        IOError: If the file doesn't exist or can't be read.\n        ValueError: If filepath is empty or the file contains invalid data.\n        RuntimeError: If there's a mismatch between saved and current network architecture.\n\n    Example:\n        &gt;&gt;&gt; agent.load('models/reinforce_policy.pth')\n        &gt;&gt;&gt; # Agent is now restored to the saved state\n    \"\"\"\n    if not filepath:\n        raise ValueError('filepath cannot be empty')\n\n    if not os.path.exists(filepath):\n        raise OSError(f'File not found: {filepath}')\n\n    state_dict = torch.load(filepath, map_location='cpu')\n\n    # Verify compatibility\n    current_params = self.policy_network.state_dict()\n    if state_dict.keys() != current_params.keys():\n        raise RuntimeError(\n            'Network architecture mismatch: saved model has different parameters'\n        )\n\n    self.policy_network.load_state_dict(state_dict)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.load--agent-is-now-restored-to-the-saved-state","title":"Agent is now restored to the saved state","text":""},{"location":"api/envs/grid_world/snake_env/","title":"<code>viberl.envs.grid_world.snake_env</code>","text":"<p>Classes:</p> Name Description <code>Direction</code> <code>SnakeGameEnv</code>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction","title":"Direction","text":"<p>               Bases: <code>Enum</code></p> <p>Attributes:</p> Name Type Description <code>UP</code> <code>RIGHT</code> <code>DOWN</code> <code>LEFT</code>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction.UP","title":"UP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UP = 0\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction.RIGHT","title":"RIGHT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RIGHT = 1\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction.DOWN","title":"DOWN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DOWN = 2\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction.LEFT","title":"LEFT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LEFT = 3\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv","title":"SnakeGameEnv","text":"<pre><code>SnakeGameEnv(render_mode: str | None = None, grid_size: int = 20)\n</code></pre> <p>               Bases: <code>Env</code></p> <p>Methods:</p> Name Description <code>reset</code> <code>step</code> <code>render</code> <code>close</code> <p>Attributes:</p> Name Type Description <code>metadata</code> <code>dict[str, int | list[str]]</code> <code>grid_size</code> <code>render_mode</code> <code>action_space</code> <code>observation_space</code> <code>window</code> <code>clock</code> <code>cell_size</code> <code>window_size</code> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def __init__(self, render_mode: str | None = None, grid_size: int = 20):\n    super().__init__()\n\n    self.grid_size = grid_size\n    self.render_mode = render_mode\n\n    # Action space: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n    self.action_space = spaces.Discrete(4)\n\n    # Observation space: grid with snake body, food, and empty spaces\n    self.observation_space = spaces.Box(\n        low=0, high=3, shape=(grid_size, grid_size), dtype=np.uint8\n    )\n\n    # Initialize game state\n    self.reset()\n\n    # Pygame setup for rendering\n    self.window = None\n    self.clock = None\n    self.cell_size = 20\n    self.window_size = self.grid_size * self.cell_size\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.metadata","title":"metadata  <code>class-attribute</code>","text":"<pre><code>metadata: dict[str, int | list[str]] = {'render_modes': ['human', 'rgb_array'], 'render_fps': 10}\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.grid_size","title":"grid_size  <code>instance-attribute</code>","text":"<pre><code>grid_size = grid_size\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.render_mode","title":"render_mode  <code>instance-attribute</code>","text":"<pre><code>render_mode = render_mode\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.action_space","title":"action_space  <code>instance-attribute</code>","text":"<pre><code>action_space = Discrete(4)\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.observation_space","title":"observation_space  <code>instance-attribute</code>","text":"<pre><code>observation_space = Box(low=0, high=3, shape=(grid_size, grid_size), dtype=uint8)\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.window","title":"window  <code>instance-attribute</code>","text":"<pre><code>window = None\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.clock","title":"clock  <code>instance-attribute</code>","text":"<pre><code>clock = None\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.cell_size","title":"cell_size  <code>instance-attribute</code>","text":"<pre><code>cell_size = 20\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = grid_size * cell_size\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.reset","title":"reset","text":"<pre><code>reset(seed: int | None = None, options: dict | None = None) -&gt; tuple[ndarray, dict[str, Any]]\n</code></pre> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def reset(\n    self, seed: int | None = None, options: dict | None = None\n) -&gt; tuple[np.ndarray, dict[str, Any]]:\n    super().reset(seed=seed)\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initialize snake at center of grid\n    center = self.grid_size // 2\n    self.snake = [(center - 1, center), (center, center), (center + 1, center)]\n    self.direction = Direction.RIGHT\n\n    # Place food\n    self.food = self._place_food()\n\n    # Game state\n    self.game_over = False\n    self.score = 0\n    self.steps = 0\n    self.max_steps = self.grid_size * self.grid_size * 4\n\n    return self._get_observation(), self._get_info()\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.step","title":"step","text":"<pre><code>step(action: int) -&gt; tuple[ndarray, float, bool, bool, dict[str, Any]]\n</code></pre> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def step(self, action: int) -&gt; tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n    if self.game_over:\n        return self._get_observation(), 0.0, True, False, self._get_info()\n\n    self.steps += 1\n\n    # Convert action to direction (ensure snake doesn't reverse into itself)\n    new_direction = Direction(action)\n\n    # Prevent moving directly opposite to current direction\n    if (\n        (new_direction == Direction.UP and self.direction == Direction.DOWN)\n        or (new_direction == Direction.DOWN and self.direction == Direction.UP)\n        or (new_direction == Direction.LEFT and self.direction == Direction.RIGHT)\n        or (new_direction == Direction.RIGHT and self.direction == Direction.LEFT)\n    ):\n        new_direction = self.direction\n\n    self.direction = new_direction\n\n    # Move snake\n    head_x, head_y = self.snake[-1]\n\n    if self.direction == Direction.UP:\n        new_head = (head_x - 1, head_y)\n    elif self.direction == Direction.RIGHT:\n        new_head = (head_x, head_y + 1)\n    elif self.direction == Direction.DOWN:\n        new_head = (head_x + 1, head_y)\n    elif self.direction == Direction.LEFT:\n        new_head = (head_x, head_y - 1)\n\n    # Check collision with walls\n    if (\n        new_head[0] &lt; 0\n        or new_head[0] &gt;= self.grid_size\n        or new_head[1] &lt; 0\n        or new_head[1] &gt;= self.grid_size\n    ):\n        self.game_over = True\n        return self._get_observation(), -10.0, True, False, self._get_info()\n\n    # Check collision with self\n    if new_head in self.snake[:-1]:\n        self.game_over = True\n        return self._get_observation(), -10.0, True, False, self._get_info()\n\n    # Move snake\n    self.snake.append(new_head)\n\n    reward = 0.0\n\n    # Check if food eaten\n    if new_head == self.food:\n        self.score += 1\n        reward = 10.0\n        self.food = self._place_food()\n    else:\n        # Remove tail if no food eaten\n        self.snake.pop(0)\n\n    # Give negative reward to encourage faster completion\n    reward += -0.1\n\n    # Check if maximum steps reached\n    if self.steps &gt;= self.max_steps:\n        self.game_over = True\n        return self._get_observation(), reward, True, False, self._get_info()\n\n    return self._get_observation(), reward, False, False, self._get_info()\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.render","title":"render","text":"<pre><code>render() -&gt; None | ndarray\n</code></pre> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def render(self) -&gt; None | np.ndarray:\n    if self.render_mode is None:\n        return None\n\n    if self.render_mode == 'rgb_array':\n        return self._render_frame()\n    if self.render_mode == 'human':\n        self._render_frame()\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.close","title":"close","text":"<pre><code>close()\n</code></pre> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def close(self):\n    if self.window is not None:\n        pygame.display.quit()\n        pygame.quit()\n</code></pre>"},{"location":"api/networks/base_network/","title":"<code>viberl.networks.base_network</code>","text":"<p>Classes:</p> Name Description <code>BaseNetwork</code> <p>Base neural network architecture for RL agents.</p>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork","title":"BaseNetwork","text":"<pre><code>BaseNetwork(input_size: int, hidden_size: int = 128, num_hidden_layers: int = 2)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base neural network architecture for RL agents.</p> <p>Methods:</p> Name Description <code>forward_backbone</code> <p>Forward pass through the shared backbone.</p> <code>init_weights</code> <p>Initialize network weights using Xavier initialization.</p> <p>Attributes:</p> Name Type Description <code>input_size</code> <code>hidden_size</code> <code>num_hidden_layers</code> <code>backbone</code> Source code in <code>viberl/networks/base_network.py</code> <pre><code>def __init__(self, input_size: int, hidden_size: int = 128, num_hidden_layers: int = 2):\n    super().__init__()\n    self.input_size = input_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n\n    # Build network layers\n    layers = []\n    layers.append(nn.Linear(input_size, hidden_size))\n    layers.append(nn.ReLU())\n\n    for _ in range(num_hidden_layers - 1):\n        layers.append(nn.Linear(hidden_size, hidden_size))\n        layers.append(nn.ReLU())\n\n    self.backbone = nn.Sequential(*layers)\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.input_size","title":"input_size  <code>instance-attribute</code>","text":"<pre><code>input_size = input_size\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.hidden_size","title":"hidden_size  <code>instance-attribute</code>","text":"<pre><code>hidden_size = hidden_size\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.num_hidden_layers","title":"num_hidden_layers  <code>instance-attribute</code>","text":"<pre><code>num_hidden_layers = num_hidden_layers\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone = Sequential(*layers)\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.forward_backbone","title":"forward_backbone","text":"<pre><code>forward_backbone(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass through the shared backbone.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_size)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Processed tensor of shape (batch_size, hidden_size)</p> Source code in <code>viberl/networks/base_network.py</code> <pre><code>def forward_backbone(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the shared backbone.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_size)\n\n    Returns:\n        Processed tensor of shape (batch_size, hidden_size)\n    \"\"\"\n    return self.backbone(x)\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.init_weights","title":"init_weights","text":"<pre><code>init_weights() -&gt; None\n</code></pre> <p>Initialize network weights using Xavier initialization.</p> <p>Uses Xavier uniform initialization for linear layers and zeros for biases. This helps with stable gradient flow during training and prevents vanishing/exploding gradients.</p> Source code in <code>viberl/networks/base_network.py</code> <pre><code>def init_weights(self) -&gt; None:\n    \"\"\"Initialize network weights using Xavier initialization.\n\n    Uses Xavier uniform initialization for linear layers and zeros for biases.\n    This helps with stable gradient flow during training and prevents\n    vanishing/exploding gradients.\n    \"\"\"\n    for module in self.modules():\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0)\n</code></pre>"},{"location":"api/networks/policy_network/","title":"<code>viberl.networks.policy_network</code>","text":"<p>Classes:</p> Name Description <code>PolicyNetwork</code> <p>Policy network for policy gradient methods like REINFORCE.</p>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork","title":"PolicyNetwork","text":"<pre><code>PolicyNetwork(\n    state_size: int, action_size: int, hidden_size: int = 128, num_hidden_layers: int = 2\n)\n</code></pre> <p>               Bases: <code>BaseNetwork</code></p> <p>Policy network for policy gradient methods like REINFORCE.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass to get action probabilities.</p> <code>act</code> <p>Select action based on current policy.</p> <code>get_action_prob</code> <p>Get probability of taking a specific action.</p> <p>Attributes:</p> Name Type Description <code>action_size</code> <code>policy_head</code> <code>softmax</code> Source code in <code>viberl/networks/policy_network.py</code> <pre><code>def __init__(\n    self, state_size: int, action_size: int, hidden_size: int = 128, num_hidden_layers: int = 2\n):\n    super().__init__(state_size, hidden_size, num_hidden_layers)\n    self.action_size = action_size\n\n    # Policy head\n    self.policy_head = nn.Linear(hidden_size, action_size)\n    self.softmax = nn.Softmax(dim=-1)\n\n    self.init_weights()\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.action_size","title":"action_size  <code>instance-attribute</code>","text":"<pre><code>action_size = action_size\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.policy_head","title":"policy_head  <code>instance-attribute</code>","text":"<pre><code>policy_head = Linear(hidden_size, action_size)\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.softmax","title":"softmax  <code>instance-attribute</code>","text":"<pre><code>softmax = Softmax(dim=-1)\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass to get action probabilities.</p> <p>Processes state features through the backbone network and policy head to produce normalized action probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input state tensor of shape (batch_size, state_size)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Action probabilities tensor of shape (batch_size, action_size)</p> <code>Tensor</code> <p>with values summing to 1 along the action dimension</p> Source code in <code>viberl/networks/policy_network.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass to get action probabilities.\n\n    Processes state features through the backbone network and policy head\n    to produce normalized action probabilities.\n\n    Args:\n        x: Input state tensor of shape (batch_size, state_size)\n\n    Returns:\n        Action probabilities tensor of shape (batch_size, action_size)\n        with values summing to 1 along the action dimension\n    \"\"\"\n    features = self.forward_backbone(x)\n    action_logits = self.policy_head(features)\n    return self.softmax(action_logits)\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.act","title":"act","text":"<pre><code>act(state: list | tuple | Tensor, deterministic: bool = False) -&gt; int\n</code></pre> <p>Select action based on current policy.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>list | tuple | Tensor</code> <p>Current state as list, tuple, or tensor</p> required <code>deterministic</code> <code>bool</code> <p>If True, always returns the most probable action.           If False, samples from the action distribution.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Selected action as integer</p> Source code in <code>viberl/networks/policy_network.py</code> <pre><code>def act(self, state: list | tuple | torch.Tensor, deterministic: bool = False) -&gt; int:\n    \"\"\"Select action based on current policy.\n\n    Args:\n        state: Current state as list, tuple, or tensor\n        deterministic: If True, always returns the most probable action.\n                      If False, samples from the action distribution.\n\n    Returns:\n        Selected action as integer\n    \"\"\"\n    if isinstance(state, list | tuple):\n        state = torch.FloatTensor(state)\n    else:\n        state = torch.FloatTensor(state).unsqueeze(0)\n\n    action_probs = self.forward(state)\n\n    if deterministic:\n        return action_probs.argmax().item()\n    else:\n        m = Categorical(action_probs)\n        return m.sample().item()\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.get_action_prob","title":"get_action_prob","text":"<pre><code>get_action_prob(state: list | tuple | Tensor, action: Tensor) -&gt; Tensor\n</code></pre> <p>Get probability of taking a specific action.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>list | tuple | Tensor</code> <p>Current state as list, tuple, or tensor</p> required <code>action</code> <code>Tensor</code> <p>Action tensor to get probability for</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Probability of taking the specified action</p> Source code in <code>viberl/networks/policy_network.py</code> <pre><code>def get_action_prob(\n    self, state: list | tuple | torch.Tensor, action: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Get probability of taking a specific action.\n\n    Args:\n        state: Current state as list, tuple, or tensor\n        action: Action tensor to get probability for\n\n    Returns:\n        Probability of taking the specified action\n    \"\"\"\n    action_probs = self.forward(state)\n    return action_probs.gather(1, action.unsqueeze(1)).squeeze(1)\n</code></pre>"},{"location":"api/networks/value_network/","title":"<code>viberl.networks.value_network</code>","text":"<p>Classes:</p> Name Description <code>VNetwork</code> <p>Value network for PPO and other policy gradient methods (returns single scalar value for state).</p> <code>QNetwork</code> <p>Q-network for value-based methods like DQN (returns Q-values for all actions).</p>"},{"location":"api/networks/value_network/#viberl.networks.value_network.VNetwork","title":"VNetwork","text":"<pre><code>VNetwork(state_size: int, hidden_size: int = 128, num_hidden_layers: int = 2)\n</code></pre> <p>               Bases: <code>BaseNetwork</code></p> <p>Value network for PPO and other policy gradient methods (returns single scalar value for state).</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass to get state value.</p> <p>Attributes:</p> Name Type Description <code>value_head</code> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def __init__(self, state_size: int, hidden_size: int = 128, num_hidden_layers: int = 2):\n    super().__init__(state_size, hidden_size, num_hidden_layers)\n\n    # Single output for state value\n    self.value_head = nn.Linear(hidden_size, 1)\n    self.init_weights()\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.VNetwork.value_head","title":"value_head  <code>instance-attribute</code>","text":"<pre><code>value_head = Linear(hidden_size, 1)\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.VNetwork.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass to get state value.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input state tensor of shape (batch_size, state_size)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>State value tensor of shape (batch_size,) representing the</p> <code>Tensor</code> <p>estimated value of the given state</p> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass to get state value.\n\n    Args:\n        x: Input state tensor of shape (batch_size, state_size)\n\n    Returns:\n        State value tensor of shape (batch_size,) representing the\n        estimated value of the given state\n    \"\"\"\n    features = self.forward_backbone(x)\n    return self.value_head(features).squeeze(-1)  # Remove last dim\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork","title":"QNetwork","text":"<pre><code>QNetwork(state_size: int, action_size: int, hidden_size: int = 128, num_hidden_layers: int = 2)\n</code></pre> <p>               Bases: <code>BaseNetwork</code></p> <p>Q-network for value-based methods like DQN (returns Q-values for all actions).</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass to get Q-values for all actions.</p> <code>get_q_values</code> <p>Get Q-values for a given state.</p> <code>get_action</code> <p>Get action using epsilon-greedy policy.</p> <p>Attributes:</p> Name Type Description <code>action_size</code> <code>q_head</code> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def __init__(\n    self, state_size: int, action_size: int, hidden_size: int = 128, num_hidden_layers: int = 2\n):\n    super().__init__(state_size, hidden_size, num_hidden_layers)\n    self.action_size = action_size\n\n    # Q-value head for all actions\n    self.q_head = nn.Linear(hidden_size, action_size)\n\n    self.init_weights()\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.action_size","title":"action_size  <code>instance-attribute</code>","text":"<pre><code>action_size = action_size\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.q_head","title":"q_head  <code>instance-attribute</code>","text":"<pre><code>q_head = Linear(hidden_size, action_size)\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass to get Q-values for all actions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input state tensor of shape (batch_size, state_size)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Q-values tensor of shape (batch_size, action_size) containing</p> <code>Tensor</code> <p>Q-values for each action in the given state</p> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass to get Q-values for all actions.\n\n    Args:\n        x: Input state tensor of shape (batch_size, state_size)\n\n    Returns:\n        Q-values tensor of shape (batch_size, action_size) containing\n        Q-values for each action in the given state\n    \"\"\"\n    features = self.forward_backbone(x)\n    return self.q_head(features)\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.get_q_values","title":"get_q_values","text":"<pre><code>get_q_values(state: list | tuple | Tensor) -&gt; Tensor\n</code></pre> <p>Get Q-values for a given state.</p> <p>Convenience method that handles various input types and ensures proper tensor formatting before forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>list | tuple | Tensor</code> <p>Current state as list, tuple, or tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Q-values tensor of shape (1, action_size) if single state,</p> <code>Tensor</code> <p>or (batch_size, action_size) if batch of states</p> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def get_q_values(self, state: list | tuple | torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Get Q-values for a given state.\n\n    Convenience method that handles various input types and ensures\n    proper tensor formatting before forward pass.\n\n    Args:\n        state: Current state as list, tuple, or tensor\n\n    Returns:\n        Q-values tensor of shape (1, action_size) if single state,\n        or (batch_size, action_size) if batch of states\n    \"\"\"\n    if isinstance(state, list | tuple):\n        state = torch.FloatTensor(state)\n    else:\n        state = torch.FloatTensor(state)\n\n    if len(state.shape) == 1:\n        state = state.unsqueeze(0)\n\n    return self.forward(state)\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.get_action","title":"get_action","text":"<pre><code>get_action(state: list | tuple | Tensor, epsilon: float = 0.0) -&gt; int\n</code></pre> <p>Get action using epsilon-greedy policy.</p> <p>Implements the epsilon-greedy action selection strategy where: - With probability epsilon: choose random action (exploration) - With probability 1-epsilon: choose best action (exploitation)</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>list | tuple | Tensor</code> <p>Current state as list, tuple, or tensor</p> required <code>epsilon</code> <code>float</code> <p>Probability of choosing random action (0.0 to 1.0)</p> <code>0.0</code> <p>Returns:</p> Type Description <code>int</code> <p>Selected action as integer</p> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def get_action(self, state: list | tuple | torch.Tensor, epsilon: float = 0.0) -&gt; int:\n    \"\"\"Get action using epsilon-greedy policy.\n\n    Implements the epsilon-greedy action selection strategy where:\n    - With probability epsilon: choose random action (exploration)\n    - With probability 1-epsilon: choose best action (exploitation)\n\n    Args:\n        state: Current state as list, tuple, or tensor\n        epsilon: Probability of choosing random action (0.0 to 1.0)\n\n    Returns:\n        Selected action as integer\n    \"\"\"\n    q_values = self.get_q_values(state)\n\n    if torch.rand(1).item() &lt; epsilon:\n        return torch.randint(0, self.action_size, (1,)).item()\n    else:\n        return q_values.argmax(dim=1).item()\n</code></pre>"},{"location":"api/utils/common/","title":"<code>viberl.utils.common</code>","text":"<p>Functions:</p> Name Description <code>set_seed</code> <p>Set random seed for reproducibility.</p> <code>get_device</code> <p>Get the best available device (CUDA if available, else CPU).</p> <code>normalize_returns</code> <p>Normalize returns to zero mean and unit variance.</p>"},{"location":"api/utils/common/#viberl.utils.common.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int) -&gt; None\n</code></pre> <p>Set random seed for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed value</p> required Source code in <code>viberl/utils/common.py</code> <pre><code>def set_seed(seed: int) -&gt; None:\n    \"\"\"\n    Set random seed for reproducibility.\n\n    Args:\n        seed: Random seed value\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n</code></pre>"},{"location":"api/utils/common/#viberl.utils.common.get_device","title":"get_device","text":"<pre><code>get_device() -&gt; device\n</code></pre> <p>Get the best available device (CUDA if available, else CPU).</p> <p>Returns:</p> Type Description <code>device</code> <p>PyTorch device</p> Source code in <code>viberl/utils/common.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"\n    Get the best available device (CUDA if available, else CPU).\n\n    Returns:\n        PyTorch device\n    \"\"\"\n    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n</code></pre>"},{"location":"api/utils/common/#viberl.utils.common.normalize_returns","title":"normalize_returns","text":"<pre><code>normalize_returns(returns: ndarray) -&gt; ndarray\n</code></pre> <p>Normalize returns to zero mean and unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Array of returns</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Normalized returns</p> Source code in <code>viberl/utils/common.py</code> <pre><code>def normalize_returns(returns: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Normalize returns to zero mean and unit variance.\n\n    Args:\n        returns: Array of returns\n\n    Returns:\n        Normalized returns\n    \"\"\"\n    if len(returns) == 0:\n        return returns\n\n    mean = np.mean(returns)\n    std = np.std(returns)\n\n    if std == 0:\n        return returns - mean\n\n    return (returns - mean) / (std + 1e-8)\n</code></pre>"},{"location":"api/utils/experiment_manager/","title":"<code>viberl.utils.experiment_manager</code>","text":"<p>Experiment management utilities.</p> <p>This module provides utilities for managing experiment directories, tensorboard logging, and model checkpoints.</p> <p>Classes:</p> Name Description <code>ExperimentManager</code> <p>Manages experiment directories with automatic naming and organization.</p> <p>Functions:</p> Name Description <code>create_experiment</code> <p>Convenience function to create a new experiment.</p>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager","title":"ExperimentManager","text":"<pre><code>ExperimentManager(\n    experiment_name: str, base_dir: str = 'experiments', timestamp_format: str = '%Y%m%d_%H%M%S'\n)\n</code></pre> <p>Manages experiment directories with automatic naming and organization.</p> <p>Creates experiment directories in the format: experiments/{experiment_name}_{timestamp}/ \u251c\u2500\u2500 tb_logs/          # TensorBoard logs \u251c\u2500\u2500 models/           # Saved model checkpoints \u2514\u2500\u2500 training.log      # Training log file</p> <p>Initialize experiment manager.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment</p> required <code>base_dir</code> <code>str</code> <p>Base directory for experiments</p> <code>'experiments'</code> <code>timestamp_format</code> <code>str</code> <p>Format for timestamp in directory name</p> <code>'%Y%m%d_%H%M%S'</code> <p>Methods:</p> Name Description <code>get_tb_logs_path</code> <p>Get path to TensorBoard logs directory.</p> <code>get_models_path</code> <p>Get path to models directory.</p> <code>get_training_log_path</code> <p>Get path to training log file.</p> <code>configure_file_logging</code> <p>Configure loguru to log to training.log file.</p> <code>log_command_line_args</code> <p>Log command line arguments to training.log.</p> <code>get_experiment_path</code> <p>Get path to experiment directory.</p> <code>get_exp_dir</code> <p>Get path to experiment directory (alias for get_experiment_path).</p> <code>save_model</code> <p>Get full path for saving a model.</p> <code>list_experiments</code> <p>List all existing experiments.</p> <code>get_latest_experiment</code> <p>Get the latest experiment directory.</p> <code>print_experiment_info</code> <p>Print information about the current experiment.</p> <code>create_from_existing</code> <p>Create ExperimentManager from existing experiment directory.</p> <p>Attributes:</p> Name Type Description <code>experiment_name</code> <code>base_dir</code> <code>timestamp_format</code> <code>experiment_dir</code> <code>exp_dir</code> <code>tb_logs_dir</code> <code>models_dir</code> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def __init__(\n    self,\n    experiment_name: str,\n    base_dir: str = 'experiments',\n    timestamp_format: str = '%Y%m%d_%H%M%S',\n):\n    \"\"\"\n    Initialize experiment manager.\n\n    Args:\n        experiment_name: Name of the experiment\n        base_dir: Base directory for experiments\n        timestamp_format: Format for timestamp in directory name\n    \"\"\"\n    self.experiment_name = experiment_name\n    self.base_dir = Path(base_dir)\n    self.timestamp_format = timestamp_format\n\n    # Create experiment directory name with timestamp\n    timestamp = datetime.now().strftime(timestamp_format)\n    self.experiment_dir = self.base_dir / f'{experiment_name}_{timestamp}'\n\n    # Core experiment directory\n    self.exp_dir = self.experiment_dir\n\n    # Standardized subdirectory paths\n    self.tb_logs_dir = self.exp_dir / 'tb_logs'\n    self.models_dir = self.exp_dir / 'models'\n\n    # Create directories\n    self._create_directories()\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.experiment_name","title":"experiment_name  <code>instance-attribute</code>","text":"<pre><code>experiment_name = experiment_name\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.base_dir","title":"base_dir  <code>instance-attribute</code>","text":"<pre><code>base_dir = Path(base_dir)\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.timestamp_format","title":"timestamp_format  <code>instance-attribute</code>","text":"<pre><code>timestamp_format = timestamp_format\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.experiment_dir","title":"experiment_dir  <code>instance-attribute</code>","text":"<pre><code>experiment_dir = base_dir / f'{experiment_name}_{timestamp}'\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.exp_dir","title":"exp_dir  <code>instance-attribute</code>","text":"<pre><code>exp_dir = experiment_dir\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.tb_logs_dir","title":"tb_logs_dir  <code>instance-attribute</code>","text":"<pre><code>tb_logs_dir = exp_dir / 'tb_logs'\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.models_dir","title":"models_dir  <code>instance-attribute</code>","text":"<pre><code>models_dir = exp_dir / 'models'\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_tb_logs_path","title":"get_tb_logs_path","text":"<pre><code>get_tb_logs_path() -&gt; Path\n</code></pre> <p>Get path to TensorBoard logs directory.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_tb_logs_path(self) -&gt; Path:\n    \"\"\"Get path to TensorBoard logs directory.\"\"\"\n    return self.tb_logs_dir\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_models_path","title":"get_models_path","text":"<pre><code>get_models_path() -&gt; Path\n</code></pre> <p>Get path to models directory.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_models_path(self) -&gt; Path:\n    \"\"\"Get path to models directory.\"\"\"\n    return self.models_dir\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_training_log_path","title":"get_training_log_path","text":"<pre><code>get_training_log_path() -&gt; Path\n</code></pre> <p>Get path to training log file.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_training_log_path(self) -&gt; Path:\n    \"\"\"Get path to training log file.\"\"\"\n    return self.experiment_dir / 'training.log'\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.configure_file_logging","title":"configure_file_logging","text":"<pre><code>configure_file_logging(log_level: str = 'INFO') -&gt; None\n</code></pre> <p>Configure loguru to log to training.log file.</p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR)</p> <code>'INFO'</code> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def configure_file_logging(self, log_level: str = 'INFO') -&gt; None:\n    \"\"\"\n    Configure loguru to log to training.log file.\n\n    Args:\n        log_level: Logging level (DEBUG, INFO, WARNING, ERROR)\n    \"\"\"\n    log_file = self.get_training_log_path()\n\n    # Remove any existing file handlers to avoid duplicates\n    logger.remove()\n\n    # Add console handler\n    logger.add(\n        lambda msg: print(msg, end=''),\n        level=log_level,\n        format='&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level:8}&lt;/level&gt; | &lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - &lt;level&gt;{message}&lt;/level&gt;',\n    )\n\n    # Add file handler\n    logger.add(\n        str(log_file),\n        level=log_level,\n        format='{time:YYYY-MM-DD HH:mm:ss} | {level:8} | {name}:{function}:{line} - {message}',\n        rotation='10 MB',  # Rotate at 10MB\n        retention='10 days',  # Keep logs for 10 days\n    )\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.log_command_line_args","title":"log_command_line_args","text":"<pre><code>log_command_line_args(args: object) -&gt; None\n</code></pre> <p>Log command line arguments to training.log.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>object</code> <p>Parsed argparse arguments object</p> required Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def log_command_line_args(self, args: object) -&gt; None:\n    \"\"\"\n    Log command line arguments to training.log.\n\n    Args:\n        args: Parsed argparse arguments object\n    \"\"\"\n    logger.info('=' * 80)\n    logger.info('TRAINING SESSION STARTED')\n    logger.info('=' * 80)\n\n    # Log command line arguments\n    logger.info('Command Line Arguments:')\n    for arg_name, arg_value in vars(args).items():\n        logger.info(f'  {arg_name}: {arg_value}')\n\n    logger.info('=' * 80)\n    logger.info(f'Experiment Directory: {self.experiment_dir}')\n    logger.info('=' * 80)\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_experiment_path","title":"get_experiment_path","text":"<pre><code>get_experiment_path() -&gt; Path\n</code></pre> <p>Get path to experiment directory.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_experiment_path(self) -&gt; Path:\n    \"\"\"Get path to experiment directory.\"\"\"\n    return self.exp_dir\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_exp_dir","title":"get_exp_dir","text":"<pre><code>get_exp_dir() -&gt; Path\n</code></pre> <p>Get path to experiment directory (alias for get_experiment_path).</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_exp_dir(self) -&gt; Path:\n    \"\"\"Get path to experiment directory (alias for get_experiment_path).\"\"\"\n    return self.exp_dir\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.save_model","title":"save_model","text":"<pre><code>save_model(model_name: str) -&gt; Path\n</code></pre> <p>Get full path for saving a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model file (without extension)</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Full path for model file</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def save_model(self, model_name: str) -&gt; Path:\n    \"\"\"\n    Get full path for saving a model.\n\n    Args:\n        model_name: Name of the model file (without extension)\n\n    Returns:\n        Full path for model file\n    \"\"\"\n    return self.models_dir / f'{model_name}.pth'\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.list_experiments","title":"list_experiments","text":"<pre><code>list_experiments() -&gt; list[Path]\n</code></pre> <p>List all existing experiments.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def list_experiments(self) -&gt; list[Path]:\n    \"\"\"List all existing experiments.\"\"\"\n    if not self.base_dir.exists():\n        return []\n\n    experiments = [\n        exp_dir\n        for exp_dir in self.base_dir.iterdir()\n        if exp_dir.is_dir() and self.experiment_name in exp_dir.name\n    ]\n\n    return sorted(experiments, reverse=True)\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_latest_experiment","title":"get_latest_experiment","text":"<pre><code>get_latest_experiment() -&gt; Path | None\n</code></pre> <p>Get the latest experiment directory.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_latest_experiment(self) -&gt; Path | None:\n    \"\"\"Get the latest experiment directory.\"\"\"\n    experiments = self.list_experiments()\n    return experiments[0] if experiments else None\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.print_experiment_info","title":"print_experiment_info","text":"<pre><code>print_experiment_info() -&gt; None\n</code></pre> <p>Print information about the current experiment.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def print_experiment_info(self) -&gt; None:\n    \"\"\"Print information about the current experiment.\"\"\"\n    logger.info(f'Experiment: {self.experiment_name}')\n    logger.info(f'Directory: {self.exp_dir}')\n    logger.info(f'TensorBoard logs: {self.tb_logs_dir}')\n    logger.info(f'Models: {self.models_dir}')\n    logger.info(f'Training log: {self.get_training_log_path()}')\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.create_from_existing","title":"create_from_existing  <code>staticmethod</code>","text":"<pre><code>create_from_existing(experiment_path: str | Path) -&gt; ExperimentManager\n</code></pre> <p>Create ExperimentManager from existing experiment directory.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_path</code> <code>str | Path</code> <p>Path to existing experiment directory</p> required <p>Returns:</p> Type Description <code>ExperimentManager</code> <p>ExperimentManager instance</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>@staticmethod\ndef create_from_existing(\n    experiment_path: str | Path,\n) -&gt; 'ExperimentManager':\n    \"\"\"\n    Create ExperimentManager from existing experiment directory.\n\n    Args:\n        experiment_path: Path to existing experiment directory\n\n    Returns:\n        ExperimentManager instance\n    \"\"\"\n    experiment_path = Path(experiment_path)\n    experiment_name = experiment_path.name.rsplit('_', 1)[0]\n\n    # Create manager but don't create directories since they exist\n    manager = ExperimentManager(experiment_name, str(experiment_path.parent))\n    return manager\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.create_experiment","title":"create_experiment","text":"<pre><code>create_experiment(\n    experiment_name: str, base_dir: str = 'experiments', print_info: bool = True\n) -&gt; ExperimentManager\n</code></pre> <p>Convenience function to create a new experiment.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment</p> required <code>base_dir</code> <code>str</code> <p>Base directory for experiments</p> <code>'experiments'</code> <code>print_info</code> <code>bool</code> <p>Whether to print experiment info</p> <code>True</code> <p>Returns:</p> Type Description <code>ExperimentManager</code> <p>ExperimentManager instance</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def create_experiment(\n    experiment_name: str,\n    base_dir: str = 'experiments',\n    print_info: bool = True,\n) -&gt; ExperimentManager:\n    \"\"\"\n    Convenience function to create a new experiment.\n\n    Args:\n        experiment_name: Name of the experiment\n        base_dir: Base directory for experiments\n        print_info: Whether to print experiment info\n\n    Returns:\n        ExperimentManager instance\n    \"\"\"\n    manager = ExperimentManager(experiment_name, base_dir)\n\n    if print_info:\n        manager.print_experiment_info()\n\n    return manager\n</code></pre>"},{"location":"api/utils/mock_env/","title":"<code>viberl.utils.mock_env</code>","text":"<p>Mock environment for testing RL algorithms.</p> <p>Provides a gymnasium-compatible environment that returns random valid values for all methods, useful for testing agents without complex environment setup.</p> <p>Classes:</p> Name Description <code>MockEnv</code> <p>A mock environment that returns random valid values for testing.</p>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv","title":"MockEnv","text":"<pre><code>MockEnv(state_size: int = 4, action_size: int = 2, max_episode_steps: int = 100)\n</code></pre> <p>               Bases: <code>Env</code></p> <p>A mock environment that returns random valid values for testing.</p> <p>This environment provides: - Random observations within observation space - Random rewards within reward range - Random terminal states - Random info dictionaries</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <code>int</code> <p>Size of the observation space</p> <code>4</code> <code>action_size</code> <code>int</code> <p>Number of discrete actions</p> <code>2</code> <code>max_episode_steps</code> <code>int</code> <p>Maximum steps before truncation</p> <code>100</code> <p>Methods:</p> Name Description <code>reset</code> <p>Reset the environment with random initial state.</p> <code>step</code> <p>Take a step with random outcomes.</p> <code>render</code> <p>Mock render - does nothing.</p> <code>close</code> <p>Mock close - does nothing.</p> <code>seed</code> <p>Set random seed for reproducibility.</p> <p>Attributes:</p> Name Type Description <code>state_size</code> <code>action_size</code> <code>max_episode_steps</code> <code>observation_space</code> <code>action_space</code> <code>current_step</code> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def __init__(\n    self,\n    state_size: int = 4,\n    action_size: int = 2,\n    max_episode_steps: int = 100,\n) -&gt; None:\n    super().__init__()\n\n    self.state_size = state_size\n    self.action_size = action_size\n    self.max_episode_steps = max_episode_steps\n\n    # Define spaces\n    self.observation_space = spaces.Box(\n        low=-1.0, high=1.0, shape=(state_size,), dtype=np.float32\n    )\n    self.action_space = spaces.Discrete(action_size)\n\n    # Internal state\n    self.current_step = 0\n    self._np_random = np.random.RandomState()\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.state_size","title":"state_size  <code>instance-attribute</code>","text":"<pre><code>state_size = state_size\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.action_size","title":"action_size  <code>instance-attribute</code>","text":"<pre><code>action_size = action_size\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.max_episode_steps","title":"max_episode_steps  <code>instance-attribute</code>","text":"<pre><code>max_episode_steps = max_episode_steps\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.observation_space","title":"observation_space  <code>instance-attribute</code>","text":"<pre><code>observation_space = Box(low=-1.0, high=1.0, shape=(state_size,), dtype=float32)\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.action_space","title":"action_space  <code>instance-attribute</code>","text":"<pre><code>action_space = Discrete(action_size)\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.current_step","title":"current_step  <code>instance-attribute</code>","text":"<pre><code>current_step = 0\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.reset","title":"reset","text":"<pre><code>reset(seed: int | None = None, options: dict | None = None) -&gt; tuple[ndarray, dict]\n</code></pre> <p>Reset the environment with random initial state.</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def reset(\n    self,\n    seed: int | None = None,\n    options: dict | None = None,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"Reset the environment with random initial state.\"\"\"\n    super().reset(seed=seed)\n\n    if seed is not None:\n        self._np_random = np.random.RandomState(seed)\n        # Set numpy's global random state for gymnasium's sample() method\n        np.random.seed(seed)\n\n    self.current_step = 0\n\n    # Generate random observation using our seeded random state\n    obs = self._np_random.uniform(\n        low=self.observation_space.low,\n        high=self.observation_space.high,\n        size=self.observation_space.shape,\n    ).astype(np.float32)\n\n    # Return with random info\n    info = {'episode': 0, 'step': 0, 'random_metric': self._np_random.random()}\n\n    return obs, info\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.step","title":"step","text":"<pre><code>step(action: int) -&gt; tuple[ndarray, float, bool, bool, dict]\n</code></pre> <p>Take a step with random outcomes.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>int</code> <p>The action to take</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, float, bool, bool, dict]</code> <p>observation, reward, terminated, truncated, info</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def step(self, action: int) -&gt; tuple[np.ndarray, float, bool, bool, dict]:\n    \"\"\"\n    Take a step with random outcomes.\n\n    Args:\n        action: The action to take\n\n    Returns:\n        observation, reward, terminated, truncated, info\n    \"\"\"\n    assert self.action_space.contains(action), f'Invalid action: {action}'\n\n    # Check if we've reached max steps (truncation happens after max_episode_steps steps)\n    if self.current_step &gt;= self.max_episode_steps:\n        obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n        reward = 0.0\n        terminated = True\n        truncated = True\n        info = {'step': self.current_step, 'truncated': True}\n        return obs, reward, terminated, truncated, info\n\n    self.current_step += 1\n\n    # Generate random observation using seeded random state\n    obs = self._np_random.uniform(\n        low=self.observation_space.low,\n        high=self.observation_space.high,\n        size=self.observation_space.shape,\n    ).astype(np.float32)\n\n    # Generate random reward (-1 to 1)\n    reward = float(self._np_random.uniform(-1.0, 1.0))\n\n    # Random termination (5% chance per step)\n    terminated = bool(self._np_random.random() &lt; 0.05)\n\n    # Truncation happens when we reach max_episode_steps\n    truncated = self.current_step &gt;= self.max_episode_steps\n\n    # Random info\n    info = {\n        'step': self.current_step,\n        'action_taken': action,\n        'random_info': self._np_random.random(),\n        'episode_complete': terminated or truncated,\n    }\n\n    return obs, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.render","title":"render","text":"<pre><code>render() -&gt; None\n</code></pre> <p>Mock render - does nothing.</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def render(self) -&gt; None:\n    \"\"\"Mock render - does nothing.\"\"\"\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Mock close - does nothing.</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Mock close - does nothing.\"\"\"\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.seed","title":"seed","text":"<pre><code>seed(seed: int | None = None) -&gt; None\n</code></pre> <p>Set random seed for reproducibility.</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def seed(self, seed: int | None = None) -&gt; None:\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    self._np_random = np.random.RandomState(seed)\n</code></pre>"},{"location":"api/utils/training/","title":"<code>viberl.utils.training</code>","text":"<p>Training utilities for VibeRL framework.</p> <p>This module provides backward compatibility for the old training interface while internally using the new Trainer class.</p> <p>Functions:</p> Name Description <code>train_agent</code> <p>Generic training function for RL agents with periodic evaluation.</p> <code>evaluate_agent</code> <p>Generic evaluation function for RL agents.</p>"},{"location":"api/utils/training/#viberl.utils.training.train_agent","title":"train_agent","text":"<pre><code>train_agent(\n    env: Env,\n    agent: Agent,\n    num_episodes: int = 1000,\n    max_steps: int = 1000,\n    render_interval: int | None = None,\n    save_interval: int | None = None,\n    save_path: str | None = None,\n    verbose: bool = True,\n    log_dir: str | None = None,\n    eval_interval: int = 100,\n    eval_episodes: int = 10,\n    log_interval: int = 1000,\n) -&gt; list[float]\n</code></pre> <p>Generic training function for RL agents with periodic evaluation.</p> <p>.. deprecated:: 1.0     Use :class:<code>viberl.trainer.Trainer</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Env</code> <p>Gymnasium environment</p> required <code>agent</code> <code>Agent</code> <p>RL agent with select_action, store_transition, and update_policy methods</p> required <code>num_episodes</code> <code>int</code> <p>Number of training episodes</p> <code>1000</code> <code>max_steps</code> <code>int</code> <p>Maximum steps per episode</p> <code>1000</code> <code>render_interval</code> <code>int | None</code> <p>Render every N episodes</p> <code>None</code> <code>save_interval</code> <code>int | None</code> <p>Save model every N episodes</p> <code>None</code> <code>save_path</code> <code>str | None</code> <p>Path to save models</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print training progress</p> <code>True</code> <code>log_dir</code> <code>str | None</code> <p>Directory for TensorBoard logs</p> <code>None</code> <code>eval_interval</code> <code>int</code> <p>Evaluate every N episodes</p> <code>100</code> <code>eval_episodes</code> <code>int</code> <p>Number of evaluation episodes</p> <code>10</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of episode rewards</p> Source code in <code>viberl/utils/training.py</code> <pre><code>def train_agent(\n    env: gym.Env,\n    agent: Agent,\n    num_episodes: int = 1000,\n    max_steps: int = 1000,\n    render_interval: int | None = None,\n    save_interval: int | None = None,\n    save_path: str | None = None,\n    verbose: bool = True,\n    log_dir: str | None = None,\n    eval_interval: int = 100,\n    eval_episodes: int = 10,\n    log_interval: int = 1000,\n) -&gt; list[float]:\n    \"\"\"\n    Generic training function for RL agents with periodic evaluation.\n\n    .. deprecated:: 1.0\n        Use :class:`viberl.trainer.Trainer` instead.\n\n    Args:\n        env: Gymnasium environment\n        agent: RL agent with select_action, store_transition, and update_policy methods\n        num_episodes: Number of training episodes\n        max_steps: Maximum steps per episode\n        render_interval: Render every N episodes\n        save_interval: Save model every N episodes\n        save_path: Path to save models\n        verbose: Print training progress\n        log_dir: Directory for TensorBoard logs\n        eval_interval: Evaluate every N episodes\n        eval_episodes: Number of evaluation episodes\n\n    Returns:\n        List of episode rewards\n    \"\"\"\n    warnings.warn(\n        'train_agent is deprecated and will be removed in a future version. '\n        'Use viberl.trainer.Trainer instead.',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n    # Create trainer using new interface\n    trainer = Trainer(\n        env=env,\n        agent=agent,\n        max_steps=max_steps,\n        log_dir=log_dir,\n    )\n\n    # Train using new trainer\n    return trainer.train(\n        num_episodes=num_episodes,\n        eval_interval=eval_interval,\n        eval_episodes=eval_episodes,\n        save_interval=save_interval,\n        save_path=save_path,\n        render_interval=render_interval,\n        log_interval=log_interval,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/utils/training/#viberl.utils.training.evaluate_agent","title":"evaluate_agent","text":"<pre><code>evaluate_agent(\n    env: Env, agent: Agent, num_episodes: int = 10, render: bool = False, max_steps: int = 1000\n) -&gt; tuple[list[float], list[int]]\n</code></pre> <p>Generic evaluation function for RL agents.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Env</code> <p>Gymnasium environment</p> required <code>agent</code> <code>Agent</code> <p>RL agent with select_action method</p> required <code>num_episodes</code> <code>int</code> <p>Number of evaluation episodes</p> <code>10</code> <code>render</code> <code>bool</code> <p>Whether to render the environment</p> <code>False</code> <code>max_steps</code> <code>int</code> <p>Maximum steps per episode</p> <code>1000</code> <p>Returns:</p> Type Description <code>tuple[list[float], list[int]]</code> <p>Tuple of (episode_rewards, episode_lengths)</p> Source code in <code>viberl/utils/training.py</code> <pre><code>def evaluate_agent(\n    env: gym.Env,\n    agent: Agent,\n    num_episodes: int = 10,\n    render: bool = False,\n    max_steps: int = 1000,\n) -&gt; tuple[list[float], list[int]]:\n    \"\"\"\n    Generic evaluation function for RL agents.\n\n    Args:\n        env: Gymnasium environment\n        agent: RL agent with select_action method\n        num_episodes: Number of evaluation episodes\n        render: Whether to render the environment\n        max_steps: Maximum steps per episode\n\n    Returns:\n        Tuple of (episode_rewards, episode_lengths)\n    \"\"\"\n    trainer = Trainer(env=env, agent=agent, max_steps=max_steps)\n    return trainer.evaluate(num_episodes=num_episodes, render=render)\n</code></pre>"}]}