{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VibeRL Documentation","text":"<p>Welcome to VibeRL - A modern Reinforcement Learning framework built with type safety and modern Python practices.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/0xWelt/VibeRL.git\ncd VibeRL\n\n# Install using uv\nuv pip install -e \".\"\n\n# Or install with development dependencies\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"#your-first-training","title":"Your First Training","text":"<pre><code># Train a REINFORCE agent\nviberl-train --alg reinforce --episodes 1000 --grid-size 10\n\n# Train a DQN agent\nviberl-train --alg dqn --episodes 2000 --grid-size 15 --memory-size 10000\n\n# Train a PPO agent\nviberl-train --alg ppo --episodes 1000 --grid-size 12 --ppo-epochs 4\n</code></pre>"},{"location":"#evaluate-and-demo","title":"Evaluate and Demo","text":"<pre><code># Evaluate a trained model\nviberl-eval --model-path experiments/reinforce_snake/final_model.pth --episodes 10 --render\n\n# Run demo with random actions\nviberl-demo --episodes 5 --grid-size 15\n</code></pre>"},{"location":"#python-api","title":"Python API","text":""},{"location":"#basic-training","title":"Basic Training","text":"<pre><code>from viberl.agents.reinforce import REINFORCEAgent\nfrom viberl.envs import SnakeGameEnv\nfrom viberl.utils.training import train_agent\n\n# Create environment\nenv = SnakeGameEnv(grid_size=10)\n\n# Create agent\nagent = REINFORCEAgent(\n    state_size=100,  # 10x10 grid\n    action_size=4,   # 4 directions\n    learning_rate=0.001\n)\n\n# Train the agent\ntrain_agent(\n    agent=agent,\n    env=env,\n    episodes=1000,\n    save_path=\"models/reinforce_snake.pth\"\n)\n</code></pre>"},{"location":"#custom-training-loop","title":"Custom Training Loop","text":"<pre><code>import numpy as np\nfrom viberl.typing import Trajectory, Transition\nfrom viberl.agents.dqn import DQNAgent\n\nenv = SnakeGameEnv(grid_size=10)\nagent = DQNAgent(state_size=100, action_size=4)\n\nfor episode in range(1000):\n    state, _ = env.reset()\n    transitions = []\n\n    while True:\n        action = agent.act(state, training=True)\n        next_state, reward, done, truncated, info = env.step(action.action)\n\n        transitions.append(Transition(\n            state=state, action=action, reward=reward,\n            next_state=next_state, done=done\n        ))\n\n        state = next_state\n        if done or truncated:\n            break\n\n    trajectory = Trajectory.from_transitions(transitions)\n    metrics = agent.learn(trajectory)\n\n    if episode % 100 == 0:\n        print(f\"Episode {episode}, Reward: {trajectory.total_reward}\")\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>Modern Type System: Pydantic-based Action, Transition, Trajectory classes</li> <li>Three Algorithms: REINFORCE, DQN, PPO with unified interface</li> <li>Type Safety: Full type annotations throughout</li> <li>CLI Interface: Complete training, evaluation, and demo commands</li> <li>Experiment Management: Automatic directory structure with TensorBoard logging</li> <li>50+ Tests: Comprehensive test suite</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>The framework follows a clean architecture:</p> <ul> <li><code>viberl/typing.py</code>: Modern type system</li> <li><code>viberl/agents/</code>: RL algorithms (REINFORCE, DQN, PPO)</li> <li><code>viberl/envs/</code>: Environments (SnakeGameEnv)</li> <li><code>viberl/networks/</code>: Neural network implementations</li> <li><code>viberl/utils/</code>: Training utilities and experiment management</li> <li><code>viberl/cli.py</code>: Command-line interface</li> </ul>"},{"location":"#algorithms","title":"Algorithms","text":""},{"location":"#reinforce","title":"REINFORCE","text":"<p>Policy gradient method using Monte Carlo returns.</p>"},{"location":"#dqn","title":"DQN","text":"<p>Deep Q-Network with experience replay and target networks.</p>"},{"location":"#ppo","title":"PPO","text":"<p>Proximal Policy Optimization with clipping and multiple epochs.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See the Contributing Guide for information on how to contribute to VibeRL.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"contributing/","title":"Contributing to VibeRL","text":"<p>Thank you for your interest in contributing to VibeRL! This guide will help you get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>uv for dependency management</li> <li>Git</li> </ul>"},{"location":"contributing/#setup-development-environment","title":"Setup Development Environment","text":"<pre><code># Clone the repository\ngit clone https://github.com/0xWelt/VibeRL.git\ncd VibeRL\n\n# Install development dependencies\nuv pip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"contributing/#verify-setup","title":"Verify Setup","text":"<pre><code># Run tests\npytest -n 8\n\n# Run linting\nruff check .\nruff format .\n\n# Run type checking\nmypy viberl/\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use: - ruff for linting and formatting - mypy for type checking - pre-commit for automated checks</p>"},{"location":"contributing/#code-formatting","title":"Code Formatting","text":"<pre><code># Format code\nuv run ruff format src/\n\n# Check formatting\nuv run ruff format src/ --check\n\n# Fix linting issues\nuv run ruff check src/ --fix\n</code></pre>"},{"location":"contributing/#type-annotations","title":"Type Annotations","text":"<p>All new code should include type annotations. We use strict type checking:</p> <pre><code># Good\ndef train_agent(agent: Agent, env: gym.Env, episodes: int) -&gt; dict[str, float]:\n    \"\"\"Train an agent on an environment.\"\"\"\n    ...\n\n# Bad\ndef train_agent(agent, env, episodes):\n    \"\"\"Train an agent on an environment.\"\"\"\n    ...\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest -n 8\n\n# Run specific test file\npytest tests/test_agents/test_reinforce.py\n\n# Run with coverage\npytest -n 8 --cov=viberl --cov-report=html\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<p>We use pytest for testing. Follow these guidelines:</p>"},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code># tests/test_agents/test_reinforce.py\nimport pytest\nfrom viberl.agents.reinforce import REINFORCEAgent\n\nclass TestREINFORCESpecific:\n    \"\"\"Test REINFORCE-specific functionality.\"\"\"\n\n    @pytest.fixture\n    def agent(self) -&gt; REINFORCEAgent:\n        \"\"\"Create REINFORCE agent for testing.\"\"\"\n        return REINFORCEAgent(state_size=4, action_size=3, learning_rate=0.01)\n\n    def test_policy_network_output_shape(self, agent: REINFORCEAgent) -&gt; None:\n        \"\"\"Test policy network outputs correct shape.\"\"\"\n        # Test implementation\n        ...\n</code></pre>"},{"location":"contributing/#use-test-fixtures","title":"Use Test Fixtures","text":"<p>Use fixtures for setup and teardown:</p> <pre><code>@pytest.fixture\ndef mock_env() -&gt; MockEnv:\n    \"\"\"Create mock environment for testing.\"\"\"\n    return MockEnv(state_size=4, action_size=2)\n\n@pytest.fixture\ndef reinforce_agent() -&gt; REINFORCEAgent:\n    \"\"\"Create REINFORCE agent for testing.\"\"\"\n    return REINFORCEAgent(state_size=4, action_size=2, learning_rate=0.01)\n</code></pre>"},{"location":"contributing/#parametrized-tests","title":"Parametrized Tests","text":"<p>Use parametrized tests for multiple scenarios:</p> <pre><code>@pytest.mark.parametrize('episodes', [1, 10, 100])\ndef test_training_different_lengths(episodes: int) -&gt; None:\n    \"\"\"Test training with different episode counts.\"\"\"\n    ...\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def train_agent(agent: Agent, env: gym.Env, episodes: int, **kwargs) -&gt; dict[str, float]:\n    \"\"\"Train an agent on an environment.\n\n    Args:\n        agent: The agent to train.\n        env: The environment to train on.\n        episodes: Number of episodes to train for.\n        **kwargs: Additional training parameters.\n\n    Returns:\n        Dictionary containing training metrics.\n\n    Raises:\n        ValueError: If episodes is negative.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"contributing/#api-documentation","title":"API Documentation","text":"<p>We use mkdocs with mkdocstrings for API documentation. To build docs:</p> <pre><code># Serve documentation locally\nmkdocs serve\n\n# Build documentation\nmkdocs build\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-create-feature-branch","title":"1. Create Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following our style guidelines</li> <li>Add tests for new functionality</li> <li>Update documentation</li> </ul>"},{"location":"contributing/#3-test-changes","title":"3. Test Changes","text":"<pre><code># Run all checks\npre-commit run --all-files\n\n# Run tests\npytest -n 8\n\n# Check documentation builds\nmkdocs build\n</code></pre>"},{"location":"contributing/#4-commit-changes","title":"4. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add new feature description\"\n</code></pre>"},{"location":"contributing/#5-push-and-create-pr","title":"5. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"contributing/#commit-message-guidelines","title":"Commit Message Guidelines","text":"<p>Use conventional commits:</p> <ul> <li><code>feat:</code> - New feature</li> <li><code>fix:</code> - Bug fix</li> <li><code>docs:</code> - Documentation changes</li> <li><code>style:</code> - Code style changes</li> <li><code>refactor:</code> - Code refactoring</li> <li><code>test:</code> - Adding or updating tests</li> <li><code>chore:</code> - Maintenance tasks</li> </ul> <p>Examples: <pre><code>feat: add PPO agent implementation\nfix: resolve memory leak in DQN replay buffer\ndocs: update API documentation for agents\n</code></pre></p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>viberl/\n\u251c\u2500\u2500 agents/          # Agent implementations\n\u251c\u2500\u2500 envs/           # Environment implementations\n\u251c\u2500\u2500 networks/       # Neural network architectures\n\u251c\u2500\u2500 utils/          # Utility functions\n\u2514\u2500\u2500 typing.py       # Type definitions\n\ntests/\n\u251c\u2500\u2500 test_agents/    # Agent tests\n\u251c\u2500\u2500 test_envs/      # Environment tests\n\u251c\u2500\u2500 test_networks/  # Network tests\n\u251c\u2500\u2500 test_utils/     # Utility tests\n\u2514\u2500\u2500 test_typing.py  # Type system tests\n\ndocs/\n\u251c\u2500\u2500 api/            # Auto-generated API docs\n\u251c\u2500\u2500 examples.md     # Usage examples\n\u251c\u2500\u2500 index.md        # Main documentation\n\u2514\u2500\u2500 quickstart.md   # Quick start guide\n</code></pre>"},{"location":"contributing/#adding-new-algorithms","title":"Adding New Algorithms","text":"<p>When adding a new algorithm:</p> <ol> <li>Create Agent Class</li> <li>Inherit from <code>Agent</code> base class</li> <li>Implement required methods: <code>act</code>, <code>learn</code>, <code>save</code>, <code>load</code></li> <li> <p>Add type annotations</p> </li> <li> <p>Add Tests</p> </li> <li>Create test file in <code>tests/test_agents/</code></li> <li>Test algorithm-specific functionality</li> <li> <p>Add to interface compliance tests</p> </li> <li> <p>Update Documentation</p> </li> <li>Add to examples.md</li> <li> <p>Update quickstart.md if needed</p> </li> <li> <p>Update CLI</p> </li> <li>Add to CLI commands in <code>cli.py</code></li> </ol>"},{"location":"contributing/#adding-new-environments","title":"Adding New Environments","text":"<ol> <li>Implement Environment</li> <li>Inherit from <code>gymnasium.Env</code></li> <li>Implement required methods</li> <li> <p>Add comprehensive tests</p> </li> <li> <p>Add Tests</p> </li> <li>Create test file in <code>tests/test_envs/</code></li> <li>Test environment-specific functionality</li> </ol>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>When reporting issues:</p> <ol> <li>Search existing issues first</li> <li>Provide minimal reproduction example</li> <li>Include environment details (Python version, OS, etc.)</li> <li>Provide error messages and stack traces</li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: For bug reports and feature requests</li> <li>GitHub Discussions: For questions and discussions</li> <li>Documentation: Check the docs first!</li> </ul>"},{"location":"contributing/#development-tips","title":"Development Tips","text":""},{"location":"contributing/#ide-setup","title":"IDE Setup","text":""},{"location":"contributing/#vs-code","title":"VS Code","text":"<p>Install these extensions: - Python - Ruff - MyPy - GitLens</p>"},{"location":"contributing/#pycharm","title":"PyCharm","text":"<p>Enable: - Ruff integration - MyPy plugin - Git integration</p>"},{"location":"contributing/#debugging","title":"Debugging","text":"<pre><code># Use logging instead of print\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Add debug logging to your code\nlogging.debug(f\"Agent state: {agent.state_dict()}\")\n</code></pre>"},{"location":"contributing/#performance-profiling","title":"Performance Profiling","text":"<pre><code># Profile training\npython -m cProfile -o profile.prof examples/train_reinforce.py\n\n# Analyze profile\npython -m pstats profile.prof\n</code></pre> <p>Thank you for contributing to VibeRL! \ud83d\ude80</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the VibeRL API documentation. This section provides file-level documentation matching the exact structure of the source code.</p>"},{"location":"api/#file-structure","title":"File Structure","text":"<p>Each <code>.py</code> file in the source code has a corresponding <code>.md</code> file in this documentation:</p> <pre><code>viberl/\n\u251c\u2500\u2500 agents/\n\u2502   \u251c\u2500\u2500 base.py \u2192 base.md\n\u2502   \u251c\u2500\u2500 reinforce.py \u2192 reinforce.md\n\u2502   \u251c\u2500\u2500 dqn.py \u2192 dqn.md\n\u2502   \u2514\u2500\u2500 ppo.py \u2192 ppo.md\n\u251c\u2500\u2500 networks/\n\u2502   \u251c\u2500\u2500 base_network.py \u2192 base_network.md\n\u2502   \u251c\u2500\u2500 policy_network.py \u2192 policy_network.md\n\u2502   \u2514\u2500\u2500 value_network.py \u2192 value_network.md\n\u251c\u2500\u2500 envs/\n\u2502   \u2514\u2500\u2500 grid_world/\n\u2502       \u2514\u2500\u2500 snake_env.py \u2192 grid_world/snake_env.md\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 common.py \u2192 common.md\n\u2502   \u251c\u2500\u2500 training.py \u2192 training.md\n\u2502   \u251c\u2500\u2500 mock_env.py \u2192 mock_env.md\n\u2502   \u2514\u2500\u2500 experiment_manager.py \u2192 experiment_manager.md\n\u251c\u2500\u2500 typing.py \u2192 typing.md\n\u2514\u2500\u2500 cli.py \u2192 cli.md\n</code></pre>"},{"location":"api/#navigation","title":"Navigation","text":"<p>Each page contains: 1. Module Overview - File-level documentation with hyperlinks 2. Classes &amp; Functions - Detailed documentation for all classes and functions in the file</p> <p>Click on any file in the navigation menu to explore its contents.</p>"},{"location":"api/cli/","title":"<code>viberl.cli</code>","text":"<p>Command-line interface for Tiny RL.</p> <p>Functions:</p> Name Description <code>train_main</code> <p>Main training CLI entry point.</p> <code>eval_main</code> <p>Main evaluation CLI entry point.</p> <code>demo_main</code> <p>Demo CLI entry point.</p>"},{"location":"api/cli/#viberl.cli.train_main","title":"train_main","text":"<pre><code>train_main()\n</code></pre> <p>Main training CLI entry point.</p> Source code in <code>viberl/cli.py</code> <pre><code>def train_main():\n    \"\"\"Main training CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(description='Train RL agents')\n    parser.add_argument('--env', choices=['snake'], default='snake', help='Environment to train on')\n    parser.add_argument(\n        '--alg',\n        choices=['reinforce', 'dqn', 'ppo'],\n        default='reinforce',\n        help='Reinforcement learning algorithm',\n    )\n    parser.add_argument('--episodes', type=int, default=1000, help='Number of training episodes')\n    parser.add_argument('--grid-size', type=int, default=15, help='Grid size for snake environment')\n    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')\n    parser.add_argument('--gamma', type=float, default=0.99, help='Discount factor')\n    parser.add_argument('--hidden-size', type=int, default=128, help='Hidden layer size')\n    parser.add_argument('--num-hidden-layers', type=int, default=2, help='Number of hidden layers')\n    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n    parser.add_argument('--name', type=str, help='Experiment name (auto-generated if not provided)')\n    parser.add_argument('--render-interval', type=int, help='Render every N episodes')\n    parser.add_argument('--save-interval', type=int, help='Save model every N episodes')\n    parser.add_argument('--device', choices=['cpu', 'cuda'], default='auto', help='Device to use')\n    parser.add_argument(\n        '--epsilon-start', type=float, default=1.0, help='Initial exploration rate (DQN)'\n    )\n    parser.add_argument(\n        '--epsilon-end', type=float, default=0.01, help='Final exploration rate (DQN)'\n    )\n    parser.add_argument(\n        '--epsilon-decay', type=float, default=0.995, help='Exploration decay rate (DQN)'\n    )\n    parser.add_argument('--memory-size', type=int, default=10000, help='Replay memory size (DQN)')\n    parser.add_argument('--batch-size', type=int, default=64, help='Batch size for training')\n    parser.add_argument(\n        '--target-update', type=int, default=10, help='Target network update frequency (DQN)'\n    )\n    parser.add_argument('--clip-epsilon', type=float, default=0.2, help='PPO clipping parameter')\n    parser.add_argument('--ppo-epochs', type=int, default=4, help='PPO epochs per update')\n    parser.add_argument('--gae-lambda', type=float, default=0.95, help='GAE lambda parameter (PPO)')\n    parser.add_argument(\n        '--value-loss-coef', type=float, default=0.5, help='Value loss coefficient (PPO)'\n    )\n    parser.add_argument(\n        '--entropy-coef', type=float, default=0.01, help='Entropy coefficient (PPO)'\n    )\n    parser.add_argument('--max-grad-norm', type=float, default=0.5, help='Max gradient norm (PPO)')\n    parser.add_argument('--eval-episodes', type=int, default=10, help='Evaluation episodes')\n    parser.add_argument(\n        '--eval-interval', type=int, default=100, help='Evaluation interval during training'\n    )\n    parser.add_argument('--no-eval', action='store_true', help='Skip evaluation after training')\n    parser.add_argument('--quiet', action='store_true', help='Suppress training progress output')\n\n    args = parser.parse_args()\n\n    # Set random seed\n    set_seed(args.seed)\n\n    # Get device\n    device = get_device() if args.device == 'auto' else torch.device(args.device)\n\n    print(f'Using device: {device}')\n\n    # Create environment\n    if args.env == 'snake':\n        env = SnakeGameEnv(grid_size=args.grid_size)\n        state_size = args.grid_size * args.grid_size\n        action_size = 4\n    else:\n        raise ValueError(f'Unknown environment: {args.env}')\n\n    # Create agent\n    base_params = {\n        'state_size': state_size,\n        'action_size': action_size,\n        'learning_rate': args.lr,\n        'gamma': args.gamma,\n        'hidden_size': args.hidden_size,\n        'num_hidden_layers': args.num_hidden_layers,\n    }\n\n    if args.alg == 'reinforce':\n        agent = REINFORCEAgent(**base_params)\n    elif args.alg == 'dqn':\n        agent = DQNAgent(\n            **base_params,\n            epsilon_start=args.epsilon_start,\n            epsilon_end=args.epsilon_end,\n            epsilon_decay=args.epsilon_decay,\n            memory_size=args.memory_size,\n            batch_size=args.batch_size,\n            target_update=args.target_update,\n        )\n    elif args.alg == 'ppo':\n        agent = PPOAgent(\n            **base_params,\n            clip_epsilon=args.clip_epsilon,\n            ppo_epochs=args.ppo_epochs,\n            lam=args.gae_lambda,\n            value_loss_coef=args.value_loss_coef,\n            entropy_coef=args.entropy_coef,\n            max_grad_norm=args.max_grad_norm,\n            batch_size=args.batch_size,\n        )\n    else:\n        raise ValueError(f'Unknown algorithm: {args.alg}')\n\n    # Move agent to device\n    if hasattr(agent, 'policy_network'):\n        agent.policy_network.to(device)\n    if hasattr(agent, 'q_network'):\n        agent.q_network.to(device)\n        agent.target_network.to(device)\n    if hasattr(agent, 'value_network'):\n        agent.value_network.to(device)\n\n    print(f'Training {args.alg} agent on {args.env} environment...')\n    print(f'Episodes: {args.episodes}')\n    print(f'Grid size: {args.grid_size}')\n    print(f'Learning rate: {args.lr}')\n    print(f'Gamma: {args.gamma}')\n\n    # Create experiment with automatic directory structure\n    experiment_name = args.name or f'{args.alg}_{args.env}'\n    exp_manager = create_experiment(experiment_name)\n    tb_logs_dir = str(exp_manager.get_tb_logs_path())\n    models_dir = exp_manager.get_models_path()\n    save_path = str(models_dir / 'model')\n\n    if tb_logs_dir:\n        print(f'TensorBoard logs: {tb_logs_dir}')\n\n    # Train agent\n    train_agent(\n        env=env,\n        agent=agent,\n        num_episodes=args.episodes,\n        render_interval=args.render_interval,\n        save_interval=args.save_interval,\n        save_path=save_path,\n        verbose=True,\n        log_dir=tb_logs_dir,\n        eval_interval=args.eval_interval,\n        eval_episodes=args.eval_episodes,\n    )\n\n    # Save final model\n    final_model_path = str(models_dir / 'final_model.pth')\n    agent.save(final_model_path)\n    print(f'Final model saved to {final_model_path}')\n    print(f'All experiment files saved in: {exp_manager.get_experiment_path()}')\n\n    env.close()\n</code></pre>"},{"location":"api/cli/#viberl.cli.eval_main","title":"eval_main","text":"<pre><code>eval_main()\n</code></pre> <p>Main evaluation CLI entry point.</p> Source code in <code>viberl/cli.py</code> <pre><code>def eval_main():\n    \"\"\"Main evaluation CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(description='Evaluate trained RL agents')\n    parser.add_argument(\n        '--env', choices=['snake'], default='snake', help='Environment to evaluate on'\n    )\n    parser.add_argument(\n        '--agent', choices=['reinforce'], default='reinforce', help='Agent algorithm'\n    )\n    parser.add_argument('--model-path', type=str, required=True, help='Path to trained model')\n    parser.add_argument('--episodes', type=int, default=10, help='Number of evaluation episodes')\n    parser.add_argument('--grid-size', type=int, default=15, help='Grid size for snake environment')\n    parser.add_argument('--render', action='store_true', help='Render the environment')\n    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n    parser.add_argument('--device', choices=['cpu', 'cuda'], default='auto', help='Device to use')\n\n    args = parser.parse_args()\n\n    # Set random seed\n    set_seed(args.seed)\n\n    # Get device\n    device = get_device() if args.device == 'auto' else torch.device(args.device)\n\n    print(f'Using device: {device}')\n\n    # Create environment\n    if args.env == 'snake':\n        env = SnakeGameEnv(render_mode='human' if args.render else None, grid_size=args.grid_size)\n        state_size = args.grid_size * args.grid_size\n        action_size = 4\n    else:\n        raise ValueError(f'Unknown environment: {args.env}')\n\n    # Create agent\n    if args.agent == 'reinforce':\n        agent = REINFORCEAgent(state_size=state_size, action_size=action_size)\n    else:\n        raise ValueError(f'Unknown agent: {args.agent}')\n\n    # Move agent to device\n    agent.policy_network.to(device)\n\n    # Load trained model\n    try:\n        agent.load_policy(args.model_path)\n        print(f'Loaded model from {args.model_path}')\n    except OSError as e:\n        print(f'Failed to load model: {e}')\n        return\n\n    # Evaluate agent\n    print(f'Evaluating {args.agent} agent on {args.env} environment...')\n\n    scores = evaluate_agent(env=env, agent=agent, num_episodes=args.episodes, render=args.render)\n\n    print('\\nEvaluation Results:')\n    print(f'Average score: {np.mean(scores):.2f} \u00b1 {np.std(scores):.2f}')\n    print(f'Min score: {np.min(scores):.2f}')\n    print(f'Max score: {np.max(scores):.2f}')\n\n    env.close()\n</code></pre>"},{"location":"api/cli/#viberl.cli.demo_main","title":"demo_main","text":"<pre><code>demo_main()\n</code></pre> <p>Demo CLI entry point.</p> Source code in <code>viberl/cli.py</code> <pre><code>def demo_main():\n    \"\"\"Demo CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(description='Run RL framework demos')\n    parser.add_argument('--env', choices=['snake'], default='snake', help='Environment to demo')\n    parser.add_argument('--episodes', type=int, default=5, help='Number of demo episodes')\n    parser.add_argument('--grid-size', type=int, default=15, help='Grid size for snake environment')\n    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n\n    args = parser.parse_args()\n\n    # Set random seed\n    set_seed(args.seed)\n\n    # Create environment\n    if args.env == 'snake':\n        env = SnakeGameEnv(render_mode='human', grid_size=args.grid_size)\n    else:\n        raise ValueError(f'Unknown environment: {args.env}')\n\n    print(f'Running {args.env} demo for {args.episodes} episodes...')\n\n    # Run demo with random actions\n    for episode in range(args.episodes):\n        state, info = env.reset()\n        total_reward = 0\n        steps = 0\n\n        print(f'\\nEpisode {episode + 1}/{args.episodes}')\n\n        while True:\n            action = env.action_space.sample()  # Random action\n            state, reward, terminated, truncated, info = env.step(action)\n            total_reward += reward\n            steps += 1\n\n            if terminated or truncated:\n                print(\n                    f'Episode finished! Score: {info.get(\"score\", 0)}, Steps: {steps}, Total reward: {total_reward}'\n                )\n                break\n\n    env.close()\n    print('\\nDemo completed!')\n</code></pre>"},{"location":"api/typing/","title":"<code>viberl.typing</code>","text":"<p>Custom typing classes for reinforcement learning using Pydantic.</p> <p>Classes:</p> Name Description <code>Action</code> <p>An action taken by an agent, optionally with log probabilities.</p> <code>Transition</code> <p>A single transition in an episode.</p> <code>Trajectory</code> <p>A complete trajectory (episode) consisting of multiple transitions.</p>"},{"location":"api/typing/#viberl.typing.Action","title":"Action","text":"<pre><code>Action(**data: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>An action taken by an agent, optionally with log probabilities.</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <code>action</code> <code>int</code> <code>logprobs</code> <code>Tensor | None</code> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api/typing/#viberl.typing.Action.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/typing/#viberl.typing.Action.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: int\n</code></pre>"},{"location":"api/typing/#viberl.typing.Action.logprobs","title":"logprobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logprobs: Tensor | None = None\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition","title":"Transition","text":"<pre><code>Transition(**data: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>A single transition in an episode.</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <code>state</code> <code>ndarray</code> <code>action</code> <code>Action</code> <code>reward</code> <code>float</code> <code>next_state</code> <code>ndarray</code> <code>done</code> <code>bool</code> <code>info</code> <code>dict[str, Any]</code> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.state","title":"state  <code>instance-attribute</code>","text":"<pre><code>state: ndarray\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.action","title":"action  <code>instance-attribute</code>","text":"<pre><code>action: Action\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.reward","title":"reward  <code>instance-attribute</code>","text":"<pre><code>reward: float\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.next_state","title":"next_state  <code>instance-attribute</code>","text":"<pre><code>next_state: ndarray\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.done","title":"done  <code>instance-attribute</code>","text":"<pre><code>done: bool\n</code></pre>"},{"location":"api/typing/#viberl.typing.Transition.info","title":"info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>info: dict[str, Any] = {}\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory","title":"Trajectory","text":"<pre><code>Trajectory(**data: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>A complete trajectory (episode) consisting of multiple transitions.</p> <p>Methods:</p> Name Description <code>from_transitions</code> <p>Create a trajectory from a list of transitions.</p> <code>to_dict</code> <p>Convert trajectory to dictionary format for agent learning.</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <code>transitions</code> <code>list[Transition]</code> <code>total_reward</code> <code>float</code> <code>length</code> <code>int</code> Source code in <code>.venv/lib/python3.12/site-packages/pydantic/main.py</code> <pre><code>def __init__(self, /, **data: Any) -&gt; None:\n    \"\"\"Create a new model by parsing and validating input data from keyword arguments.\n\n    Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n    validated to form a valid model.\n\n    `self` is explicitly positional-only to allow `self` as a field name.\n    \"\"\"\n    # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    __tracebackhide__ = True\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    if self is not validated_self:\n        warnings.warn(\n            'A custom validator is returning a value other than `self`.\\n'\n            \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n            'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.transitions","title":"transitions  <code>instance-attribute</code>","text":"<pre><code>transitions: list[Transition]\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.total_reward","title":"total_reward  <code>instance-attribute</code>","text":"<pre><code>total_reward: float\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.length","title":"length  <code>instance-attribute</code>","text":"<pre><code>length: int\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.from_transitions","title":"from_transitions  <code>classmethod</code>","text":"<pre><code>from_transitions(transitions: list[Transition]) -&gt; Trajectory\n</code></pre> <p>Create a trajectory from a list of transitions.</p> Source code in <code>viberl/typing.py</code> <pre><code>@classmethod\ndef from_transitions(cls, transitions: list[Transition]) -&gt; Trajectory:\n    \"\"\"Create a trajectory from a list of transitions.\"\"\"\n    total_reward = sum(t.reward for t in transitions)\n    return cls(transitions=transitions, total_reward=total_reward, length=len(transitions))\n</code></pre>"},{"location":"api/typing/#viberl.typing.Trajectory.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert trajectory to dictionary format for agent learning.</p> Source code in <code>viberl/typing.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert trajectory to dictionary format for agent learning.\"\"\"\n    return {\n        'states': [t.state for t in self.transitions],\n        'actions': [t.action.action for t in self.transitions],\n        'rewards': [t.reward for t in self.transitions],\n        'next_states': [t.next_state for t in self.transitions],\n        'dones': [t.done for t in self.transitions],\n        'logprobs': [\n            t.action.logprobs for t in self.transitions if t.action.logprobs is not None\n        ],\n        'infos': [t.info for t in self.transitions],\n    }\n</code></pre>"},{"location":"api/agents/base/","title":"<code>viberl.agents.base</code>","text":"<p>Base Agent class for all RL agents.</p> <p>Classes:</p> Name Description <code>Agent</code> <p>Abstract base class for all RL agents.</p>"},{"location":"api/agents/base/#viberl.agents.base.Agent","title":"Agent","text":"<pre><code>Agent(state_size: int, action_size: int, **kwargs: dict)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all RL agents.</p> <p>Initialize the agent.</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <code>int</code> <p>Size of the state space</p> required <code>action_size</code> <code>int</code> <p>Size of the action space</p> required <code>**kwargs</code> <code>dict</code> <p>Additional configuration parameters</p> <code>{}</code> <p>Methods:</p> Name Description <code>act</code> <p>Select an action given the current state.</p> <code>learn</code> <p>Perform one learning step.</p> <code>reset</code> <p>Reset agent state for new episode.</p> <code>get_metrics</code> <p>Get current training metrics.</p> <code>save</code> <p>Save agent state to file.</p> <code>load</code> <p>Load agent state from file.</p> <code>setup_training</code> <p>Setup agent for training in given environment.</p> <code>setup_evaluation</code> <p>Setup agent for evaluation mode.</p> <p>Attributes:</p> Name Type Description <code>state_size</code> <code>action_size</code> Source code in <code>viberl/agents/base.py</code> <pre><code>def __init__(self, state_size: int, action_size: int, **kwargs: dict):\n    \"\"\"Initialize the agent.\n\n    Args:\n        state_size: Size of the state space\n        action_size: Size of the action space\n        **kwargs: Additional configuration parameters\n    \"\"\"\n    self.state_size = state_size\n    self.action_size = action_size\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.state_size","title":"state_size  <code>instance-attribute</code>","text":"<pre><code>state_size = state_size\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.action_size","title":"action_size  <code>instance-attribute</code>","text":"<pre><code>action_size = action_size\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.act","title":"act  <code>abstractmethod</code>","text":"<pre><code>act(state: ndarray, training: bool = True) -&gt; Action\n</code></pre> <p>Select an action given the current state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Current state observation</p> required <code>training</code> <code>bool</code> <p>Whether in training mode (affects exploration)</p> <code>True</code> <p>Returns:</p> Type Description <code>Action</code> <p>Action object containing the selected action and optional metadata</p> Source code in <code>viberl/agents/base.py</code> <pre><code>@abstractmethod\ndef act(self, state: np.ndarray, training: bool = True) -&gt; Action:\n    \"\"\"Select an action given the current state.\n\n    Args:\n        state: Current state observation\n        training: Whether in training mode (affects exploration)\n\n    Returns:\n        Action object containing the selected action and optional metadata\n    \"\"\"\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.learn","title":"learn  <code>abstractmethod</code>","text":"<pre><code>learn(**kwargs: dict) -&gt; dict[str, float]\n</code></pre> <p>Perform one learning step.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>dict</code> <p>Learning-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary of step metrics (e.g., loss values)</p> Source code in <code>viberl/agents/base.py</code> <pre><code>@abstractmethod\ndef learn(self, **kwargs: dict) -&gt; dict[str, float]:\n    \"\"\"Perform one learning step.\n\n    Args:\n        **kwargs: Learning-specific parameters\n\n    Returns:\n        Dictionary of step metrics (e.g., loss values)\n    \"\"\"\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset agent state for new episode.</p> Source code in <code>viberl/agents/base.py</code> <pre><code>def reset(self):\n    \"\"\"Reset agent state for new episode.\"\"\"\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.get_metrics","title":"get_metrics","text":"<pre><code>get_metrics() -&gt; dict[str, float]\n</code></pre> <p>Get current training metrics.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary of metrics</p> Source code in <code>viberl/agents/base.py</code> <pre><code>def get_metrics(self) -&gt; dict[str, float]:\n    \"\"\"Get current training metrics.\n\n    Returns:\n        Dictionary of metrics\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.save","title":"save","text":"<pre><code>save(filepath: str)\n</code></pre> <p>Save agent state to file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the model</p> required Source code in <code>viberl/agents/base.py</code> <pre><code>def save(self, filepath: str):\n    \"\"\"Save agent state to file.\n\n    Args:\n        filepath: Path to save the model\n    \"\"\"\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.load","title":"load","text":"<pre><code>load(filepath: str)\n</code></pre> <p>Load agent state from file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to load the model from</p> required Source code in <code>viberl/agents/base.py</code> <pre><code>def load(self, filepath: str):\n    \"\"\"Load agent state from file.\n\n    Args:\n        filepath: Path to load the model from\n    \"\"\"\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.setup_training","title":"setup_training","text":"<pre><code>setup_training(env: Env)\n</code></pre> <p>Setup agent for training in given environment.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Env</code> <p>Training environment</p> required Source code in <code>viberl/agents/base.py</code> <pre><code>def setup_training(self, env: gym.Env):\n    \"\"\"Setup agent for training in given environment.\n\n    Args:\n        env: Training environment\n    \"\"\"\n</code></pre>"},{"location":"api/agents/base/#viberl.agents.base.Agent.setup_evaluation","title":"setup_evaluation","text":"<pre><code>setup_evaluation()\n</code></pre> <p>Setup agent for evaluation mode.</p> Source code in <code>viberl/agents/base.py</code> <pre><code>def setup_evaluation(self):\n    \"\"\"Setup agent for evaluation mode.\"\"\"\n</code></pre>"},{"location":"api/agents/dqn/","title":"<code>viberl.agents.dqn</code>","text":"<p>Classes:</p> Name Description <code>DQNAgent</code> <p>Deep Q-Network (DQN) agent with experience replay and target network.</p>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent","title":"DQNAgent","text":"<pre><code>DQNAgent(\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 0.001,\n    gamma: float = 0.99,\n    epsilon_start: float = 1.0,\n    epsilon_end: float = 0.01,\n    epsilon_decay: float = 0.995,\n    memory_size: int = 10000,\n    batch_size: int = 64,\n    target_update: int = 10,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n)\n</code></pre> <p>               Bases: <code>Agent</code></p> <p>Deep Q-Network (DQN) agent with experience replay and target network.</p> <p>Implements the DQN algorithm with the following features: - Experience replay buffer for stable learning - Target network for reduced correlation - Epsilon-greedy exploration strategy - Batch learning for improved sample efficiency</p> <p>The algorithm learns Q-values that approximate the optimal action-value function using the Bellman equation: Q(s,a) = r + gamma * max_a' Q(s',a')</p> <p>Initialize DQN agent with comprehensive hyperparameter configuration.</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <code>int</code> <p>Size of the state space (input features to network)</p> required <code>action_size</code> <code>int</code> <p>Number of possible actions (output dimension)</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate for Adam optimizer (typically 1e-4 to 1e-2)</p> <code>0.001</code> <code>gamma</code> <code>float</code> <p>Discount factor for future rewards (0.9 to 0.99, default 0.99)</p> <code>0.99</code> <code>epsilon_start</code> <code>float</code> <p>Initial exploration rate (1.0 for full exploration)</p> <code>1.0</code> <code>epsilon_end</code> <code>float</code> <p>Minimum exploration rate (0.01 for minimal exploration)</p> <code>0.01</code> <code>epsilon_decay</code> <code>float</code> <p>Decay rate for epsilon per episode (0.995 for gradual decay)</p> <code>0.995</code> <code>memory_size</code> <code>int</code> <p>Size of experience replay buffer (1000 to 100000)</p> <code>10000</code> <code>batch_size</code> <code>int</code> <p>Number of samples per training batch (32 to 256)</p> <code>64</code> <code>target_update</code> <code>int</code> <p>Frequency of target network updates (in episodes)</p> <code>10</code> <code>hidden_size</code> <code>int</code> <p>Size of hidden layers in Q-network (64 to 512)</p> <code>128</code> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers (2 to 4)</p> <code>2</code> Note <p>The epsilon parameter starts at epsilon_start and decays exponentially: epsilon = max(epsilon_end, epsilon * epsilon_decay)</p> <p>Methods:</p> Name Description <code>act</code> <p>Select action using epsilon-greedy policy.</p> <code>learn</code> <p>Perform one learning step using Q-learning with experience replay.</p> <p>Attributes:</p> Name Type Description <code>learning_rate</code> <code>gamma</code> <code>epsilon_start</code> <code>epsilon_end</code> <code>epsilon_decay</code> <code>memory_size</code> <code>batch_size</code> <code>target_update</code> <code>epsilon</code> <code>q_network</code> <code>target_network</code> <code>optimizer</code> <code>memory</code> Source code in <code>viberl/agents/dqn.py</code> <pre><code>def __init__(\n    self,\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.99,\n    epsilon_start: float = 1.0,\n    epsilon_end: float = 0.01,\n    epsilon_decay: float = 0.995,\n    memory_size: int = 10000,\n    batch_size: int = 64,\n    target_update: int = 10,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n):\n    \"\"\"Initialize DQN agent with comprehensive hyperparameter configuration.\n\n    Args:\n        state_size: Size of the state space (input features to network)\n        action_size: Number of possible actions (output dimension)\n        learning_rate: Learning rate for Adam optimizer (typically 1e-4 to 1e-2)\n        gamma: Discount factor for future rewards (0.9 to 0.99, default 0.99)\n        epsilon_start: Initial exploration rate (1.0 for full exploration)\n        epsilon_end: Minimum exploration rate (0.01 for minimal exploration)\n        epsilon_decay: Decay rate for epsilon per episode (0.995 for gradual decay)\n        memory_size: Size of experience replay buffer (1000 to 100000)\n        batch_size: Number of samples per training batch (32 to 256)\n        target_update: Frequency of target network updates (in episodes)\n        hidden_size: Size of hidden layers in Q-network (64 to 512)\n        num_hidden_layers: Number of hidden layers (2 to 4)\n\n    Note:\n        The epsilon parameter starts at epsilon_start and decays exponentially:\n        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n    \"\"\"\n    super().__init__(state_size, action_size)\n    self.learning_rate = learning_rate\n    self.gamma = gamma\n    self.epsilon_start = epsilon_start\n    self.epsilon_end = epsilon_end\n    self.epsilon_decay = epsilon_decay\n    self.memory_size = memory_size\n    self.batch_size = batch_size\n    self.target_update = target_update\n    self.epsilon = epsilon_start\n\n    # Neural networks\n    self.q_network = QNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n    self.target_network = QNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n    self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n\n    # Copy weights to target network\n    self._update_target_network()\n\n    # Experience replay buffer\n    self.memory = deque(maxlen=memory_size)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.learning_rate","title":"learning_rate  <code>instance-attribute</code>","text":"<pre><code>learning_rate = learning_rate\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.epsilon_start","title":"epsilon_start  <code>instance-attribute</code>","text":"<pre><code>epsilon_start = epsilon_start\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.epsilon_end","title":"epsilon_end  <code>instance-attribute</code>","text":"<pre><code>epsilon_end = epsilon_end\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.epsilon_decay","title":"epsilon_decay  <code>instance-attribute</code>","text":"<pre><code>epsilon_decay = epsilon_decay\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.memory_size","title":"memory_size  <code>instance-attribute</code>","text":"<pre><code>memory_size = memory_size\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.target_update","title":"target_update  <code>instance-attribute</code>","text":"<pre><code>target_update = target_update\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.epsilon","title":"epsilon  <code>instance-attribute</code>","text":"<pre><code>epsilon = epsilon_start\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.q_network","title":"q_network  <code>instance-attribute</code>","text":"<pre><code>q_network = QNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.target_network","title":"target_network  <code>instance-attribute</code>","text":"<pre><code>target_network = QNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = Adam(parameters(), lr=learning_rate)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.memory","title":"memory  <code>instance-attribute</code>","text":"<pre><code>memory = deque(maxlen=memory_size)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.act","title":"act","text":"<pre><code>act(state: ndarray, training: bool = True) -&gt; Action\n</code></pre> <p>Select action using epsilon-greedy policy.</p> <p>Implements the epsilon-greedy action selection strategy: - In training mode: With probability epsilon, choose random action (exploration) - In training mode: With probability 1-epsilon, choose best action (exploitation) - In evaluation mode: Always choose best action (greedy)</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Current state as numpy array</p> required <code>training</code> <code>bool</code> <p>Whether in training mode (affects epsilon-greedy behavior)</p> <code>True</code> <p>Returns:</p> Type Description <code>Action</code> <p>Action object containing the selected action</p> Example <p>agent = DQNAgent(state_size=4, action_size=2) state = np.array([0.1, 0.2, 0.3, 0.4]) action = agent.act(state)  # Uses epsilon-greedy action = agent.act(state, training=False)  # Greedy action</p> Source code in <code>viberl/agents/dqn.py</code> <pre><code>def act(self, state: np.ndarray, training: bool = True) -&gt; Action:\n    \"\"\"Select action using epsilon-greedy policy.\n\n    Implements the epsilon-greedy action selection strategy:\n    - In training mode: With probability epsilon, choose random action (exploration)\n    - In training mode: With probability 1-epsilon, choose best action (exploitation)\n    - In evaluation mode: Always choose best action (greedy)\n\n    Args:\n        state: Current state as numpy array\n        training: Whether in training mode (affects epsilon-greedy behavior)\n\n    Returns:\n        Action object containing the selected action\n\n    Example:\n        &gt;&gt;&gt; agent = DQNAgent(state_size=4, action_size=2)\n        &gt;&gt;&gt; state = np.array([0.1, 0.2, 0.3, 0.4])\n        &gt;&gt;&gt; action = agent.act(state)  # Uses epsilon-greedy\n        &gt;&gt;&gt; action = agent.act(state, training=False)  # Greedy action\n    \"\"\"\n    if training and random.random() &lt; self.epsilon:\n        action = random.randint(0, self.action_size - 1)\n    else:\n        with torch.no_grad():\n            q_values = self.q_network.get_q_values(state)\n            action = q_values.argmax().item()\n\n    return Action(action=action)\n</code></pre>"},{"location":"api/agents/dqn/#viberl.agents.dqn.DQNAgent.learn","title":"learn","text":"<pre><code>learn(trajectory: Trajectory, **kwargs) -&gt; dict[str, float]\n</code></pre> <p>Perform one learning step using Q-learning with experience replay.</p> <p>The algorithm follows these steps: 1. Store new transitions in replay buffer 2. Sample a batch from memory if enough experiences exist 3. Compute Q-learning targets using target network 4. Update Q-network using mean squared error loss 5. Decay epsilon for exploration 6. Update target network periodically</p> <p>The Q-learning update rule: L = (r + gamma * max_a' Q_target(s', a') - Q(s, a))\u00b2</p> <p>Parameters:</p> Name Type Description Default <code>trajectory</code> <code>Trajectory</code> <p>Complete trajectory containing transitions to learn from</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary containing training metrics:</p> <code>dict[str, float]</code> <ul> <li>'dqn/loss': Mean squared error loss</li> </ul> <code>dict[str, float]</code> <ul> <li>'dqn/epsilon': Current exploration rate</li> </ul> <code>dict[str, float]</code> <ul> <li>'dqn/memory_size': Number of experiences in buffer</li> </ul> Note <p>Learning only occurs when the replay buffer contains at least batch_size experiences to ensure stable training.</p> Source code in <code>viberl/agents/dqn.py</code> <pre><code>def learn(\n    self,\n    trajectory: Trajectory,\n    **kwargs,\n) -&gt; dict[str, float]:\n    \"\"\"Perform one learning step using Q-learning with experience replay.\n\n    The algorithm follows these steps:\n    1. Store new transitions in replay buffer\n    2. Sample a batch from memory if enough experiences exist\n    3. Compute Q-learning targets using target network\n    4. Update Q-network using mean squared error loss\n    5. Decay epsilon for exploration\n    6. Update target network periodically\n\n    The Q-learning update rule:\n    L = (r + gamma * max_a' Q_target(s', a') - Q(s, a))\u00b2\n\n    Args:\n        trajectory: Complete trajectory containing transitions to learn from\n\n    Returns:\n        Dictionary containing training metrics:\n        - 'dqn/loss': Mean squared error loss\n        - 'dqn/epsilon': Current exploration rate\n        - 'dqn/memory_size': Number of experiences in buffer\n\n    Note:\n        Learning only occurs when the replay buffer contains at least\n        batch_size experiences to ensure stable training.\n    \"\"\"\n    if not trajectory.transitions:\n        return {}\n\n    # Store transitions in memory\n    for transition in trajectory.transitions:\n        self.memory.append(transition)\n\n    if len(self.memory) &lt; self.batch_size:\n        return {'dqn/memory_size': len(self.memory)}\n\n    # Sample batch from memory\n    batch = random.sample(self.memory, self.batch_size)\n\n    # Extract batch data\n    states = torch.FloatTensor([t.state for t in batch])\n    actions = torch.LongTensor([t.action.action for t in batch])\n    rewards = torch.FloatTensor([t.reward for t in batch])\n    next_states = torch.FloatTensor([t.next_state for t in batch])\n    dones = torch.BoolTensor([t.done for t in batch])\n\n    # Current Q values\n    current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n\n    # Next Q values from target network\n    with torch.no_grad():\n        next_q_values = self.target_network(next_states).max(1)[0]\n        target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n\n    # Compute loss\n    loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n    # Optimize\n    self.optimizer.zero_grad()\n    loss.backward()\n    self.optimizer.step()\n\n    # Update target network\n    self._update_target_network()\n\n    # Decay epsilon\n    self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n\n    return {\n        'dqn/loss': loss.item(),\n        'dqn/epsilon': self.epsilon,\n        'dqn/memory_size': len(self.memory),\n    }\n</code></pre>"},{"location":"api/agents/ppo/","title":"<code>viberl.agents.ppo</code>","text":"<p>Proximal Policy Optimization (PPO) agent implementation.</p> <p>PPO is a policy gradient method that uses a clipped surrogate objective to prevent large policy updates, making training more stable.</p> <p>Classes:</p> Name Description <code>PPOAgent</code> <p>Proximal Policy Optimization (PPO) agent.</p>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent","title":"PPOAgent","text":"<pre><code>PPOAgent(\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 0.0003,\n    gamma: float = 0.99,\n    lam: float = 0.95,\n    clip_epsilon: float = 0.2,\n    value_loss_coef: float = 0.5,\n    entropy_coef: float = 0.01,\n    max_grad_norm: float = 0.5,\n    ppo_epochs: int = 4,\n    batch_size: int = 64,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n    device: str = 'auto',\n)\n</code></pre> <p>               Bases: <code>Agent</code></p> <p>Proximal Policy Optimization (PPO) agent.</p> <p>Methods:</p> Name Description <code>act</code> <p>Select action using current policy.</p> <code>learn</code> <p>Update policy and value networks using PPO.</p> <p>Attributes:</p> Name Type Description <code>gamma</code> <code>lam</code> <code>clip_epsilon</code> <code>value_loss_coef</code> <code>entropy_coef</code> <code>max_grad_norm</code> <code>ppo_epochs</code> <code>batch_size</code> <code>device</code> <code>policy_network</code> <code>value_network</code> <code>optimizer</code> Source code in <code>viberl/agents/ppo.py</code> <pre><code>def __init__(\n    self,\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 3e-4,\n    gamma: float = 0.99,\n    lam: float = 0.95,\n    clip_epsilon: float = 0.2,\n    value_loss_coef: float = 0.5,\n    entropy_coef: float = 0.01,\n    max_grad_norm: float = 0.5,\n    ppo_epochs: int = 4,\n    batch_size: int = 64,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n    device: str = 'auto',\n):\n    super().__init__(state_size, action_size)\n    self.gamma = gamma\n    self.lam = lam\n    self.clip_epsilon = clip_epsilon\n    self.value_loss_coef = value_loss_coef\n    self.entropy_coef = entropy_coef\n    self.max_grad_norm = max_grad_norm\n    self.ppo_epochs = ppo_epochs\n    self.batch_size = batch_size\n\n    # Set device\n    if device == 'auto':\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    else:\n        self.device = torch.device(device)\n\n    # Initialize networks\n    self.policy_network = PolicyNetwork(\n        state_size=state_size,\n        action_size=action_size,\n        hidden_size=hidden_size,\n        num_hidden_layers=num_hidden_layers,\n    ).to(self.device)\n\n    self.value_network = VNetwork(\n        state_size=state_size,\n        hidden_size=hidden_size,\n        num_hidden_layers=num_hidden_layers,\n    ).to(self.device)\n\n    # Initialize optimizer\n    self.optimizer = torch.optim.Adam(\n        list(self.policy_network.parameters()) + list(self.value_network.parameters()),\n        lr=learning_rate,\n    )\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.lam","title":"lam  <code>instance-attribute</code>","text":"<pre><code>lam = lam\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.clip_epsilon","title":"clip_epsilon  <code>instance-attribute</code>","text":"<pre><code>clip_epsilon = clip_epsilon\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.value_loss_coef","title":"value_loss_coef  <code>instance-attribute</code>","text":"<pre><code>value_loss_coef = value_loss_coef\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.entropy_coef","title":"entropy_coef  <code>instance-attribute</code>","text":"<pre><code>entropy_coef = entropy_coef\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.max_grad_norm","title":"max_grad_norm  <code>instance-attribute</code>","text":"<pre><code>max_grad_norm = max_grad_norm\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.ppo_epochs","title":"ppo_epochs  <code>instance-attribute</code>","text":"<pre><code>ppo_epochs = ppo_epochs\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device('cuda' if is_available() else 'cpu')\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.policy_network","title":"policy_network  <code>instance-attribute</code>","text":"<pre><code>policy_network = to(device)\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.value_network","title":"value_network  <code>instance-attribute</code>","text":"<pre><code>value_network = to(device)\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = Adam(list(parameters()) + list(parameters()), lr=learning_rate)\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.act","title":"act","text":"<pre><code>act(state: ndarray, training: bool = True) -&gt; Action\n</code></pre> <p>Select action using current policy.</p> Source code in <code>viberl/agents/ppo.py</code> <pre><code>def act(self, state: np.ndarray, training: bool = True) -&gt; Action:\n    \"\"\"Select action using current policy.\"\"\"\n    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n    with torch.no_grad():\n        action_probs = self.policy_network(state_tensor)\n        dist = Categorical(action_probs)\n        action = dist.sample()\n        log_prob = dist.log_prob(action)\n\n    return Action(action=action.item(), logprobs=log_prob)\n</code></pre>"},{"location":"api/agents/ppo/#viberl.agents.ppo.PPOAgent.learn","title":"learn","text":"<pre><code>learn(trajectory: Trajectory, **kwargs) -&gt; dict[str, float]\n</code></pre> <p>Update policy and value networks using PPO.</p> <p>Parameters:</p> Name Type Description Default <code>trajectory</code> <code>Trajectory</code> <p>A complete trajectory containing transitions with logprobs and values</p> required Source code in <code>viberl/agents/ppo.py</code> <pre><code>def learn(\n    self,\n    trajectory: Trajectory,\n    **kwargs,\n) -&gt; dict[str, float]:\n    \"\"\"Update policy and value networks using PPO.\n\n    Args:\n        trajectory: A complete trajectory containing transitions with logprobs and values\n    \"\"\"\n    if not trajectory.transitions:\n        return {}\n\n    # Extract data from trajectory\n    states = [t.state for t in trajectory.transitions]\n    actions = [t.action.action for t in trajectory.transitions]\n    rewards = [t.reward for t in trajectory.transitions]\n    log_probs = [\n        t.action.logprobs.item() if t.action.logprobs is not None else 0.0\n        for t in trajectory.transitions\n    ]\n\n    # Compute values for each state\n    values = []\n    for state in states:\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        with torch.no_grad():\n            value = self.value_network(state_tensor).squeeze(-1).item()\n            values.append(value)\n\n    dones = [t.done for t in trajectory.transitions]\n\n    # Convert to tensors\n    states_tensor = torch.FloatTensor(np.array(states)).to(self.device)\n    actions_tensor = torch.LongTensor(actions).to(self.device)\n    old_log_probs_tensor = torch.FloatTensor(log_probs).to(self.device)\n\n    # Compute advantages and returns\n    advantages, returns = self._compute_gae(rewards, values, dones)\n    advantages_tensor = torch.FloatTensor(advantages).to(self.device)\n    returns_tensor = torch.FloatTensor(returns).to(self.device)\n\n    # Normalize advantages (handle small sample sizes)\n    if len(advantages_tensor) &gt; 1:\n        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (\n            advantages_tensor.std() + 1e-8\n        )\n    else:\n        advantages_tensor = advantages_tensor - advantages_tensor.mean()\n\n    # Create dataset\n    dataset_size = len(states)\n    indices = np.arange(dataset_size)\n\n    metrics = {\n        'ppo/policy_loss': 0.0,\n        'ppo/value_loss': 0.0,\n        'ppo/entropy_loss': 0.0,\n        'ppo/total_loss': 0.0,\n    }\n\n    # PPO epochs\n    for _epoch in range(self.ppo_epochs):\n        np.random.shuffle(indices)\n\n        for start in range(0, dataset_size, self.batch_size):\n            end = start + self.batch_size\n            batch_indices = indices[start:end]\n\n            batch_states = states_tensor[batch_indices]\n            batch_actions = actions_tensor[batch_indices]\n            batch_old_log_probs = old_log_probs_tensor[batch_indices]\n            batch_advantages = advantages_tensor[batch_indices]\n            batch_returns = returns_tensor[batch_indices]\n\n            # Forward pass\n            action_probs = self.policy_network(batch_states)\n            # Ensure action_probs are valid probabilities\n            action_probs = torch.clamp(action_probs, 1e-8, 1 - 1e-8)\n            action_probs = action_probs / action_probs.sum(dim=1, keepdim=True)\n\n            values = self.value_network(batch_states).squeeze(-1)\n\n            dist = Categorical(action_probs)\n            new_log_probs = dist.log_prob(batch_actions)\n            entropy = dist.entropy()\n\n            # Compute ratio for PPO\n            ratio = torch.exp(new_log_probs - batch_old_log_probs)\n\n            # Clipped surrogate objective\n            surr1 = ratio * batch_advantages\n            surr2 = (\n                torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n                * batch_advantages\n            )\n            policy_loss = -torch.min(surr1, surr2).mean()\n\n            # Value loss\n            value_loss = nn.MSELoss()(values.squeeze(), batch_returns.squeeze())\n\n            # Entropy loss\n            entropy_loss = -entropy.mean()\n\n            # Total loss\n            total_loss = (\n                policy_loss\n                + self.value_loss_coef * value_loss\n                + self.entropy_coef * entropy_loss\n            )\n\n            # Update networks\n            self.optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(\n                list(self.policy_network.parameters()) + list(self.value_network.parameters()),\n                self.max_grad_norm,\n            )\n            self.optimizer.step()\n\n            # Accumulate metrics\n            metrics['ppo/policy_loss'] += policy_loss.item()\n            metrics['ppo/value_loss'] += value_loss.item()\n            metrics['ppo/entropy_loss'] += entropy_loss.item()\n            metrics['ppo/total_loss'] += total_loss.item()\n\n    # Average metrics over all batches and epochs\n    num_batches = (dataset_size + self.batch_size - 1) // self.batch_size\n    for key in metrics:\n        metrics[key] /= num_batches * self.ppo_epochs\n\n    return metrics\n</code></pre>"},{"location":"api/agents/reinforce/","title":"<code>viberl.agents.reinforce</code>","text":"<p>Classes:</p> Name Description <code>REINFORCEAgent</code> <p>REINFORCE policy gradient agent.</p>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent","title":"REINFORCEAgent","text":"<pre><code>REINFORCEAgent(\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 0.001,\n    gamma: float = 0.99,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n)\n</code></pre> <p>               Bases: <code>Agent</code></p> <p>REINFORCE policy gradient agent.</p> <p>Initialize REINFORCE agent.</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <code>int</code> <p>Size of the state space</p> required <code>action_size</code> <code>int</code> <p>Size of the action space</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate for policy optimization</p> <code>0.001</code> <code>gamma</code> <code>float</code> <p>Discount factor for future rewards (0.0 to 1.0)</p> <code>0.99</code> <code>hidden_size</code> <code>int</code> <p>Size of hidden layers in policy network</p> <code>128</code> <code>num_hidden_layers</code> <code>int</code> <p>Number of hidden layers in policy network</p> <code>2</code> <p>Methods:</p> Name Description <code>act</code> <p>Select action using current policy.</p> <code>learn</code> <p>Perform one learning step using REINFORCE algorithm.</p> <p>Attributes:</p> Name Type Description <code>gamma</code> <code>policy_network</code> <code>optimizer</code> Source code in <code>viberl/agents/reinforce.py</code> <pre><code>def __init__(\n    self,\n    state_size: int,\n    action_size: int,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.99,\n    hidden_size: int = 128,\n    num_hidden_layers: int = 2,\n):\n    \"\"\"Initialize REINFORCE agent.\n\n    Args:\n        state_size: Size of the state space\n        action_size: Size of the action space\n        learning_rate: Learning rate for policy optimization\n        gamma: Discount factor for future rewards (0.0 to 1.0)\n        hidden_size: Size of hidden layers in policy network\n        num_hidden_layers: Number of hidden layers in policy network\n    \"\"\"\n    super().__init__(state_size, action_size)\n    self.gamma = gamma\n    self.policy_network = PolicyNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n    self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.policy_network","title":"policy_network  <code>instance-attribute</code>","text":"<pre><code>policy_network = PolicyNetwork(state_size, action_size, hidden_size, num_hidden_layers)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = Adam(parameters(), lr=learning_rate)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.act","title":"act","text":"<pre><code>act(state: ndarray, training: bool = True) -&gt; Action\n</code></pre> <p>Select action using current policy.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Current state as numpy array</p> required <code>training</code> <code>bool</code> <p>Whether in training mode (affects action selection)</p> <code>True</code> <p>Returns:</p> Type Description <code>Action</code> <p>Action object containing the selected action</p> Source code in <code>viberl/agents/reinforce.py</code> <pre><code>def act(self, state: np.ndarray, training: bool = True) -&gt; Action:\n    \"\"\"Select action using current policy.\n\n    Args:\n        state: Current state as numpy array\n        training: Whether in training mode (affects action selection)\n\n    Returns:\n        Action object containing the selected action\n    \"\"\"\n    action = self.policy_network.act(state)\n    return Action(action=action)\n</code></pre>"},{"location":"api/agents/reinforce/#viberl.agents.reinforce.REINFORCEAgent.learn","title":"learn","text":"<pre><code>learn(trajectory: Trajectory, **kwargs) -&gt; dict[str, float]\n</code></pre> <p>Perform one learning step using REINFORCE algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>trajectory</code> <code>Trajectory</code> <p>A complete trajectory containing transitions</p> required Source code in <code>viberl/agents/reinforce.py</code> <pre><code>def learn(\n    self,\n    trajectory: Trajectory,\n    **kwargs,\n) -&gt; dict[str, float]:\n    \"\"\"Perform one learning step using REINFORCE algorithm.\n\n    Args:\n        trajectory: A complete trajectory containing transitions\n    \"\"\"\n    if not trajectory.transitions:\n        return {}\n\n    # Extract data from trajectory\n    states = [t.state for t in trajectory.transitions]\n    actions = [t.action.action for t in trajectory.transitions]\n    rewards = [t.reward for t in trajectory.transitions]\n\n    # Compute returns\n    returns = self._compute_returns(rewards)\n\n    # Normalize returns for stability\n    returns = torch.FloatTensor(returns)\n    if returns.std() &gt; 0:\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n\n    # Convert states and actions to tensors\n    states_tensor = torch.FloatTensor(np.array(states))\n    actions_tensor = torch.LongTensor(actions)\n\n    # Get action probabilities\n    action_probs = self.policy_network(states_tensor)\n\n    # Compute loss\n    m = Categorical(action_probs)\n    log_probs = m.log_prob(actions_tensor)\n    loss = -torch.mean(log_probs * returns)\n\n    # Update policy\n    self.optimizer.zero_grad()\n    loss.backward()\n    self.optimizer.step()\n\n    return {\n        'reinforce/policy_loss': loss.item(),\n        'reinforce/return_mean': returns.mean().item(),\n    }\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/","title":"<code>viberl.envs.grid_world.snake_env</code>","text":"<p>Classes:</p> Name Description <code>Direction</code> <code>SnakeGameEnv</code>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction","title":"Direction","text":"<p>               Bases: <code>Enum</code></p> <p>Attributes:</p> Name Type Description <code>UP</code> <code>RIGHT</code> <code>DOWN</code> <code>LEFT</code>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction.UP","title":"UP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UP = 0\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction.RIGHT","title":"RIGHT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>RIGHT = 1\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction.DOWN","title":"DOWN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DOWN = 2\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.Direction.LEFT","title":"LEFT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LEFT = 3\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv","title":"SnakeGameEnv","text":"<pre><code>SnakeGameEnv(render_mode: str | None = None, grid_size: int = 20)\n</code></pre> <p>               Bases: <code>Env</code></p> <p>Methods:</p> Name Description <code>reset</code> <code>step</code> <code>render</code> <code>close</code> <p>Attributes:</p> Name Type Description <code>metadata</code> <code>dict[str, Any]</code> <code>grid_size</code> <code>render_mode</code> <code>action_space</code> <code>observation_space</code> <code>window</code> <code>clock</code> <code>cell_size</code> <code>window_size</code> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def __init__(self, render_mode: str | None = None, grid_size: int = 20):\n    super().__init__()\n\n    self.grid_size = grid_size\n    self.render_mode = render_mode\n\n    # Action space: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n    self.action_space = spaces.Discrete(4)\n\n    # Observation space: grid with snake body, food, and empty spaces\n    self.observation_space = spaces.Box(\n        low=0, high=3, shape=(grid_size, grid_size), dtype=np.uint8\n    )\n\n    # Initialize game state\n    self.reset()\n\n    # Pygame setup for rendering\n    self.window = None\n    self.clock = None\n    self.cell_size = 20\n    self.window_size = self.grid_size * self.cell_size\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.metadata","title":"metadata  <code>class-attribute</code>","text":"<pre><code>metadata: dict[str, Any] = {'render_modes': ['human', 'rgb_array'], 'render_fps': 10}\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.grid_size","title":"grid_size  <code>instance-attribute</code>","text":"<pre><code>grid_size = grid_size\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.render_mode","title":"render_mode  <code>instance-attribute</code>","text":"<pre><code>render_mode = render_mode\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.action_space","title":"action_space  <code>instance-attribute</code>","text":"<pre><code>action_space = Discrete(4)\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.observation_space","title":"observation_space  <code>instance-attribute</code>","text":"<pre><code>observation_space = Box(low=0, high=3, shape=(grid_size, grid_size), dtype=uint8)\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.window","title":"window  <code>instance-attribute</code>","text":"<pre><code>window = None\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.clock","title":"clock  <code>instance-attribute</code>","text":"<pre><code>clock = None\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.cell_size","title":"cell_size  <code>instance-attribute</code>","text":"<pre><code>cell_size = 20\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.window_size","title":"window_size  <code>instance-attribute</code>","text":"<pre><code>window_size = grid_size * cell_size\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.reset","title":"reset","text":"<pre><code>reset(seed: int | None = None, options: dict | None = None) -&gt; tuple[ndarray, dict[str, Any]]\n</code></pre> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def reset(\n    self, seed: int | None = None, options: dict | None = None\n) -&gt; tuple[np.ndarray, dict[str, Any]]:\n    super().reset(seed=seed)\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Initialize snake at center of grid\n    center = self.grid_size // 2\n    self.snake = [(center - 1, center), (center, center), (center + 1, center)]\n    self.direction = Direction.RIGHT\n\n    # Place food\n    self.food = self._place_food()\n\n    # Game state\n    self.game_over = False\n    self.score = 0\n    self.steps = 0\n    self.max_steps = self.grid_size * self.grid_size * 4\n\n    return self._get_observation(), self._get_info()\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.step","title":"step","text":"<pre><code>step(action: int) -&gt; tuple[ndarray, float, bool, bool, dict[str, Any]]\n</code></pre> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def step(self, action: int) -&gt; tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n    if self.game_over:\n        return self._get_observation(), 0.0, True, False, self._get_info()\n\n    self.steps += 1\n\n    # Convert action to direction (ensure snake doesn't reverse into itself)\n    new_direction = Direction(action)\n\n    # Prevent moving directly opposite to current direction\n    if (\n        (new_direction == Direction.UP and self.direction == Direction.DOWN)\n        or (new_direction == Direction.DOWN and self.direction == Direction.UP)\n        or (new_direction == Direction.LEFT and self.direction == Direction.RIGHT)\n        or (new_direction == Direction.RIGHT and self.direction == Direction.LEFT)\n    ):\n        new_direction = self.direction\n\n    self.direction = new_direction\n\n    # Move snake\n    head_x, head_y = self.snake[-1]\n\n    if self.direction == Direction.UP:\n        new_head = (head_x - 1, head_y)\n    elif self.direction == Direction.RIGHT:\n        new_head = (head_x, head_y + 1)\n    elif self.direction == Direction.DOWN:\n        new_head = (head_x + 1, head_y)\n    elif self.direction == Direction.LEFT:\n        new_head = (head_x, head_y - 1)\n\n    # Check collision with walls\n    if (\n        new_head[0] &lt; 0\n        or new_head[0] &gt;= self.grid_size\n        or new_head[1] &lt; 0\n        or new_head[1] &gt;= self.grid_size\n    ):\n        self.game_over = True\n        return self._get_observation(), -10.0, True, False, self._get_info()\n\n    # Check collision with self\n    if new_head in self.snake[:-1]:\n        self.game_over = True\n        return self._get_observation(), -10.0, True, False, self._get_info()\n\n    # Move snake\n    self.snake.append(new_head)\n\n    reward = 0.0\n\n    # Check if food eaten\n    if new_head == self.food:\n        self.score += 1\n        reward = 10.0\n        self.food = self._place_food()\n    else:\n        # Remove tail if no food eaten\n        self.snake.pop(0)\n\n    # Give negative reward to encourage faster completion\n    reward += -0.1\n\n    # Check if maximum steps reached\n    if self.steps &gt;= self.max_steps:\n        self.game_over = True\n        return self._get_observation(), reward, True, False, self._get_info()\n\n    return self._get_observation(), reward, False, False, self._get_info()\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.render","title":"render","text":"<pre><code>render() -&gt; None | ndarray\n</code></pre> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def render(self) -&gt; None | np.ndarray:\n    if self.render_mode is None:\n        return None\n\n    if self.render_mode == 'rgb_array':\n        return self._render_frame()\n    if self.render_mode == 'human':\n        self._render_frame()\n</code></pre>"},{"location":"api/envs/grid_world/snake_env/#viberl.envs.grid_world.snake_env.SnakeGameEnv.close","title":"close","text":"<pre><code>close()\n</code></pre> Source code in <code>viberl/envs/grid_world/snake_env.py</code> <pre><code>def close(self):\n    if self.window is not None:\n        pygame.display.quit()\n        pygame.quit()\n</code></pre>"},{"location":"api/networks/base_network/","title":"<code>viberl.networks.base_network</code>","text":"<p>Classes:</p> Name Description <code>BaseNetwork</code> <p>Base neural network architecture for RL agents.</p>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork","title":"BaseNetwork","text":"<pre><code>BaseNetwork(input_size: int, hidden_size: int = 128, num_hidden_layers: int = 2)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base neural network architecture for RL agents.</p> <p>Methods:</p> Name Description <code>forward_backbone</code> <p>Forward pass through the shared backbone.</p> <code>init_weights</code> <p>Initialize network weights using Xavier initialization.</p> <p>Attributes:</p> Name Type Description <code>input_size</code> <code>hidden_size</code> <code>num_hidden_layers</code> <code>backbone</code> Source code in <code>viberl/networks/base_network.py</code> <pre><code>def __init__(self, input_size: int, hidden_size: int = 128, num_hidden_layers: int = 2):\n    super().__init__()\n    self.input_size = input_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n\n    # Build network layers\n    layers = []\n    layers.append(nn.Linear(input_size, hidden_size))\n    layers.append(nn.ReLU())\n\n    for _ in range(num_hidden_layers - 1):\n        layers.append(nn.Linear(hidden_size, hidden_size))\n        layers.append(nn.ReLU())\n\n    self.backbone = nn.Sequential(*layers)\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.input_size","title":"input_size  <code>instance-attribute</code>","text":"<pre><code>input_size = input_size\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.hidden_size","title":"hidden_size  <code>instance-attribute</code>","text":"<pre><code>hidden_size = hidden_size\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.num_hidden_layers","title":"num_hidden_layers  <code>instance-attribute</code>","text":"<pre><code>num_hidden_layers = num_hidden_layers\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone = Sequential(*layers)\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.forward_backbone","title":"forward_backbone","text":"<pre><code>forward_backbone(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass through the shared backbone.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_size)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Processed tensor of shape (batch_size, hidden_size)</p> Source code in <code>viberl/networks/base_network.py</code> <pre><code>def forward_backbone(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the shared backbone.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_size)\n\n    Returns:\n        Processed tensor of shape (batch_size, hidden_size)\n    \"\"\"\n    return self.backbone(x)\n</code></pre>"},{"location":"api/networks/base_network/#viberl.networks.base_network.BaseNetwork.init_weights","title":"init_weights","text":"<pre><code>init_weights() -&gt; None\n</code></pre> <p>Initialize network weights using Xavier initialization.</p> <p>Uses Xavier uniform initialization for linear layers and zeros for biases. This helps with stable gradient flow during training and prevents vanishing/exploding gradients.</p> Source code in <code>viberl/networks/base_network.py</code> <pre><code>def init_weights(self) -&gt; None:\n    \"\"\"Initialize network weights using Xavier initialization.\n\n    Uses Xavier uniform initialization for linear layers and zeros for biases.\n    This helps with stable gradient flow during training and prevents\n    vanishing/exploding gradients.\n    \"\"\"\n    for module in self.modules():\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0)\n</code></pre>"},{"location":"api/networks/policy_network/","title":"<code>viberl.networks.policy_network</code>","text":"<p>Classes:</p> Name Description <code>PolicyNetwork</code> <p>Policy network for policy gradient methods like REINFORCE.</p>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork","title":"PolicyNetwork","text":"<pre><code>PolicyNetwork(\n    state_size: int, action_size: int, hidden_size: int = 128, num_hidden_layers: int = 2\n)\n</code></pre> <p>               Bases: <code>BaseNetwork</code></p> <p>Policy network for policy gradient methods like REINFORCE.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass to get action probabilities.</p> <code>act</code> <p>Select action based on current policy.</p> <code>get_action_prob</code> <p>Get probability of taking a specific action.</p> <p>Attributes:</p> Name Type Description <code>action_size</code> <code>policy_head</code> <code>softmax</code> Source code in <code>viberl/networks/policy_network.py</code> <pre><code>def __init__(\n    self, state_size: int, action_size: int, hidden_size: int = 128, num_hidden_layers: int = 2\n):\n    super().__init__(state_size, hidden_size, num_hidden_layers)\n    self.action_size = action_size\n\n    # Policy head\n    self.policy_head = nn.Linear(hidden_size, action_size)\n    self.softmax = nn.Softmax(dim=-1)\n\n    self.init_weights()\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.action_size","title":"action_size  <code>instance-attribute</code>","text":"<pre><code>action_size = action_size\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.policy_head","title":"policy_head  <code>instance-attribute</code>","text":"<pre><code>policy_head = Linear(hidden_size, action_size)\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.softmax","title":"softmax  <code>instance-attribute</code>","text":"<pre><code>softmax = Softmax(dim=-1)\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass to get action probabilities.</p> <p>Processes state features through the backbone network and policy head to produce normalized action probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input state tensor of shape (batch_size, state_size)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Action probabilities tensor of shape (batch_size, action_size)</p> <code>Tensor</code> <p>with values summing to 1 along the action dimension</p> Source code in <code>viberl/networks/policy_network.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass to get action probabilities.\n\n    Processes state features through the backbone network and policy head\n    to produce normalized action probabilities.\n\n    Args:\n        x: Input state tensor of shape (batch_size, state_size)\n\n    Returns:\n        Action probabilities tensor of shape (batch_size, action_size)\n        with values summing to 1 along the action dimension\n    \"\"\"\n    features = self.forward_backbone(x)\n    action_logits = self.policy_head(features)\n    return self.softmax(action_logits)\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.act","title":"act","text":"<pre><code>act(state: list | tuple | Tensor, deterministic: bool = False) -&gt; int\n</code></pre> <p>Select action based on current policy.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>list | tuple | Tensor</code> <p>Current state as list, tuple, or tensor</p> required <code>deterministic</code> <code>bool</code> <p>If True, always returns the most probable action.           If False, samples from the action distribution.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Selected action as integer</p> Source code in <code>viberl/networks/policy_network.py</code> <pre><code>def act(self, state: list | tuple | torch.Tensor, deterministic: bool = False) -&gt; int:\n    \"\"\"Select action based on current policy.\n\n    Args:\n        state: Current state as list, tuple, or tensor\n        deterministic: If True, always returns the most probable action.\n                      If False, samples from the action distribution.\n\n    Returns:\n        Selected action as integer\n    \"\"\"\n    if isinstance(state, list | tuple):\n        state = torch.FloatTensor(state)\n    else:\n        state = torch.FloatTensor(state).unsqueeze(0)\n\n    action_probs = self.forward(state)\n\n    if deterministic:\n        return action_probs.argmax().item()\n    else:\n        m = Categorical(action_probs)\n        return m.sample().item()\n</code></pre>"},{"location":"api/networks/policy_network/#viberl.networks.policy_network.PolicyNetwork.get_action_prob","title":"get_action_prob","text":"<pre><code>get_action_prob(state: list | tuple | Tensor, action: Tensor) -&gt; Tensor\n</code></pre> <p>Get probability of taking a specific action.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>list | tuple | Tensor</code> <p>Current state as list, tuple, or tensor</p> required <code>action</code> <code>Tensor</code> <p>Action tensor to get probability for</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Probability of taking the specified action</p> Source code in <code>viberl/networks/policy_network.py</code> <pre><code>def get_action_prob(\n    self, state: list | tuple | torch.Tensor, action: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Get probability of taking a specific action.\n\n    Args:\n        state: Current state as list, tuple, or tensor\n        action: Action tensor to get probability for\n\n    Returns:\n        Probability of taking the specified action\n    \"\"\"\n    action_probs = self.forward(state)\n    return action_probs.gather(1, action.unsqueeze(1)).squeeze(1)\n</code></pre>"},{"location":"api/networks/value_network/","title":"<code>viberl.networks.value_network</code>","text":"<p>Classes:</p> Name Description <code>VNetwork</code> <p>Value network for PPO and other policy gradient methods (returns single scalar value for state).</p> <code>QNetwork</code> <p>Q-network for value-based methods like DQN (returns Q-values for all actions).</p>"},{"location":"api/networks/value_network/#viberl.networks.value_network.VNetwork","title":"VNetwork","text":"<pre><code>VNetwork(state_size: int, hidden_size: int = 128, num_hidden_layers: int = 2)\n</code></pre> <p>               Bases: <code>BaseNetwork</code></p> <p>Value network for PPO and other policy gradient methods (returns single scalar value for state).</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass to get state value.</p> <p>Attributes:</p> Name Type Description <code>value_head</code> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def __init__(self, state_size: int, hidden_size: int = 128, num_hidden_layers: int = 2):\n    super().__init__(state_size, hidden_size, num_hidden_layers)\n\n    # Single output for state value\n    self.value_head = nn.Linear(hidden_size, 1)\n    self.init_weights()\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.VNetwork.value_head","title":"value_head  <code>instance-attribute</code>","text":"<pre><code>value_head = Linear(hidden_size, 1)\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.VNetwork.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass to get state value.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input state tensor of shape (batch_size, state_size)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>State value tensor of shape (batch_size,) representing the</p> <code>Tensor</code> <p>estimated value of the given state</p> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass to get state value.\n\n    Args:\n        x: Input state tensor of shape (batch_size, state_size)\n\n    Returns:\n        State value tensor of shape (batch_size,) representing the\n        estimated value of the given state\n    \"\"\"\n    features = self.forward_backbone(x)\n    return self.value_head(features).squeeze(-1)  # Remove last dim\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork","title":"QNetwork","text":"<pre><code>QNetwork(state_size: int, action_size: int, hidden_size: int = 128, num_hidden_layers: int = 2)\n</code></pre> <p>               Bases: <code>BaseNetwork</code></p> <p>Q-network for value-based methods like DQN (returns Q-values for all actions).</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass to get Q-values for all actions.</p> <code>get_q_values</code> <p>Get Q-values for a given state.</p> <code>get_action</code> <p>Get action using epsilon-greedy policy.</p> <p>Attributes:</p> Name Type Description <code>action_size</code> <code>q_head</code> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def __init__(\n    self, state_size: int, action_size: int, hidden_size: int = 128, num_hidden_layers: int = 2\n):\n    super().__init__(state_size, hidden_size, num_hidden_layers)\n    self.action_size = action_size\n\n    # Q-value head for all actions\n    self.q_head = nn.Linear(hidden_size, action_size)\n\n    self.init_weights()\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.action_size","title":"action_size  <code>instance-attribute</code>","text":"<pre><code>action_size = action_size\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.q_head","title":"q_head  <code>instance-attribute</code>","text":"<pre><code>q_head = Linear(hidden_size, action_size)\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass to get Q-values for all actions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input state tensor of shape (batch_size, state_size)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Q-values tensor of shape (batch_size, action_size) containing</p> <code>Tensor</code> <p>Q-values for each action in the given state</p> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass to get Q-values for all actions.\n\n    Args:\n        x: Input state tensor of shape (batch_size, state_size)\n\n    Returns:\n        Q-values tensor of shape (batch_size, action_size) containing\n        Q-values for each action in the given state\n    \"\"\"\n    features = self.forward_backbone(x)\n    return self.q_head(features)\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.get_q_values","title":"get_q_values","text":"<pre><code>get_q_values(state: list | tuple | Tensor) -&gt; Tensor\n</code></pre> <p>Get Q-values for a given state.</p> <p>Convenience method that handles various input types and ensures proper tensor formatting before forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>list | tuple | Tensor</code> <p>Current state as list, tuple, or tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Q-values tensor of shape (1, action_size) if single state,</p> <code>Tensor</code> <p>or (batch_size, action_size) if batch of states</p> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def get_q_values(self, state: list | tuple | torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Get Q-values for a given state.\n\n    Convenience method that handles various input types and ensures\n    proper tensor formatting before forward pass.\n\n    Args:\n        state: Current state as list, tuple, or tensor\n\n    Returns:\n        Q-values tensor of shape (1, action_size) if single state,\n        or (batch_size, action_size) if batch of states\n    \"\"\"\n    if isinstance(state, list | tuple):\n        state = torch.FloatTensor(state)\n    else:\n        state = torch.FloatTensor(state)\n\n    if len(state.shape) == 1:\n        state = state.unsqueeze(0)\n\n    return self.forward(state)\n</code></pre>"},{"location":"api/networks/value_network/#viberl.networks.value_network.QNetwork.get_action","title":"get_action","text":"<pre><code>get_action(state: list | tuple | Tensor, epsilon: float = 0.0) -&gt; int\n</code></pre> <p>Get action using epsilon-greedy policy.</p> <p>Implements the epsilon-greedy action selection strategy where: - With probability epsilon: choose random action (exploration) - With probability 1-epsilon: choose best action (exploitation)</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>list | tuple | Tensor</code> <p>Current state as list, tuple, or tensor</p> required <code>epsilon</code> <code>float</code> <p>Probability of choosing random action (0.0 to 1.0)</p> <code>0.0</code> <p>Returns:</p> Type Description <code>int</code> <p>Selected action as integer</p> Source code in <code>viberl/networks/value_network.py</code> <pre><code>def get_action(self, state: list | tuple | torch.Tensor, epsilon: float = 0.0) -&gt; int:\n    \"\"\"Get action using epsilon-greedy policy.\n\n    Implements the epsilon-greedy action selection strategy where:\n    - With probability epsilon: choose random action (exploration)\n    - With probability 1-epsilon: choose best action (exploitation)\n\n    Args:\n        state: Current state as list, tuple, or tensor\n        epsilon: Probability of choosing random action (0.0 to 1.0)\n\n    Returns:\n        Selected action as integer\n    \"\"\"\n    q_values = self.get_q_values(state)\n\n    if torch.rand(1).item() &lt; epsilon:\n        return torch.randint(0, self.action_size, (1,)).item()\n    else:\n        return q_values.argmax(dim=1).item()\n</code></pre>"},{"location":"api/utils/common/","title":"<code>viberl.utils.common</code>","text":"<p>Functions:</p> Name Description <code>set_seed</code> <p>Set random seed for reproducibility.</p> <code>get_device</code> <p>Get the best available device (CUDA if available, else CPU).</p> <code>normalize_returns</code> <p>Normalize returns to zero mean and unit variance.</p>"},{"location":"api/utils/common/#viberl.utils.common.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int) -&gt; None\n</code></pre> <p>Set random seed for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed value</p> required Source code in <code>viberl/utils/common.py</code> <pre><code>def set_seed(seed: int) -&gt; None:\n    \"\"\"\n    Set random seed for reproducibility.\n\n    Args:\n        seed: Random seed value\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n</code></pre>"},{"location":"api/utils/common/#viberl.utils.common.get_device","title":"get_device","text":"<pre><code>get_device() -&gt; device\n</code></pre> <p>Get the best available device (CUDA if available, else CPU).</p> <p>Returns:</p> Type Description <code>device</code> <p>PyTorch device</p> Source code in <code>viberl/utils/common.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"\n    Get the best available device (CUDA if available, else CPU).\n\n    Returns:\n        PyTorch device\n    \"\"\"\n    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n</code></pre>"},{"location":"api/utils/common/#viberl.utils.common.normalize_returns","title":"normalize_returns","text":"<pre><code>normalize_returns(returns: ndarray) -&gt; ndarray\n</code></pre> <p>Normalize returns to zero mean and unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>ndarray</code> <p>Array of returns</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Normalized returns</p> Source code in <code>viberl/utils/common.py</code> <pre><code>def normalize_returns(returns: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Normalize returns to zero mean and unit variance.\n\n    Args:\n        returns: Array of returns\n\n    Returns:\n        Normalized returns\n    \"\"\"\n    if len(returns) == 0:\n        return returns\n\n    mean = np.mean(returns)\n    std = np.std(returns)\n\n    if std == 0:\n        return returns - mean\n\n    return (returns - mean) / (std + 1e-8)\n</code></pre>"},{"location":"api/utils/experiment_manager/","title":"<code>viberl.utils.experiment_manager</code>","text":"<p>Experiment management utilities.</p> <p>This module provides utilities for managing experiment directories, tensorboard logging, and model checkpoints.</p> <p>Classes:</p> Name Description <code>ExperimentManager</code> <p>Manages experiment directories with automatic naming and organization.</p> <p>Functions:</p> Name Description <code>create_experiment</code> <p>Convenience function to create a new experiment.</p>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager","title":"ExperimentManager","text":"<pre><code>ExperimentManager(\n    experiment_name: str, base_dir: str = 'experiments', timestamp_format: str = '%Y%m%d_%H%M%S'\n)\n</code></pre> <p>Manages experiment directories with automatic naming and organization.</p> <p>Creates experiment directories in the format: experiments/{experiment_name}_{timestamp}/ \u251c\u2500\u2500 tb_logs/          # TensorBoard logs \u2514\u2500\u2500 models/           # Saved model checkpoints</p> <p>Initialize experiment manager.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment</p> required <code>base_dir</code> <code>str</code> <p>Base directory for experiments</p> <code>'experiments'</code> <code>timestamp_format</code> <code>str</code> <p>Format for timestamp in directory name</p> <code>'%Y%m%d_%H%M%S'</code> <p>Methods:</p> Name Description <code>get_tb_logs_path</code> <p>Get path to TensorBoard logs directory.</p> <code>get_models_path</code> <p>Get path to models directory.</p> <code>get_experiment_path</code> <p>Get path to experiment directory.</p> <code>save_model</code> <p>Get full path for saving a model.</p> <code>list_experiments</code> <p>List all existing experiments.</p> <code>get_latest_experiment</code> <p>Get the latest experiment directory.</p> <code>print_experiment_info</code> <p>Print information about the current experiment.</p> <code>create_from_existing</code> <p>Create ExperimentManager from existing experiment directory.</p> <p>Attributes:</p> Name Type Description <code>experiment_name</code> <code>base_dir</code> <code>timestamp_format</code> <code>experiment_dir</code> <code>tb_logs_dir</code> <code>models_dir</code> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def __init__(\n    self,\n    experiment_name: str,\n    base_dir: str = 'experiments',\n    timestamp_format: str = '%Y%m%d_%H%M%S',\n):\n    \"\"\"\n    Initialize experiment manager.\n\n    Args:\n        experiment_name: Name of the experiment\n        base_dir: Base directory for experiments\n        timestamp_format: Format for timestamp in directory name\n    \"\"\"\n    self.experiment_name = experiment_name\n    self.base_dir = Path(base_dir)\n    self.timestamp_format = timestamp_format\n\n    # Create experiment directory name with timestamp\n    timestamp = datetime.now().strftime(timestamp_format)\n    self.experiment_dir = self.base_dir / f'{experiment_name}_{timestamp}'\n\n    # Create subdirectories\n    self.tb_logs_dir = self.experiment_dir / 'tb_logs'\n    self.models_dir = self.experiment_dir / 'models'\n\n    # Create directories\n    self._create_directories()\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.experiment_name","title":"experiment_name  <code>instance-attribute</code>","text":"<pre><code>experiment_name = experiment_name\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.base_dir","title":"base_dir  <code>instance-attribute</code>","text":"<pre><code>base_dir = Path(base_dir)\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.timestamp_format","title":"timestamp_format  <code>instance-attribute</code>","text":"<pre><code>timestamp_format = timestamp_format\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.experiment_dir","title":"experiment_dir  <code>instance-attribute</code>","text":"<pre><code>experiment_dir = base_dir / f'{experiment_name}_{timestamp}'\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.tb_logs_dir","title":"tb_logs_dir  <code>instance-attribute</code>","text":"<pre><code>tb_logs_dir = experiment_dir / 'tb_logs'\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.models_dir","title":"models_dir  <code>instance-attribute</code>","text":"<pre><code>models_dir = experiment_dir / 'models'\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_tb_logs_path","title":"get_tb_logs_path","text":"<pre><code>get_tb_logs_path() -&gt; Path\n</code></pre> <p>Get path to TensorBoard logs directory.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_tb_logs_path(self) -&gt; Path:\n    \"\"\"Get path to TensorBoard logs directory.\"\"\"\n    return self.tb_logs_dir\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_models_path","title":"get_models_path","text":"<pre><code>get_models_path() -&gt; Path\n</code></pre> <p>Get path to models directory.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_models_path(self) -&gt; Path:\n    \"\"\"Get path to models directory.\"\"\"\n    return self.models_dir\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_experiment_path","title":"get_experiment_path","text":"<pre><code>get_experiment_path() -&gt; Path\n</code></pre> <p>Get path to experiment directory.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_experiment_path(self) -&gt; Path:\n    \"\"\"Get path to experiment directory.\"\"\"\n    return self.experiment_dir\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.save_model","title":"save_model","text":"<pre><code>save_model(model_name: str) -&gt; Path\n</code></pre> <p>Get full path for saving a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model file (without extension)</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Full path for model file</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def save_model(self, model_name: str) -&gt; Path:\n    \"\"\"\n    Get full path for saving a model.\n\n    Args:\n        model_name: Name of the model file (without extension)\n\n    Returns:\n        Full path for model file\n    \"\"\"\n    return self.models_dir / f'{model_name}.pth'\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.list_experiments","title":"list_experiments","text":"<pre><code>list_experiments() -&gt; list[Path]\n</code></pre> <p>List all existing experiments.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def list_experiments(self) -&gt; list[Path]:\n    \"\"\"List all existing experiments.\"\"\"\n    if not self.base_dir.exists():\n        return []\n\n    experiments = [\n        exp_dir\n        for exp_dir in self.base_dir.iterdir()\n        if exp_dir.is_dir() and self.experiment_name in exp_dir.name\n    ]\n\n    return sorted(experiments, reverse=True)\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.get_latest_experiment","title":"get_latest_experiment","text":"<pre><code>get_latest_experiment() -&gt; Path | None\n</code></pre> <p>Get the latest experiment directory.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def get_latest_experiment(self) -&gt; Path | None:\n    \"\"\"Get the latest experiment directory.\"\"\"\n    experiments = self.list_experiments()\n    return experiments[0] if experiments else None\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.print_experiment_info","title":"print_experiment_info","text":"<pre><code>print_experiment_info() -&gt; None\n</code></pre> <p>Print information about the current experiment.</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def print_experiment_info(self) -&gt; None:\n    \"\"\"Print information about the current experiment.\"\"\"\n    print(f'Experiment: {self.experiment_name}')\n    print(f'Directory: {self.experiment_dir}')\n    print(f'TensorBoard logs: {self.tb_logs_dir}')\n    print(f'Models: {self.models_dir}')\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.ExperimentManager.create_from_existing","title":"create_from_existing  <code>staticmethod</code>","text":"<pre><code>create_from_existing(experiment_path: str | Path) -&gt; ExperimentManager\n</code></pre> <p>Create ExperimentManager from existing experiment directory.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_path</code> <code>str | Path</code> <p>Path to existing experiment directory</p> required <p>Returns:</p> Type Description <code>ExperimentManager</code> <p>ExperimentManager instance</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>@staticmethod\ndef create_from_existing(\n    experiment_path: str | Path,\n) -&gt; 'ExperimentManager':\n    \"\"\"\n    Create ExperimentManager from existing experiment directory.\n\n    Args:\n        experiment_path: Path to existing experiment directory\n\n    Returns:\n        ExperimentManager instance\n    \"\"\"\n    experiment_path = Path(experiment_path)\n    experiment_name = experiment_path.name.rsplit('_', 1)[0]\n\n    # Create manager but don't create directories since they exist\n    manager = ExperimentManager(experiment_name, str(experiment_path.parent))\n    return manager\n</code></pre>"},{"location":"api/utils/experiment_manager/#viberl.utils.experiment_manager.create_experiment","title":"create_experiment","text":"<pre><code>create_experiment(\n    experiment_name: str, base_dir: str = 'experiments', print_info: bool = True\n) -&gt; ExperimentManager\n</code></pre> <p>Convenience function to create a new experiment.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment</p> required <code>base_dir</code> <code>str</code> <p>Base directory for experiments</p> <code>'experiments'</code> <code>print_info</code> <code>bool</code> <p>Whether to print experiment info</p> <code>True</code> <p>Returns:</p> Type Description <code>ExperimentManager</code> <p>ExperimentManager instance</p> Source code in <code>viberl/utils/experiment_manager.py</code> <pre><code>def create_experiment(\n    experiment_name: str,\n    base_dir: str = 'experiments',\n    print_info: bool = True,\n) -&gt; ExperimentManager:\n    \"\"\"\n    Convenience function to create a new experiment.\n\n    Args:\n        experiment_name: Name of the experiment\n        base_dir: Base directory for experiments\n        print_info: Whether to print experiment info\n\n    Returns:\n        ExperimentManager instance\n    \"\"\"\n    manager = ExperimentManager(experiment_name, base_dir)\n\n    if print_info:\n        manager.print_experiment_info()\n\n    return manager\n</code></pre>"},{"location":"api/utils/mock_env/","title":"<code>viberl.utils.mock_env</code>","text":"<p>Mock environment for testing RL algorithms.</p> <p>Provides a gymnasium-compatible environment that returns random valid values for all methods, useful for testing agents without complex environment setup.</p> <p>Classes:</p> Name Description <code>MockEnv</code> <p>A mock environment that returns random valid values for testing.</p>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv","title":"MockEnv","text":"<pre><code>MockEnv(state_size: int = 4, action_size: int = 2, max_episode_steps: int = 100)\n</code></pre> <p>               Bases: <code>Env</code></p> <p>A mock environment that returns random valid values for testing.</p> <p>This environment provides: - Random observations within observation space - Random rewards within reward range - Random terminal states - Random info dictionaries</p> <p>Parameters:</p> Name Type Description Default <code>state_size</code> <code>int</code> <p>Size of the observation space</p> <code>4</code> <code>action_size</code> <code>int</code> <p>Number of discrete actions</p> <code>2</code> <code>max_episode_steps</code> <code>int</code> <p>Maximum steps before truncation</p> <code>100</code> <p>Methods:</p> Name Description <code>reset</code> <p>Reset the environment with random initial state.</p> <code>step</code> <p>Take a step with random outcomes.</p> <code>render</code> <p>Mock render - does nothing.</p> <code>close</code> <p>Mock close - does nothing.</p> <code>seed</code> <p>Set random seed for reproducibility.</p> <p>Attributes:</p> Name Type Description <code>state_size</code> <code>action_size</code> <code>max_episode_steps</code> <code>observation_space</code> <code>action_space</code> <code>current_step</code> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def __init__(\n    self,\n    state_size: int = 4,\n    action_size: int = 2,\n    max_episode_steps: int = 100,\n) -&gt; None:\n    super().__init__()\n\n    self.state_size = state_size\n    self.action_size = action_size\n    self.max_episode_steps = max_episode_steps\n\n    # Define spaces\n    self.observation_space = spaces.Box(\n        low=-1.0, high=1.0, shape=(state_size,), dtype=np.float32\n    )\n    self.action_space = spaces.Discrete(action_size)\n\n    # Internal state\n    self.current_step = 0\n    self._np_random = np.random.RandomState()\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.state_size","title":"state_size  <code>instance-attribute</code>","text":"<pre><code>state_size = state_size\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.action_size","title":"action_size  <code>instance-attribute</code>","text":"<pre><code>action_size = action_size\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.max_episode_steps","title":"max_episode_steps  <code>instance-attribute</code>","text":"<pre><code>max_episode_steps = max_episode_steps\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.observation_space","title":"observation_space  <code>instance-attribute</code>","text":"<pre><code>observation_space = Box(low=-1.0, high=1.0, shape=(state_size,), dtype=float32)\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.action_space","title":"action_space  <code>instance-attribute</code>","text":"<pre><code>action_space = Discrete(action_size)\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.current_step","title":"current_step  <code>instance-attribute</code>","text":"<pre><code>current_step = 0\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.reset","title":"reset","text":"<pre><code>reset(seed: int | None = None, options: dict | None = None) -&gt; tuple[ndarray, dict]\n</code></pre> <p>Reset the environment with random initial state.</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def reset(\n    self,\n    seed: int | None = None,\n    options: dict | None = None,\n) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"Reset the environment with random initial state.\"\"\"\n    super().reset(seed=seed)\n\n    if seed is not None:\n        self._np_random = np.random.RandomState(seed)\n        # Set numpy's global random state for gymnasium's sample() method\n        np.random.seed(seed)\n\n    self.current_step = 0\n\n    # Generate random observation using our seeded random state\n    obs = self._np_random.uniform(\n        low=self.observation_space.low,\n        high=self.observation_space.high,\n        size=self.observation_space.shape,\n    ).astype(np.float32)\n\n    # Return with random info\n    info = {'episode': 0, 'step': 0, 'random_metric': self._np_random.random()}\n\n    return obs, info\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.step","title":"step","text":"<pre><code>step(action: int) -&gt; tuple[ndarray, float, bool, bool, dict]\n</code></pre> <p>Take a step with random outcomes.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>int</code> <p>The action to take</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, float, bool, bool, dict]</code> <p>observation, reward, terminated, truncated, info</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def step(self, action: int) -&gt; tuple[np.ndarray, float, bool, bool, dict]:\n    \"\"\"\n    Take a step with random outcomes.\n\n    Args:\n        action: The action to take\n\n    Returns:\n        observation, reward, terminated, truncated, info\n    \"\"\"\n    assert self.action_space.contains(action), f'Invalid action: {action}'\n\n    # Check if we've reached max steps (truncation happens after max_episode_steps steps)\n    if self.current_step &gt;= self.max_episode_steps:\n        obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n        reward = 0.0\n        terminated = True\n        truncated = True\n        info = {'step': self.current_step, 'truncated': True}\n        return obs, reward, terminated, truncated, info\n\n    self.current_step += 1\n\n    # Generate random observation using seeded random state\n    obs = self._np_random.uniform(\n        low=self.observation_space.low,\n        high=self.observation_space.high,\n        size=self.observation_space.shape,\n    ).astype(np.float32)\n\n    # Generate random reward (-1 to 1)\n    reward = float(self._np_random.uniform(-1.0, 1.0))\n\n    # Random termination (5% chance per step)\n    terminated = bool(self._np_random.random() &lt; 0.05)\n\n    # Truncation happens when we reach max_episode_steps\n    truncated = self.current_step &gt;= self.max_episode_steps\n\n    # Random info\n    info = {\n        'step': self.current_step,\n        'action_taken': action,\n        'random_info': self._np_random.random(),\n        'episode_complete': terminated or truncated,\n    }\n\n    return obs, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.render","title":"render","text":"<pre><code>render() -&gt; None\n</code></pre> <p>Mock render - does nothing.</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def render(self) -&gt; None:\n    \"\"\"Mock render - does nothing.\"\"\"\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Mock close - does nothing.</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Mock close - does nothing.\"\"\"\n</code></pre>"},{"location":"api/utils/mock_env/#viberl.utils.mock_env.MockEnv.seed","title":"seed","text":"<pre><code>seed(seed: int | None = None) -&gt; None\n</code></pre> <p>Set random seed for reproducibility.</p> Source code in <code>viberl/utils/mock_env.py</code> <pre><code>def seed(self, seed: int | None = None) -&gt; None:\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    self._np_random = np.random.RandomState(seed)\n</code></pre>"},{"location":"api/utils/training/","title":"<code>viberl.utils.training</code>","text":"<p>Functions:</p> Name Description <code>train_agent</code> <p>Generic training function for RL agents with periodic evaluation.</p> <code>evaluate_agent</code> <p>Generic evaluation function for RL agents.</p>"},{"location":"api/utils/training/#viberl.utils.training.train_agent","title":"train_agent","text":"<pre><code>train_agent(\n    env: Env,\n    agent: Agent,\n    num_episodes: int = 1000,\n    max_steps: int = 1000,\n    render_interval: int | None = None,\n    save_interval: int | None = None,\n    save_path: str | None = None,\n    verbose: bool = True,\n    log_dir: str | None = None,\n    eval_interval: int = 100,\n    eval_episodes: int = 10,\n) -&gt; list[float]\n</code></pre> <p>Generic training function for RL agents with periodic evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Env</code> <p>Gymnasium environment</p> required <code>agent</code> <code>Agent</code> <p>RL agent with select_action, store_transition, and update_policy methods</p> required <code>num_episodes</code> <code>int</code> <p>Number of training episodes</p> <code>1000</code> <code>max_steps</code> <code>int</code> <p>Maximum steps per episode</p> <code>1000</code> <code>render_interval</code> <code>int | None</code> <p>Render every N episodes</p> <code>None</code> <code>save_interval</code> <code>int | None</code> <p>Save model every N episodes</p> <code>None</code> <code>save_path</code> <code>str | None</code> <p>Path to save models</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print training progress</p> <code>True</code> <code>log_dir</code> <code>str | None</code> <p>Directory for TensorBoard logs</p> <code>None</code> <code>eval_interval</code> <code>int</code> <p>Evaluate every N episodes</p> <code>100</code> <code>eval_episodes</code> <code>int</code> <p>Number of evaluation episodes</p> <code>10</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of episode rewards</p> Source code in <code>viberl/utils/training.py</code> <pre><code>def train_agent(\n    env: gym.Env,\n    agent: Agent,\n    num_episodes: int = 1000,\n    max_steps: int = 1000,\n    render_interval: int | None = None,\n    save_interval: int | None = None,\n    save_path: str | None = None,\n    verbose: bool = True,\n    log_dir: str | None = None,\n    eval_interval: int = 100,\n    eval_episodes: int = 10,\n) -&gt; list[float]:\n    \"\"\"\n    Generic training function for RL agents with periodic evaluation.\n\n    Args:\n        env: Gymnasium environment\n        agent: RL agent with select_action, store_transition, and update_policy methods\n        num_episodes: Number of training episodes\n        max_steps: Maximum steps per episode\n        render_interval: Render every N episodes\n        save_interval: Save model every N episodes\n        save_path: Path to save models\n        verbose: Print training progress\n        log_dir: Directory for TensorBoard logs\n        eval_interval: Evaluate every N episodes\n        eval_episodes: Number of evaluation episodes\n\n    Returns:\n        List of episode rewards\n    \"\"\"\n    scores = []\n    eval_scores = []\n\n    # Initialize TensorBoard writer\n    writer = None\n    if log_dir is not None:\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        writer = SummaryWriter(log_dir=log_dir)\n\n    # Create evaluation environment\n    # Handle both gym.make() environments and custom environments\n    from viberl.envs import SnakeGameEnv\n\n    if hasattr(env, 'grid_size'):\n        eval_env = SnakeGameEnv(render_mode=None, grid_size=env.grid_size)\n    else:\n        eval_env = SnakeGameEnv(render_mode=None)\n\n    for episode in range(num_episodes):\n        state, _ = env.reset()\n        state = state.flatten()  # Flatten 2D grid to 1D vector\n        episode_reward = 0\n\n        # Collect transitions for this episode\n        transitions = []\n\n        from viberl.agents.ppo import PPOAgent\n\n        for _step in range(max_steps):\n            # Select action using unified Agent interface\n            action_obj = agent.act(state)\n            action = action_obj.action\n\n            # For PPO, collect additional information\n            log_prob = None\n            if isinstance(agent, PPOAgent):\n                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n                with torch.no_grad():\n                    action_probs = agent.policy_network(state_tensor)\n                    dist = torch.distributions.Categorical(action_probs)\n                    log_prob = dist.log_prob(torch.tensor(action))\n                    action_obj.logprobs = log_prob\n\n            # Take action in environment\n            next_state, reward, terminated, truncated, info = env.step(action)\n            done = terminated or truncated\n\n            # Flatten next state\n            next_state = next_state.flatten()\n\n            # Create transition\n            transition = Transition(\n                state=state,\n                action=action_obj,\n                reward=reward,\n                next_state=next_state,\n                done=done,\n                info=info,\n            )\n            transitions.append(transition)\n\n            episode_reward += reward\n            state = next_state\n\n            # Render if specified\n            if render_interval and episode % render_interval == 0:\n                env.render()\n\n            if done:\n                break\n\n        # Create trajectory and update policy\n        trajectory = Trajectory.from_transitions(transitions)\n        learn_metrics = {}\n        if (episode + 1) % 10 == 0:  # Update every 10 episodes\n            learn_metrics = agent.learn(trajectory=trajectory)\n            if verbose and learn_metrics:\n                # Display returned metrics\n                metrics_str = ', '.join(f'{k}: {v:.4f}' for k, v in learn_metrics.items())\n                print(f'Update - {metrics_str}')\n\n        # Log learn/ metrics\n        if writer is not None and learn_metrics:\n            for metric_name, metric_value in learn_metrics.items():\n                writer.add_scalar(f'learn/{metric_name}', metric_value, episode)\n\n        scores.append(episode_reward)\n\n        # Log training metrics to TensorBoard\n        if writer is not None:\n            # rollout_train/ metrics - training environment metrics\n            writer.add_scalar('rollout_train/average_return', episode_reward, episode)\n            writer.add_scalar('rollout_train/episode_length', _step + 1, episode)\n\n        # Print progress\n        if verbose and (episode + 1) % 100 == 0:\n            avg_score = np.mean(scores[-100:])\n            print(\n                f'Episode {episode + 1}/{num_episodes}, Average Score (last 100): {avg_score:.2f}'\n            )\n\n        # Periodic evaluation\n        if (episode + 1) % eval_interval == 0:\n            eval_rewards, eval_lengths = evaluate_agent(\n                eval_env, agent, num_episodes=eval_episodes, render=False, max_steps=max_steps\n            )\n            eval_mean = np.mean(eval_rewards)\n            eval_std = np.std(eval_rewards)\n            eval_scores.extend(eval_rewards)\n\n            if writer is not None:\n                # rollout_eval/ metrics - evaluation environment metrics\n                writer.add_scalar('rollout_eval/average_return', eval_mean, episode)\n                writer.add_scalar(\n                    'rollout_eval/episode_length',\n                    np.mean(eval_lengths) if eval_lengths else 0,\n                    episode,\n                )\n\n            if verbose:\n                print(\n                    f'Evaluation at episode {episode + 1}: Mean={eval_mean:.2f}, Std={eval_std:.2f}'\n                )\n\n        # Save model if specified\n        if (\n            save_interval\n            and save_path\n            and (episode + 1) % save_interval == 0\n            and hasattr(agent, 'save_policy')\n        ):\n            # Ensure save_path is a directory path\n            save_dir = os.path.dirname(save_path)\n            if save_dir and not os.path.exists(save_dir):\n                os.makedirs(save_dir, exist_ok=True)\n\n            # Create proper filename\n            filename = os.path.join(save_dir, f'model_episode_{episode + 1}.pth')\n            agent.save(filename)\n\n    # Close environments\n    if writer is not None:\n        writer.close()\n    eval_env.close()\n\n    return scores\n</code></pre>"},{"location":"api/utils/training/#viberl.utils.training.evaluate_agent","title":"evaluate_agent","text":"<pre><code>evaluate_agent(\n    env: Env, agent: Agent, num_episodes: int = 10, render: bool = False, max_steps: int = 1000\n) -&gt; tuple[list[float], list[int]]\n</code></pre> <p>Generic evaluation function for RL agents.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Env</code> <p>Gymnasium environment</p> required <code>agent</code> <code>Agent</code> <p>RL agent with select_action method</p> required <code>num_episodes</code> <code>int</code> <p>Number of evaluation episodes</p> <code>10</code> <code>render</code> <code>bool</code> <p>Whether to render the environment</p> <code>False</code> <code>max_steps</code> <code>int</code> <p>Maximum steps per episode</p> <code>1000</code> <p>Returns:</p> Type Description <code>tuple[list[float], list[int]]</code> <p>Tuple of (episode_rewards, episode_lengths)</p> Source code in <code>viberl/utils/training.py</code> <pre><code>def evaluate_agent(\n    env: gym.Env, agent: Agent, num_episodes: int = 10, render: bool = False, max_steps: int = 1000\n) -&gt; tuple[list[float], list[int]]:\n    \"\"\"\n    Generic evaluation function for RL agents.\n\n    Args:\n        env: Gymnasium environment\n        agent: RL agent with select_action method\n        num_episodes: Number of evaluation episodes\n        render: Whether to render the environment\n        max_steps: Maximum steps per episode\n\n    Returns:\n        Tuple of (episode_rewards, episode_lengths)\n    \"\"\"\n    scores = []\n    lengths = []\n\n    for episode in range(num_episodes):\n        state, _ = env.reset()\n        state = state.flatten()  # Flatten 2D grid to 1D vector\n        episode_reward = 0\n        episode_length = 0\n\n        for _step in range(max_steps):\n            # Use unified Agent interface\n            action_obj = agent.act(state, training=False)\n            action = action_obj.action\n\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n\n            # Flatten next state\n            next_state = next_state.flatten()\n\n            episode_reward += reward\n            episode_length += 1\n            state = next_state\n\n            if render:\n                env.render()\n\n            if done:\n                break\n\n        scores.append(episode_reward)\n        lengths.append(episode_length)\n\n        agent_name = agent.__class__.__name__ if hasattr(agent, '__class__') else 'Agent'\n\n        print(\n            f'{agent_name} - Evaluation Episode {episode + 1}: Score = {episode_reward}, Length = {episode_length}'\n        )\n\n    print(\n        f'Average Score: {np.mean(scores):.2f} \u00b1 {np.std(scores):.2f}, Average Length: {np.mean(lengths):.2f}'\n    )\n    return scores, lengths\n</code></pre>"}]}